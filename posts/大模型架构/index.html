<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>大模型架构 | Huaipeng's Blog</title><meta name=keywords content><meta name=description content="1. NLP发展历程与模型架构

NLP的发展渐趋于统一化：在深度学习时代到来之前，NLP任务高度依赖于手工设计的复杂特征。随后，深度学习的出现极大地减轻了这种特征工程的负担。以BERT和GPT-1为代表的预训练和微调范式，标志着手工特征设计时代的终结。大模型的出现宣告了NLP中间任务的消亡，所有任务都可以统一到语言模型的范畴中。"><meta name=author content="Huaipeng Zhao"><link rel=canonical href=https://hpzhao.github.io/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/><link crossorigin=anonymous href=/assets/css/stylesheet.da3211e5ef867bf2b75fd5a6515cfed7195c011e8ab735694e203810a827097b.css integrity="sha256-2jIR5e+Ge/K3X9WmUVz+1xlcAR6KtzVpTiA4EKgnCXs=" rel="preload stylesheet" as=style><link rel=icon href=https://hpzhao.github.io/img/logo.gif><link rel=icon type=image/png sizes=16x16 href=https://hpzhao.github.io/img/logo.gif><link rel=icon type=image/png sizes=32x32 href=https://hpzhao.github.io/img/logo.gif><link rel=apple-touch-icon href=https://hpzhao.github.io/logo.gif><link rel=mask-icon href=https://hpzhao.github.io/logo.gif><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://hpzhao.github.io/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:url" content="https://hpzhao.github.io/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/"><meta property="og:site_name" content="Huaipeng's Blog"><meta property="og:title" content="大模型架构"><meta property="og:description" content="1. NLP发展历程与模型架构 NLP的发展渐趋于统一化：在深度学习时代到来之前，NLP任务高度依赖于手工设计的复杂特征。随后，深度学习的出现极大地减轻了这种特征工程的负担。以BERT和GPT-1为代表的预训练和微调范式，标志着手工特征设计时代的终结。大模型的出现宣告了NLP中间任务的消亡，所有任务都可以统一到语言模型的范畴中。"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-20T01:03:53+08:00"><meta property="article:modified_time" content="2023-10-20T01:03:53+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="大模型架构"><meta name=twitter:description content="1. NLP发展历程与模型架构

NLP的发展渐趋于统一化：在深度学习时代到来之前，NLP任务高度依赖于手工设计的复杂特征。随后，深度学习的出现极大地减轻了这种特征工程的负担。以BERT和GPT-1为代表的预训练和微调范式，标志着手工特征设计时代的终结。大模型的出现宣告了NLP中间任务的消亡，所有任务都可以统一到语言模型的范畴中。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://hpzhao.github.io/posts/"},{"@type":"ListItem","position":2,"name":"大模型架构","item":"https://hpzhao.github.io/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"大模型架构","name":"大模型架构","description":"1. NLP发展历程与模型架构 NLP的发展渐趋于统一化：在深度学习时代到来之前，NLP任务高度依赖于手工设计的复杂特征。随后，深度学习的出现极大地减轻了这种特征工程的负担。以BERT和GPT-1为代表的预训练和微调范式，标志着手工特征设计时代的终结。大模型的出现宣告了NLP中间任务的消亡，所有任务都可以统一到语言模型的范畴中。\n","keywords":[],"articleBody":"1. NLP发展历程与模型架构 NLP的发展渐趋于统一化：在深度学习时代到来之前，NLP任务高度依赖于手工设计的复杂特征。随后，深度学习的出现极大地减轻了这种特征工程的负担。以BERT和GPT-1为代表的预训练和微调范式，标志着手工特征设计时代的终结。大模型的出现宣告了NLP中间任务的消亡，所有任务都可以统一到语言模型的范畴中。\n1.1. 规则阶段(1956～1992) 1.1.1. 阶段特点 基于规则的机器翻译系统是在内部把各种功能的模块串到一起，由人先从数据中获取知识，归纳出规则，写出来教给机器，然后机器来执行这套规则，从而完成特定任务。\n1.1.2. 模型架构 经验规则+系统设计\n1.2. 统计机器学习阶段(1993~2012) 1.2.1. 阶段特点 机器翻译系统可拆成语言模型和翻译模型。该阶段相比上一阶段突变性较高，由人转述知识变成机器自动从数据中学习知识，主流技术包括SVM、HMM、MaxEnt、CRF、LM等，当时人工标注数据量在百万级左右。\n语言模型(LM)是根据句子一部分来预测下一个词。语言模型训练采用交叉熵Loss，评估采用困惑度(Perplexity，PPL)，困惑度越小说明模型准确预测下一个词的把握越大：\n语言模型比较常见的应用是输入法提示：\n而大语言模型(LLM)最显著的特点就是训练数据量大、模型参数量大。大多数知识都能用自然语言来描述，互联网文本量大且蕴含大量知识，因此用语言模型来学习知识是个自然而又天才的想法。\n1.2.2. 模型架构 浅层模型：\n1.3. 深度学习阶段(2013~2018) 1.3.1. 阶段特点 相对上一阶段突变性较低，从离散匹配发展到embedding连续匹配，模型变得更大。该阶段典型技术栈包括Encoder-Decoder、LSTM、Attention、Embedding等，标注数据量提升到千万级。该阶段特点是以神经网络来做表征，下图是经典的词表征学习word2vec结构：\n1.3.2. 模型架构 该阶段模型架构主要为了解决具体NLP任务。NLP任务分为自然语言理解(NLU)和自然语言生成(NLG)两大类。\nNLU任务一般是对语言基础信息的理解，比如命名实体识别、句法分析、分词、语义角色标注等任务。不同任务会有不同的结构，例如利用stack LSTM解决依存句法分析任务：\nNLG任务一般是语言生成任务，比如生成式摘要，对话生成等任务。基础架构一般是encoder-decoder，例如较早结合encoder-decoder+attention解决机器翻译的工作：\n这一阶段，NLP的重心是为每个任务设计合适的模型结构提高任务指标。\n1.4. 预训练阶段(2018~2022) 1.4.1. 阶段特点 相比之前的最大变化是加入自监督学习。该阶段系统可分为预训练和微调两个阶段，将预训练数据量扩大3到5倍，典型技术栈包括Encoder-Decoder、Transformer、Attention等。\n1.4.2. 模型架构 word2vec只是利用了词共现，每个词的表示还是唯一的。但每个词在不同上下文环境下语义差别很大，例如\"苹果”既可以表示水果，也可以表示公司，用同一个词向量表示显然是不合理的。此时动态词向量技术应运而生，例如ELMo，利用双向LSTM生成动态词向量。GPT和BERT也是预训练技术的重要代表工作，我们将在后文介绍。\n1.5. 大语言模型阶段(2023~) 1.5.1. 阶段特点 从2023年起，目的是让机器能听懂人的命令、遵循人的价值观。其特性是在第一个阶段把过去的两个阶段缩成一个预训练阶段，第二阶段转换成与人的价值观对齐，而不是向领域迁移。这个阶段的突变性是很高的，已经从专用任务转向通用任务，或是以自然语言人机接口的方式呈现。\n1.5.2. 模型架构 基于Transformer架构的组合和演化，衍生出了包括仅编码器（Encoder Only）、仅解码器（Decoder Only）以及编解码器结合（Encoder-Decoder）等多种架构。虽然从严格意义上来讲，目前的主流大型模型架构并不包括仅编码器模式，但为了完整性考虑，本文后续部分也将详细阐述该架构。\n2. LLM架构基石-Transformer 2.1. 整体架构 整体是一个encoder-decoder框架：encoder主体由多层self-attention构成，decoder相比encoder多了Encoder-Decoder Attention。\n2.2. Encoder 2.2.1. Self-Attention 2.2.1.1. Overview 2.2.1.2. Detail 矩阵计算视角：\n关键设计：Scale\n为什么要做Scale? Score是由两个向量内积得到，容易产生较大的值导致落入softmax函数梯度平缓区，容易导致梯度消失。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from scipy.special import softmax import numpy as np def test_gradient(dim, time_steps=50, scale=1.0): # Assume components of the query and keys are drawn from N(0, 1) independently q = np.random.randn(dim) ks = np.random.randn(time_steps, dim) x = np.sum(q * ks, axis=1) / scale # x.shape = (time_steps,) y = softmax(x) grad = np.diag(y) - np.outer(y, y) return np.max(np.abs(grad)) # the maximum component of gradients NUMBER_OF_EXPERIMENTS = 5 # results of 5 random runs without scaling print([test_gradient(100) for _ in range(NUMBER_OF_EXPERIMENTS)]) print([test_gradient(1000) for _ in range(NUMBER_OF_EXPERIMENTS)]) # results of 5 random runs with scaling print([test_gradient(100, scale=np.sqrt(100)) for _ in range(NUMBER_OF_EXPERIMENTS)]) print([test_gradient(1000, scale=np.sqrt(1000)) for _ in range(NUMBER_OF_EXPERIMENTS)]) 不scale结果：\n1 2 [0.012874944171283764, 0.24825918021760482, 0.0024202364125776032, 0.23220639834592616, 0.002987739995810368] [0.05744804282820459, 1.3786222829992312e-07, 1.3083745276532e-05, 0.016805764159904646, 1.7290933418401266e-07] scale后结果：\n1 2 [0.10416117163013226, 0.12557648647665576, 0.10035918562130762, 0.18524244407243343, 0.08591912714184714] [0.08396345043318147, 0.09472133857223597, 0.11776894854413103, 0.12306855963215982, 0.11043084033677895] 为什么是$\\sqrt{d_k}$ 根据假设$q_i$和$k_i$是两个相互独立，且均值为0，方差为1的随机变量，那么有：\n因此$QK^T$除以$\\sqrt{d_k}$可以将方差纠正为接近1，这样大部分值都在合理的区间了。\n2.2.1.3. MultiHead 引入MultiHead有两点好处：\n不同head可以重点关注不同位置的信息。 增加了Attention Layer的表示子空间。 筛选其中2个head结果\t8个head完整结果\n2.2.2. Position Embedding 上述self-Attention结构没有建模位置信息，而“顺序”对于NLP至关重要，比如“吃饭不”和“不吃饭”句子含义差异很大。\n计算公式：\n可视化：\n位置编码设计成这样有以下几个考虑：\nsin/cos自变量范围较小，很难重复，基本可以保障编码的唯一性。 对输入长度没限制，可以无限扩展 位置编码不仅表示绝对位置，也蕴含相对位置。 pos+k的位置编码可以由pos位置和k位置线性组合得到。\n问题1：为什么同时采用sin和cos？\n个人认为是为了利用三角函数特性，能够蕴含相对位置。\n问题2：sin-cos编码比绝对位置embedding更好么？\n在文中提过绝对位置Embedding和sin-cos位置编码效果类似，具体还需要根据任务效果来选择。比如BERT中就采用了绝对位置Embedding。\n2.2.3. Add \u0026 Normalize 残差和LN都能帮助缓解梯度消失问题，下面是一个实践经验：\n我们分析梯度值发现multi-head target attention参数的梯度值量级非常小，影响模型收敛速度。layer norm可以实现对梯度的平移和缩放，加入layer norm后梯度值被放大，最终带来1～2千分点的提升。\na) Multi-head Target Attention\nb) Multi-head Target Attention With Layer Norm\n2.3. Decoder Decoder与Encoder架构上有几个区别：\ndecoder多了个Encoder-Decoder Attention，为了在生成的时候结合输入信息。具体操作是Encoder最后一层表达生成$K$和$V$, 然后Decoder生成$Q$, 其他操作就是Multihead Attention。 decoder时self-attention只能用前面已生成的序列 解码时一般有greedy和beam search两种方式。 3. LLM架构范式 3.1. Encoder-Only Encoder-Only架构又称之为自编码(AutoEncoder)\n3.1.1. BERT 3.1.1.1. 动机 在BERT(Bidirectional Encoder Representations from Transformers)出来前，动态词表示有两个研究分支：一是feature-based，代表工作为ELMo(Embeddings from Language Models)，利用双向LSTM语言模型建模词表示，作为下游任务的特征使用。二是fine-tuning，代表工作是GPT(Generative Pre-trained Transformer)，利用单向Transformer训练语言模型，与下游任务配合fine-tune。\nELMo和GPT在训练的时候都是单向语言模型，用上文或者下文来预测下个词。上下文理解能力对很多任务至关重要，BERT提出MLM (Masked Language Model)建模上下文理解能力。简单来说，就是随机挖掉中间词，让模型通过上下文信息预测中间词的方式建模上下文信息，而Transformer本身就具备每个位置看到所有位置信息的能力。\n3.1.1.2. 两个训练任务 任务1：MLM\n引入两个特殊token：[CLS]代表分类符，训练NSP和下游Fine-tuning时都会用到；[MASK]代表该位置token被mask掉。\n核心设计-Mask：随机mask 15%的词级别tokens，该位置替换为[MASK]标识，让模型根据上下文预测该词。但是考虑到下游应用并没有[MASK]标识，因此将随机选中的tokens以100%概率替换为[MASK]，10%概率保留原词，10%概率替换为随机词。\n问题1：为什么不保留100%的[MASK]，而只保留100%呢？\n除了文中提到的下游应用没有[MASK]带来不一致，另外也避免[MASK]只学习到训练语料词的分布，比如训练数据有20%的词都是Is，那么[Mask]也倾向于有20%概率预测Is。\n问题2：为什么随机替换随机词？会对影响训练么？\n保留随机词有两个好处：一是增加模型鲁棒性，当见到脏数据模型也有能力做预测；二是会迫使模型利用上下文进行预测当前词。这类数据占比15% x 10% = 1.5%，数量很少，对模型训练影响不大。\n不同Mask策略实验结果如下：\n任务2：NSP (next sentence prediction)\n引入句子分隔符[SEP]\n有些任务需要句子级别依赖，例如阅读理解。为了建模长句子依赖，BERT引入NSP任务，判断B句子是不是A句子的下文。A句子下文50%概率保留原文作为正样本，50%概率随机替换别的句子作为负样本。\n3.1.1.3. BERT For fine-tuning BERT所有参数都会参与下游任务联合训练。\n3.1.1.4. BERT for feature extraction 只拼接最后4层就能获取很高的准确率。\n3.2. Decoder-Only Decoder-Only架构又称为自回归(AutoRegressive)。\n3.2.1. Causal Decoder 因果解码器架构采用单向注意力掩码，以确保每个输入 token 只能关注过去的 token 和它本身。输入和输出token通过解码器以相同的方式进行处理。代表工作是GPT系列。\n更好理解GPT架构执行流程，可以参考一个可视化网站：https://bbycroft.net/llm\n3.2.1.1. GPT-1：半监督学习 GPT-1是通过半监督学习方式解决NLP任务，即无监督的预训练+有监督的微调。\nGPT-1是一个基于Transformer的前向语言模型。在生成下一个单词时，Transformer只能读到左边已生成的部分。\n由于每个位置只能看到前面的序列，因此采用了Masked Self-Attention，这和BERT将不可见词替换为[MASK]标志不同。\n计算到第2个位置时，第一个位置产生的K和V可以复用，无需重复产出。\n其Fine-tune的模式如下：\n有两个关键设计：\n由于预训练阶段没有区分句子顺序能力，例如A句子后接B句子，那么语义上可能会学到B是A的下文，而对于句子相似度任务这种顺序会影响模型判断，因此同时保留A-B和B-A两种输入关系。 fine-tuning时也引入语言模型任务作为辅助任务，既能提高模型泛化能力，也能加速任务收敛速度。 GPT-1开始关注到decoder-only架构在解决zero-shot问题有一定优势，为GPT-2埋下伏笔。\n3.2.1.2. GPT-2：多任务学习 当时主流的NLP范式是预训练+微调，这种方式成本较高，而且缺乏泛化能力，每个新任务都需要标注数据进行fine-tune。GPT-2的核心尝试不微调，直接通过预训练学习多任务能力，无需微调！GPT-2有两个核心设计：一是将任务描述也作为模型的输入(即prompt)，二是大力出奇迹。\n将任务作为模型的输入是一个天才的想法，是NLP范式的一个巨大转变，核心思路一个公式就能讲清楚：\n对于翻译任务，输入可以定义为: (translate to french, english text, french text)\n对于阅读理解任务，输入可以定义为：(answer the question, document, question, answer)\n….\n从架构上，GPT-2与GPT-1一致，参数量增加了10几倍。GPT-1是117M，GPT-2是1.5B。从各种数据集合的表现来看，效果随着参数量增加也显著提升。\n以117M模型为例，每一层的decoder参数如下：\n整体的参数量计算如下：\n最终计算出124M，和论文提到的117M有一些出入。\n小知识：GPT输入最小粒度为什么是token，而不是word呢？\nGPT采用的输入切分方式是BPE(Byte Pair Encoding)分词：通过不断地合并出现频率高的子词，BPE分词算法可以得到一组细粒度的子词单元，这些子词单元可以更好地表示原始文本中的词汇和词组。由于BPE分词算法是基于频率的统计方法，它可以自适应地根据特定文本的特点生成合适的子词单元，从而在不同的自然语言处理任务中取得良好的效果。所以输入粒度并不是word概念。\n3.2.1.3. GPT-3：上下文学习 GPT-2通过将任务描述加入到输入中，让模型能够根据描述解决特定任务。但是这种做法上限较低，即使是人在解决新问题时也很难处理好。但有几个示例后，人能够根据知识和示例快速学习。GPT-3的提出就是这种动机，核心设计有两个：一是引入上下文学习(In-context learning, ICL)，大大提高了模型能力，尤其是处理复杂任务的能力；二是极致的暴力美学。\n我们再回顾下这句话“人能够根据知识和示例快速学习”，这种根据知识和示例快速学习的想法并不是第一次提出。Meta-Learning有异曲同工之妙。假设有一个 task 的分布，我们从这个分布中采样了许多 task 作为训练集。我们希望 meta learning 模型在这个训练集上训练后，能在这个分布中所有的 task 上都有良好的表现，即使是从来没见过的 task。\nMAML是比较经典的工作，目的是学习一个好的初始化，使得模型能快速适应新任务：\n其迭代分为内循环和外循环两个框架：\n外循环负责筛选一批任务进行一次梯度更新，内循环负责为每个任务计算更新梯度，但不会立马更新该任务梯度，而是在外循环中一次更新多个任务的梯度。这样的好处就是避免参数过于偏向最近训练的任务。\n回到GPT-3上来，上下文分为三种：Zero-shot、One-shot和Few-shot：\n对应到Meta-Learning框架上：\n不过GPT-3相比Meta-Learning是个更优雅的范式。面对新任务时，无需训练参数，只需要在输入给任务描述和少量示例即可。\nGPT-3另一个特点是极致的暴力美学：\n有两个发现：一是参数量对模型性能有极大影响，而是one-/few-shot对参数量大的模型性能提升更显著。\n原文细节非常多，这里不再赘述。\n3.2.2. Prefix Decoder 前缀解码器架构(也称非因果解码器架构)修正了因果解码器的掩码机制，以使其能够对前缀token执行双向注意力，并仅对生成的 token 执行单向注意力。代表工作是GLM(General Language Model)系列。\n回归模型从左到右学习语言模型，适合于长文本生成和少样本学习，但不能捕捉上下文词之间的双向依赖关系。\n自编码模型通过去噪目标学习双向上下文编码器，适合于自然语言理解任务，但不能直接用于文本生成。\n编码器-解码器模型结合了双向注意力和单向注意力，适合于有条件的生成任务，如文本摘要和回复生成。\n这三类语言模型各有优缺点，但没有一种框架能够在所有的自然语言处理任务中都表现出色。因此清华提出一个通用架构，想结合这三种框架的优点。\nGLM有两个关键设计：一是随机挖掉若干个子序列，并打乱顺序做生成；二是二维位置编码。位置也是为了前者服务。\n本质上GLM还是个自回归模式，只是预测的并不是句子的下个词，而是中间的序列。并且通过打乱机制让模型学到上下文能力。\n3.3. Encoder-Decoder Encoder-Decoder的代表工作BART，T5和Switch Transformer等，这里只介绍BART。\nBART(Bidirectional and Auto-Regressive Transformers)设计如下：\n除了上述架构，还有个比较重要的点是对输入加了噪声。\n3.4. MOE 对于上述架构，我们可以通过专家混合（MoE）扩展来进一步扩展它们，在这种扩展中，每个输入的一部分神经网络权重被稀疏激活，例如Switch Transforme。MoE的主要优点是它是一种灵活的方式来扩大模型参数，同时保持恒定的计算成本。有研究表明，通过增加专家数量或总参数大小可以观察到显著的性能改进。尽管有这些优点，但由于路由操作的复杂、硬切换特性，训练大型MoE模型可能会遇到不稳定性问题。\n3.5. 架构选择总结 目前主流的LLM架构是Causal Decoder，目前还无法从理论上证明这种架构的优越性。\n4. 参考文献 Apertium: a free/open-source platform for rule-based machine translation An Introduction to Conditional Random Fields for Relational Learning Neural Machine Translation by Jointly Learning to Align and Translate. The Illustrated Transformer The Illustrated BERT, ELMo, and co. The Illustrated GPT-2 如何理解Transformer论文中的positional encoding Meta Learning：一种套娃算法 Transformer的Score为什么Scale ChatGPT调研报告 AIGC：从不存在到存在 清华大学通用预训练模型：GLM A Survey of Large Language Models Transition-Based Dependency Parsing with Stack Long Short-Term Memory Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond Efficient Estimation of Word Representations in Vector Space BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Deep contextualized word representations Improving Language Understanding by Generative Pre-Training Language Models are Unsupervised Multitask Learners Language Models are Few-Shot Learners LLaMA: Open and Efficient Foundation Language Models Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks GLM: General Language Model Pretraining with Autoregressive Blank Infilling All NLP Tasks Are Generation Tasks: A General Pretraining Framework BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer ","wordCount":"6908","inLanguage":"en","datePublished":"2023-10-20T01:03:53+08:00","dateModified":"2023-10-20T01:03:53+08:00","author":[{"@type":"Person","name":"Huaipeng Zhao"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://hpzhao.github.io/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/"},"publisher":{"@type":"Organization","name":"Huaipeng's Blog","logo":{"@type":"ImageObject","url":"https://hpzhao.github.io/img/logo.gif"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://hpzhao.github.io/ accesskey=h title="Huaipeng's Blog (Alt + H)">Huaipeng's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://hpzhao.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://hpzhao.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://hpzhao.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://hpzhao.github.io/tags title=Tags><span>Tags</span></a></li><li><a href=https://hpzhao.github.io/about title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://hpzhao.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://hpzhao.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">大模型架构</h1><div class=post-meta><span title='2023-10-20 01:03:53 +0800 +0800'>2023-10-20</span>&nbsp;·&nbsp;<span>14 min</span>&nbsp;·&nbsp;<span>Huaipeng Zhao</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-nlp%e5%8f%91%e5%b1%95%e5%8e%86%e7%a8%8b%e4%b8%8e%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84 aria-label="1. NLP发展历程与模型架构">1. NLP发展历程与模型架构</a><ul><li><a href=#11-%e8%a7%84%e5%88%99%e9%98%b6%e6%ae%b519561992 aria-label="1.1. 规则阶段(1956～1992)">1.1. 规则阶段(1956～1992)</a><ul><li><a href=#111-%e9%98%b6%e6%ae%b5%e7%89%b9%e7%82%b9 aria-label="1.1.1. 阶段特点">1.1.1. 阶段特点</a></li><li><a href=#112-%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84 aria-label="1.1.2. 模型架构">1.1.2. 模型架构</a></li></ul></li><li><a href=#12-%e7%bb%9f%e8%ae%a1%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e9%98%b6%e6%ae%b519932012 aria-label="1.2. 统计机器学习阶段(1993~2012)">1.2. 统计机器学习阶段(1993~2012)</a><ul><li><a href=#121-%e9%98%b6%e6%ae%b5%e7%89%b9%e7%82%b9 aria-label="1.2.1. 阶段特点">1.2.1. 阶段特点</a></li><li><a href=#122-%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84 aria-label="1.2.2. 模型架构">1.2.2. 模型架构</a></li></ul></li><li><a href=#13-%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e9%98%b6%e6%ae%b520132018 aria-label="1.3. 深度学习阶段(2013~2018)">1.3. 深度学习阶段(2013~2018)</a><ul><li><a href=#131-%e9%98%b6%e6%ae%b5%e7%89%b9%e7%82%b9 aria-label="1.3.1. 阶段特点">1.3.1. 阶段特点</a></li><li><a href=#132-%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84 aria-label="1.3.2. 模型架构">1.3.2. 模型架构</a></li></ul></li><li><a href=#14-%e9%a2%84%e8%ae%ad%e7%bb%83%e9%98%b6%e6%ae%b520182022 aria-label="1.4. 预训练阶段(2018~2022)">1.4. 预训练阶段(2018~2022)</a><ul><li><a href=#141-%e9%98%b6%e6%ae%b5%e7%89%b9%e7%82%b9 aria-label="1.4.1. 阶段特点">1.4.1. 阶段特点</a></li><li><a href=#142-%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84 aria-label="1.4.2. 模型架构">1.4.2. 模型架构</a></li></ul></li><li><a href=#15-%e5%a4%a7%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e9%98%b6%e6%ae%b52023 aria-label="1.5. 大语言模型阶段(2023~)">1.5. 大语言模型阶段(2023~)</a><ul><li><a href=#151-%e9%98%b6%e6%ae%b5%e7%89%b9%e7%82%b9 aria-label="1.5.1. 阶段特点">1.5.1. 阶段特点</a></li><li><a href=#152-%e6%a8%a1%e5%9e%8b%e6%9e%b6%e6%9e%84 aria-label="1.5.2. 模型架构">1.5.2. 模型架构</a></li></ul></li></ul></li><li><a href=#2-llm%e6%9e%b6%e6%9e%84%e5%9f%ba%e7%9f%b3-transformer aria-label="2. LLM架构基石-Transformer">2. LLM架构基石-Transformer</a><ul><li><a href=#21-%e6%95%b4%e4%bd%93%e6%9e%b6%e6%9e%84 aria-label="2.1. 整体架构">2.1. 整体架构</a></li><li><a href=#22-encoder aria-label="2.2. Encoder">2.2. Encoder</a><ul><li><a href=#221-self-attention aria-label="2.2.1. Self-Attention">2.2.1. Self-Attention</a><ul><li><a href=#2211-overview aria-label="2.2.1.1. Overview">2.2.1.1. Overview</a></li><li><a href=#2212-detail aria-label="2.2.1.2. Detail">2.2.1.2. Detail</a></li><li><a href=#2213-multihead aria-label="2.2.1.3. MultiHead">2.2.1.3. MultiHead</a></li></ul></li><li><a href=#222-position-embedding aria-label="2.2.2. Position Embedding">2.2.2. Position Embedding</a></li><li><a href=#223-add--normalize aria-label="2.2.3. Add & Normalize">2.2.3. Add & Normalize</a></li></ul></li><li><a href=#23-decoder aria-label="2.3. Decoder">2.3. Decoder</a></li></ul></li><li><a href=#3-llm%e6%9e%b6%e6%9e%84%e8%8c%83%e5%bc%8f aria-label="3. LLM架构范式">3. LLM架构范式</a><ul><li><a href=#31-encoder-only aria-label="3.1. Encoder-Only">3.1. Encoder-Only</a><ul><li><a href=#311-bert aria-label="3.1.1. BERT">3.1.1. BERT</a><ul><li><a href=#3111-%e5%8a%a8%e6%9c%ba aria-label="3.1.1.1. 动机">3.1.1.1. 动机</a></li><li><a href=#3112-%e4%b8%a4%e4%b8%aa%e8%ae%ad%e7%bb%83%e4%bb%bb%e5%8a%a1 aria-label="3.1.1.2. 两个训练任务">3.1.1.2. 两个训练任务</a></li><li><a href=#3113-bert-for-fine-tuning aria-label="3.1.1.3. BERT For fine-tuning">3.1.1.3. BERT For fine-tuning</a></li><li><a href=#3114-bert-for-feature-extraction aria-label="3.1.1.4. BERT for feature extraction">3.1.1.4. BERT for feature extraction</a></li></ul></li></ul></li><li><a href=#32-decoder-only aria-label="3.2. Decoder-Only">3.2. Decoder-Only</a><ul><li><a href=#321-causal-decoder aria-label="3.2.1. Causal Decoder">3.2.1. Causal Decoder</a><ul><li><a href=#3211-gpt-1%e5%8d%8a%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0 aria-label="3.2.1.1. GPT-1：半监督学习">3.2.1.1. GPT-1：半监督学习</a></li><li><a href=#3212-gpt-2%e5%a4%9a%e4%bb%bb%e5%8a%a1%e5%ad%a6%e4%b9%a0 aria-label="3.2.1.2. GPT-2：多任务学习">3.2.1.2. GPT-2：多任务学习</a></li><li><a href=#3213-gpt-3%e4%b8%8a%e4%b8%8b%e6%96%87%e5%ad%a6%e4%b9%a0 aria-label="3.2.1.3. GPT-3：上下文学习">3.2.1.3. GPT-3：上下文学习</a></li></ul></li><li><a href=#322-prefix-decoder aria-label="3.2.2. Prefix Decoder">3.2.2. Prefix Decoder</a></li></ul></li><li><a href=#33-encoder-decoder aria-label="3.3. Encoder-Decoder">3.3. Encoder-Decoder</a></li><li><a href=#34-moe aria-label="3.4. MOE">3.4. MOE</a></li><li><a href=#35-%e6%9e%b6%e6%9e%84%e9%80%89%e6%8b%a9%e6%80%bb%e7%bb%93 aria-label="3.5. 架构选择总结">3.5. 架构选择总结</a></li></ul></li><li><a href=#4-%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae aria-label="4. 参考文献">4. 参考文献</a></li></ul></div></details></div><div class=post-content><h1 id=1-nlp发展历程与模型架构>1. NLP发展历程与模型架构<a hidden class=anchor aria-hidden=true href=#1-nlp发展历程与模型架构>#</a></h1><div align=center><img src=/img/1701143310613-4745c04a-1690-474a-8242-d98d0d54b07f.png alt=img style=zoom:40%></div><p>NLP的发展渐趋于统一化：在深度学习时代到来之前，NLP任务高度依赖于手工设计的复杂特征。随后，深度学习的出现极大地减轻了这种特征工程的负担。以BERT和GPT-1为代表的预训练和微调范式，标志着手工特征设计时代的终结。大模型的出现宣告了NLP中间任务的消亡，所有任务都可以统一到语言模型的范畴中。</p><h2 id=11-规则阶段19561992>1.1. 规则阶段(1956～1992)<a hidden class=anchor aria-hidden=true href=#11-规则阶段19561992>#</a></h2><h3 id=111-阶段特点>1.1.1. 阶段特点<a hidden class=anchor aria-hidden=true href=#111-阶段特点>#</a></h3><p>基于规则的机器翻译系统是在内部把各种功能的模块串到一起，由人先从数据中获取知识，归纳出规则，写出来教给机器，然后机器来执行这套规则，从而完成特定任务。</p><h3 id=112-模型架构>1.1.2. 模型架构<a hidden class=anchor aria-hidden=true href=#112-模型架构>#</a></h3><p>经验规则+系统设计</p><div align=center><img src=/img/1706162818733-99a7db53-aefe-4f08-a5fa-f6fa51c1c6bf.png alt=img style=zoom:100%></div><h2 id=12-统计机器学习阶段19932012>1.2. <strong>统计机器学习阶段</strong>(1993~2012)<a hidden class=anchor aria-hidden=true href=#12-统计机器学习阶段19932012>#</a></h2><h3 id=121-阶段特点>1.2.1. 阶段特点<a hidden class=anchor aria-hidden=true href=#121-阶段特点>#</a></h3><p>机器翻译系统可拆成语言模型和翻译模型。该阶段相比上一阶段突变性较高，由人转述知识变成机器自动从数据中学习知识，主流技术包括SVM、HMM、MaxEnt、CRF、LM等，当时人工标注数据量在百万级左右。</p><p>语言模型(LM)是根据句子一部分来预测下一个词。语言模型训练采用交叉熵Loss，评估采用困惑度(Perplexity，<strong>PPL</strong>)，困惑度越小说明模型准确预测下一个词的把握越大：</p><div align=center><img src=/img/3c83eeb7211de6f770556fc494720b86.svg alt=img width=60%></div><p>语言模型比较常见的应用是输入法提示：</p><div align=center><img src=/img/1701934193795-1d42283d-c279-4a9b-a33c-d77648aa2b24.jpeg alt=img style=zoom:33%></div><p>而大语言模型(LLM)最显著的特点就是训练数据量大、模型参数量大。大多数知识都能用自然语言来描述，互联网文本量大且蕴含大量知识，因此用语言模型来学习知识是个自然而又天才的想法。</p><h3 id=122-模型架构>1.2.2. 模型架构<a hidden class=anchor aria-hidden=true href=#122-模型架构>#</a></h3><p>浅层模型：</p><div align=center><img src=/img/1706166081247-b744b377-d81b-4ecf-ba17-bdd0de30b03d.png alt=img style=zoom:80%></div><h2 id=13-深度学习阶段20132018>1.3. <strong>深度学习阶段</strong>(2013~2018)<a hidden class=anchor aria-hidden=true href=#13-深度学习阶段20132018>#</a></h2><h3 id=131-阶段特点>1.3.1. 阶段特点<a hidden class=anchor aria-hidden=true href=#131-阶段特点>#</a></h3><p>相对上一阶段突变性较低，从离散匹配发展到embedding连续匹配，模型变得更大。该阶段典型技术栈包括Encoder-Decoder、LSTM、Attention、Embedding等，标注数据量提升到千万级。该阶段特点是以神经网络来做表征，下图是经典的词表征学习word2vec结构：</p><div align=center><img src=/img/1701328236008-d7e5b1ee-6318-4b82-ac5b-f989f9d2932d.png alt=img style=zoom:33%></div><h3 id=132-模型架构>1.3.2. 模型架构<a hidden class=anchor aria-hidden=true href=#132-模型架构>#</a></h3><p>该阶段模型架构主要为了解决具体NLP任务。NLP任务分为自然语言理解(<strong>NLU</strong>)和自然语言生成(<strong>NLG</strong>)两大类。</p><p>NLU任务一般是对语言基础信息的理解，比如命名实体识别、句法分析、分词、语义角色标注等任务。不同任务会有不同的结构，例如利用stack LSTM解决依存句法分析任务：</p><div align=center><img src=/img/1701332606777-ed83660d-124c-4a2c-ba87-b453904706e4.png alt=img style=zoom:33%></div><p>NLG任务一般是语言生成任务，比如生成式摘要，对话生成等任务。基础架构一般是encoder-decoder，例如较早结合encoder-decoder+attention解决机器翻译的工作：</p><div align=center><img src=/img/1706166980865-163bfc6f-1cc7-4f00-a546-e582da54302b.png alt=img style=zoom:50%></div><p>这一阶段，NLP的重心是为每个任务设计合适的模型结构提高任务指标。</p><h2 id=14-预训练阶段20182022>1.4. <strong>预训练阶段</strong>(2018~2022)<a hidden class=anchor aria-hidden=true href=#14-预训练阶段20182022>#</a></h2><h3 id=141-阶段特点>1.4.1. 阶段特点<a hidden class=anchor aria-hidden=true href=#141-阶段特点>#</a></h3><p>相比之前的最大变化是加入自监督学习。该阶段系统可分为预训练和微调两个阶段，将预训练数据量扩大3到5倍，典型技术栈包括Encoder-Decoder、Transformer、Attention等。</p><h3 id=142-模型架构>1.4.2. 模型架构<a hidden class=anchor aria-hidden=true href=#142-模型架构>#</a></h3><p>word2vec只是利用了词共现，每个词的表示还是唯一的。但每个词在不同上下文环境下语义差别很大，例如"苹果”既可以表示水果，也可以表示公司，用同一个词向量表示显然是不合理的。此时动态词向量技术应运而生，例如ELMo，利用双向LSTM生成动态词向量。GPT和BERT也是预训练技术的重要代表工作，我们将在后文介绍。</p><div align=center><img src=/img/1701679146386-e84a5755-6059-43c2-8a3d-5074c31cc350.png alt=img style=zoom:50%></div><div align=center><img src=/img/1701679158743-6f8b193b-ae59-4f9b-bed1-843efffe8dd2.png alt=img style=zoom:50%></div><h2 id=15-大语言模型阶段2023>1.5. <strong>大语言模型阶段</strong>(2023~)<a hidden class=anchor aria-hidden=true href=#15-大语言模型阶段2023>#</a></h2><h3 id=151-阶段特点>1.5.1. 阶段特点<a hidden class=anchor aria-hidden=true href=#151-阶段特点>#</a></h3><p>从2023年起，目的是让机器能听懂人的命令、遵循人的价值观。其特性是在第一个阶段把过去的两个阶段缩成一个预训练阶段，第二阶段转换成与人的价值观对齐，而不是向领域迁移。这个阶段的突变性是很高的，已经从专用任务转向通用任务，或是以自然语言人机接口的方式呈现。</p><h3 id=152-模型架构>1.5.2. 模型架构<a hidden class=anchor aria-hidden=true href=#152-模型架构>#</a></h3><p>基于Transformer架构的组合和演化，衍生出了包括仅编码器（Encoder Only）、仅解码器（Decoder Only）以及编解码器结合（Encoder-Decoder）等多种架构。虽然从严格意义上来讲，目前的主流大型模型架构并不包括仅编码器模式，但为了完整性考虑，本文后续部分也将详细阐述该架构。</p><div align=center><img src=/img/1706167424389-c0fda78f-e0b4-4d62-bf11-d8fb917978f9.png alt=img style=zoom:50%></div><h1 id=2-llm架构基石-transformer>2. LLM架构基石-Transformer<a hidden class=anchor aria-hidden=true href=#2-llm架构基石-transformer>#</a></h1><h2 id=21-整体架构>2.1. 整体架构<a hidden class=anchor aria-hidden=true href=#21-整体架构>#</a></h2><div align=center><img src=/img/1701153374220-571d7ee9-9d61-4239-a818-25afebf86e2a.png alt=img style=zoom:33%></div><p>整体是一个encoder-decoder框架：encoder主体由多层self-attention构成，decoder相比encoder多了Encoder-Decoder Attention。</p><h2 id=22-encoder>2.2. Encoder<a hidden class=anchor aria-hidden=true href=#22-encoder>#</a></h2><h3 id=221-self-attention>2.2.1. Self-Attention<a hidden class=anchor aria-hidden=true href=#221-self-attention>#</a></h3><h4 id=2211-overview>2.2.1.1. Overview<a hidden class=anchor aria-hidden=true href=#2211-overview>#</a></h4><div align=center><img src=/img/1701153821370-77ea4d49-885a-4ce3-b909-6ea955b29295.png alt=img style=zoom:33%></div><h4 id=2212-detail>2.2.1.2. Detail<a hidden class=anchor aria-hidden=true href=#2212-detail>#</a></h4><div align=center><img src=/img/1701154149494-cee98016-5d60-4ef7-8414-1785b9266ac9.png alt=img style=zoom:50%></div><p>矩阵计算视角：</p><div align=center><img src=/img/1701154261582-4b4749a1-6e83-46c8-8756-bec68dea4b11.png alt=img style=zoom:50%></div><div align=center><img src=/img/1701154269454-ff6f24ad-f06a-41ed-8a5c-43339ef9c29c.png alt=img style=zoom:50%></div><p>关键设计：Scale</p><ol><li><strong>为什么要做Scale?</strong></li></ol><p>Score是由两个向量内积得到，容易产生较大的值导致落入softmax函数梯度平缓区，容易导致梯度消失。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>scipy.special</span> <span class=kn>import</span> <span class=n>softmax</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>test_gradient</span><span class=p>(</span><span class=n>dim</span><span class=p>,</span> <span class=n>time_steps</span><span class=o>=</span><span class=mi>50</span><span class=p>,</span> <span class=n>scale</span><span class=o>=</span><span class=mf>1.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># Assume components of the query and keys are drawn from N(0, 1) independently</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ks</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>time_steps</span><span class=p>,</span> <span class=n>dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>q</span> <span class=o>*</span> <span class=n>ks</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=o>/</span> <span class=n>scale</span>  <span class=c1># x.shape = (time_steps,) </span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>softmax</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>grad</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>diag</span><span class=p>(</span><span class=n>y</span><span class=p>)</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>outer</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>grad</span><span class=p>))</span>  <span class=c1># the maximum component of gradients</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>NUMBER_OF_EXPERIMENTS</span> <span class=o>=</span> <span class=mi>5</span>
</span></span><span class=line><span class=cl><span class=c1># results of 5 random runs without scaling</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>([</span><span class=n>test_gradient</span><span class=p>(</span><span class=mi>100</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>NUMBER_OF_EXPERIMENTS</span><span class=p>)])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>([</span><span class=n>test_gradient</span><span class=p>(</span><span class=mi>1000</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>NUMBER_OF_EXPERIMENTS</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># results of 5 random runs with scaling</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>([</span><span class=n>test_gradient</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=n>scale</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>100</span><span class=p>))</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>NUMBER_OF_EXPERIMENTS</span><span class=p>)])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>([</span><span class=n>test_gradient</span><span class=p>(</span><span class=mi>1000</span><span class=p>,</span> <span class=n>scale</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=mi>1000</span><span class=p>))</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>NUMBER_OF_EXPERIMENTS</span><span class=p>)])</span>
</span></span></code></pre></td></tr></table></div></div><p>不scale结果：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>[</span><span class=mf>0.012874944171283764</span><span class=p>,</span> <span class=mf>0.24825918021760482</span><span class=p>,</span> <span class=mf>0.0024202364125776032</span><span class=p>,</span> <span class=mf>0.23220639834592616</span><span class=p>,</span> <span class=mf>0.002987739995810368</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=mf>0.05744804282820459</span><span class=p>,</span> <span class=mf>1.3786222829992312e-07</span><span class=p>,</span> <span class=mf>1.3083745276532e-05</span><span class=p>,</span> <span class=mf>0.016805764159904646</span><span class=p>,</span> <span class=mf>1.7290933418401266e-07</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>scale后结果：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>[</span><span class=mf>0.10416117163013226</span><span class=p>,</span> <span class=mf>0.12557648647665576</span><span class=p>,</span> <span class=mf>0.10035918562130762</span><span class=p>,</span> <span class=mf>0.18524244407243343</span><span class=p>,</span> <span class=mf>0.08591912714184714</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>[</span><span class=mf>0.08396345043318147</span><span class=p>,</span> <span class=mf>0.09472133857223597</span><span class=p>,</span> <span class=mf>0.11776894854413103</span><span class=p>,</span> <span class=mf>0.12306855963215982</span><span class=p>,</span> <span class=mf>0.11043084033677895</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><ol><li><strong>为什么是$\sqrt{d_k}$</strong></li></ol><p>根据假设$q_i$和$k_i$是两个相互独立，且均值为0，方差为1的随机变量，那么有：</p><div align=center><img src=/img/06bce98d3250da8d0458bb00a97d3ee1.svg alt=img width=80%></div><div align=center><img src=/img/b2c772ca67006eab45985d3a1451352f.svg alt=img width=40%></div><p>因此$QK^T$除以$\sqrt{d_k}$可以将方差纠正为接近1，这样大部分值都在合理的区间了。</p><h4 id=2213-multihead>2.2.1.3. MultiHead<a hidden class=anchor aria-hidden=true href=#2213-multihead>#</a></h4><div align=center><img src=/img/1701157892323-66c38fd1-3548-4265-b078-a299887f11d3.png alt=img style=zoom:50%></div><p>引入MultiHead有两点好处：</p><ol><li>不同head可以重点关注不同位置的信息。</li><li>增加了Attention Layer的表示子空间。</li></ol><div align=center><img src=/img/1701157934247-e8c9c087-dd83-444a-a450-eeb798669ae5.png alt=img width=100%></div><div align=center><img src=/img/1701157942066-ca7f687f-6347-439b-a304-b93b867127e7.png alt=img width=100%></div><p>筛选其中2个head结果 8个head完整结果</p><h3 id=222-position-embedding>2.2.2. Position Embedding<a hidden class=anchor aria-hidden=true href=#222-position-embedding>#</a></h3><p>上述self-Attention结构没有建模位置信息，而“顺序”对于NLP至关重要，比如“吃饭不”和“不吃饭”句子含义差异很大。</p><p><strong>计算公式：</strong></p><div align=center><img src=/img/1701158941253-d85b62f3-76ca-45b6-ab43-98d0d71c34c3.png alt=img style=zoom:25%></div><p><strong>可视化：</strong></p><div align=center><img src=/img/1701159234446-95849bd7-95ef-48aa-a70a-69f55ca7f11d.png alt=img style=zoom:67%></div><p>位置编码设计成这样有以下几个考虑：</p><ol><li>sin/cos自变量范围较小，很难重复，基本可以保障编码的唯一性。</li><li>对输入长度没限制，可以无限扩展</li><li>位置编码不仅表示绝对位置，也蕴含相对位置。</li></ol><div align=center><img src=/img/1701171541883-de1c779f-af7b-4245-a482-0830ea5d50b1.png alt=img style=zoom:33%></div><div align=center><img src=/img/1701171622556-e6d853cb-295d-4964-86e1-030e84e13000.png alt=img style=zoom:33%></div><p>pos+k的位置编码可以由pos位置和k位置线性组合得到。</p><p>问题1：为什么同时采用sin和cos？</p><p>个人认为是为了利用三角函数特性，能够蕴含相对位置。</p><p>问题2：sin-cos编码比绝对位置embedding更好么？</p><p>在文中提过绝对位置Embedding和sin-cos位置编码效果类似，具体还需要根据任务效果来选择。比如BERT中就采用了绝对位置Embedding。</p><h3 id=223-add--normalize>2.2.3. Add & Normalize<a hidden class=anchor aria-hidden=true href=#223-add--normalize>#</a></h3><div align=center><img src=/img/1701172170088-c486f772-fe98-480e-8122-4351b342616c.png alt=img style=zoom:67%></div><p>残差和LN都能帮助缓解梯度消失问题，下面是一个实践经验：</p><p>我们分析梯度值发现multi-head target attention参数的梯度值量级非常小，影响模型收敛速度。layer norm可以实现对梯度的平移和缩放，加入layer norm后梯度值被放大，最终带来1～2千分点的提升。</p><div align=center><img src=/img/1582774073917-7ed65197-7e91-4547-891a-19cfb1b97e19.png alt=img style=zoom:50%></div><div align=center><img src=/img/1582774003806-cbea2922-a24a-48e8-82c5-90cbd626a75c.png alt=img style=zoom:50%></div><p>a) Multi-head Target Attention</p><div align=center><img src=/img/1582774134959-3441d399-878c-46a0-a3eb-0bb6a562b253.png alt=img style=zoom:50%></div><div align=center><img src=/img/1582774153749-8b4c7de9-ae86-4f30-9620-4376d815ecb6.png alt=img style=zoom:50%></div><p>b) Multi-head Target Attention With Layer Norm</p><h2 id=23-decoder>2.3. Decoder<a hidden class=anchor aria-hidden=true href=#23-decoder>#</a></h2><div align=center><img src=/img/1701172896610-a064fea4-b57e-4e74-867c-1820de8f0fa7.png alt=img style=zoom:50%></div><p>Decoder与Encoder架构上有几个区别：</p><ol><li>decoder多了个Encoder-Decoder Attention，为了在生成的时候结合输入信息。具体操作是Encoder最后一层表达生成$K$和$V$, 然后Decoder生成$Q$, 其他操作就是Multihead Attention。</li><li>decoder时self-attention只能用前面已生成的序列</li><li>解码时一般有greedy和beam search两种方式。</li></ol><h1 id=3-llm架构范式>3. LLM架构范式<a hidden class=anchor aria-hidden=true href=#3-llm架构范式>#</a></h1><h2 id=31-encoder-only>3.1. Encoder-Only<a hidden class=anchor aria-hidden=true href=#31-encoder-only>#</a></h2><p>Encoder-Only架构又称之为自编码(AutoEncoder)</p><h3 id=311-bert>3.1.1. BERT<a hidden class=anchor aria-hidden=true href=#311-bert>#</a></h3><h4 id=3111-动机>3.1.1.1. 动机<a hidden class=anchor aria-hidden=true href=#3111-动机>#</a></h4><p>在<strong>BERT</strong>(<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers)出来前，动态词表示有两个研究分支：一是<strong>feature-based</strong>，代表工作为<strong>ELMo</strong>(<strong>E</strong>mbeddings from <strong>L</strong>anguage <strong>Mo</strong>dels)，利用双向LSTM语言模型建模词表示，作为下游任务的特征使用。二是<strong>fine-tuning</strong>，代表工作是<strong>GPT</strong>(<strong>G</strong>enerative <strong>P</strong>re-trained <strong>T</strong>ransformer)，利用单向Transformer训练语言模型，与下游任务配合fine-tune。</p><p>ELMo和GPT在训练的时候都是单向语言模型，用上文或者下文来预测下个词。上下文理解能力对很多任务至关重要，BERT提出<strong>MLM</strong> (<strong>M</strong>asked <strong>L</strong>anguage <strong>M</strong>odel)建模上下文理解能力。简单来说，就是随机挖掉中间词，让模型通过上下文信息预测中间词的方式建模上下文信息，而Transformer本身就具备每个位置看到所有位置信息的能力。</p><div align=center><img src=/img/1701687723820-1cea0063-4efd-43b4-b3b3-1f92933a9eb1.png alt=img width=100%></div><h4 id=3112-两个训练任务>3.1.1.2. 两个训练任务<a hidden class=anchor aria-hidden=true href=#3112-两个训练任务>#</a></h4><p><strong>任务1：MLM</strong></p><div align=center><img src=/img/1701688965769-bf822eb1-b967-427b-950e-739efe4564e6.png alt=img style=zoom:50%></div><p>引入两个特殊token：[CLS]代表分类符，训练NSP和下游Fine-tuning时都会用到；[MASK]代表该位置token被mask掉。</p><p><strong>核心设计-Mask</strong>：随机mask 15%的词级别tokens，该位置替换为[MASK]标识，让模型根据上下文预测该词。但是考虑到下游应用并没有[MASK]标识，因此将随机选中的tokens以100%概率替换为[MASK]，10%概率保留原词，10%概率替换为随机词。</p><p><strong>问题1</strong>：为什么不保留100%的[MASK]，而只保留100%呢？</p><p>除了文中提到的下游应用没有[MASK]带来不一致，另外也避免[MASK]只学习到训练语料词的分布，比如训练数据有20%的词都是Is，那么[Mask]也倾向于有20%概率预测Is。</p><p><strong>问题2</strong>：为什么随机替换随机词？会对影响训练么？</p><p>保留随机词有两个好处：一是增加模型鲁棒性，当见到脏数据模型也有能力做预测；二是会迫使模型利用上下文进行预测当前词。这类数据占比15% x 10% = 1.5%，数量很少，对模型训练影响不大。</p><p>不同Mask策略实验结果如下：</p><div align=center><img src=/img/1701689859439-c2a2a635-8767-43e2-a0cc-45a105922d86.png alt=img style=zoom:100%></div><p><strong>任务2：NSP</strong> (next sentence prediction)</p><p>引入句子分隔符[SEP]</p><p>有些任务需要句子级别依赖，例如阅读理解。为了建模长句子依赖，BERT引入NSP任务，判断B句子是不是A句子的下文。A句子下文50%概率保留原文作为正样本，50%概率随机替换别的句子作为负样本。</p><div align=center><img src=/img/1701690101695-639c6b05-652d-4293-9f68-924fd9494601.png alt=img style=zoom:80%></div><h4 id=3113-bert-for-fine-tuning>3.1.1.3. BERT For fine-tuning<a hidden class=anchor aria-hidden=true href=#3113-bert-for-fine-tuning>#</a></h4><p>BERT所有参数都会参与下游任务联合训练。</p><div align=center><img src=/img/1701690406934-66332ae2-3020-42d4-a23c-c7bd74c10cc9.png alt=img style=zoom:100%></div><h4 id=3114-bert-for-feature-extraction>3.1.1.4. BERT for feature extraction<a hidden class=anchor aria-hidden=true href=#3114-bert-for-feature-extraction>#</a></h4><p>只拼接最后4层就能获取很高的准确率。</p><div align=center><img src=/img/1701690617413-fb9f8dcc-a4fd-4b0c-b049-dc180020adbf.png alt=img style=zoom:50%></div><h2 id=32-decoder-only>3.2. Decoder-Only<a hidden class=anchor aria-hidden=true href=#32-decoder-only>#</a></h2><p>Decoder-Only架构又称为自回归(AutoRegressive)。</p><h3 id=321-causal-decoder>3.2.1. Causal Decoder<a hidden class=anchor aria-hidden=true href=#321-causal-decoder>#</a></h3><p>因果解码器架构采用单向注意力掩码，以确保每个输入 token 只能关注过去的 token 和它本身。输入和输出token通过解码器以相同的方式进行处理。代表工作是GPT系列。</p><p>更好理解GPT架构执行流程，可以参考一个可视化网站：https://bbycroft.net/llm</p><div align=center><img src=/img/1706170544960-bbfeeec1-1bfc-4ace-91fb-e5ed2e003577.png alt=img style=zoom:33%></div><h4 id=3211-gpt-1半监督学习>3.2.1.1. GPT-1：半监督学习<a hidden class=anchor aria-hidden=true href=#3211-gpt-1半监督学习>#</a></h4><p>GPT-1是通过半监督学习方式解决NLP任务，即<strong>无监督的预训练</strong>+<strong>有监督的微调</strong>。</p><p>GPT-1是一个基于Transformer的前向语言模型。在生成下一个单词时，Transformer只能读到左边已生成的部分。</p><div align=center><img src=/img/1701755864376-92e6c67c-2b7f-4712-813d-3d99e4dd9a2c.png alt=img style=zoom:50%></div><p>由于每个位置只能看到前面的序列，因此采用了Masked Self-Attention，这和BERT将不可见词替换为[MASK]标志不同。</p><div align=center><img src=/img/1701779763316-2cc02202-75d4-431f-9271-a416420ab816.png alt=img style=zoom:33%></div><div align=center><img src=/img/1701780235390-d9f91e17-aaa4-4bb7-baee-c565df3d9e21.png alt=img style=zoom:50%></div><p>计算到第2个位置时，第一个位置产生的K和V可以复用，无需重复产出。</p><p>其Fine-tune的模式如下：</p><div align=center><img src=/img/1701756134493-161c9fb4-3855-4a18-a536-fd60e394b158.png alt=img style=zoom:50%></div><p>有两个关键设计：</p><ol><li>由于预训练阶段没有区分句子顺序能力，例如A句子后接B句子，那么语义上可能会学到B是A的下文，而对于句子相似度任务这种顺序会影响模型判断，因此同时保留<strong>A-B</strong>和<strong>B-A</strong>两种输入关系。</li><li>fine-tuning时也引入语言模型任务作为辅助任务，既能提高模型泛化能力，也能加速任务收敛速度。</li></ol><p><strong>GPT-1开始关注到decoder-only架构在解决zero-shot问题有一定优势，为GPT-2埋下伏笔</strong>。</p><h4 id=3212-gpt-2多任务学习>3.2.1.2. GPT-2：多任务学习<a hidden class=anchor aria-hidden=true href=#3212-gpt-2多任务学习>#</a></h4><p>当时主流的NLP范式是预训练+微调，这种方式成本较高，而且缺乏泛化能力，每个新任务都需要标注数据进行fine-tune。GPT-2的核心尝试不微调，直接通过预训练学习多任务能力，无需微调！GPT-2有两个核心设计：一是将任务描述也作为模型的输入(即prompt)，二是大力出奇迹。</p><p>将任务作为模型的输入是一个天才的想法，是NLP范式的一个巨大转变，核心思路一个公式就能讲清楚：</p><div align=center><img src=/img/2b1eb3076101770a374a4205cebfb9a3.svg alt=img width=80%></div><p>对于翻译任务，输入可以定义为: (translate to french, english text, french text)</p><p>对于阅读理解任务，输入可以定义为：(answer the question, document, question, answer)</p><p>&mldr;.</p><p>从架构上，GPT-2与GPT-1一致，参数量增加了10几倍。GPT-1是117M，GPT-2是1.5B。从各种数据集合的表现来看，效果随着参数量增加也显著提升。</p><div align=center><img src=/img/1701949360804-2cb8f01b-a51d-4305-a12f-b7b6dc0dc235.png alt=img style=zoom:50%></div><p>以117M模型为例，每一层的decoder参数如下：</p><div align=center><img src=/img/1701949680553-e2c30976-a11d-4be4-bc47-8fda2010e3f3.png alt=img style=zoom:33%></div><p>整体的参数量计算如下：</p><div align=center><img src=/img/1701949849483-8310fffb-5806-4856-a7cc-01267a029ed4.png alt=img style=zoom:50%></div><div align=center><img src=/img/1701949860763-7cb7d644-0d1a-4261-9767-ff4e4f9297d4.png alt=img style=zoom:67%></div><p>最终计算出124M，和论文提到的117M有一些出入。</p><p>小知识：GPT输入最小粒度为什么是token，而不是word呢？</p><p>GPT采用的输入切分方式是BPE(Byte Pair Encoding)分词：通过不断地合并出现频率高的子词，BPE分词算法可以得到一组细粒度的子词单元，这些子词单元可以更好地表示原始文本中的词汇和词组。由于BPE分词算法是基于频率的统计方法，它可以自适应地根据特定文本的特点生成合适的子词单元，从而在不同的自然语言处理任务中取得良好的效果。所以输入粒度并不是word概念。</p><h4 id=3213-gpt-3上下文学习>3.2.1.3. GPT-3：上下文学习<a hidden class=anchor aria-hidden=true href=#3213-gpt-3上下文学习>#</a></h4><p>GPT-2通过将任务描述加入到输入中，让模型能够根据描述解决特定任务。但是这种做法上限较低，即使是人在解决新问题时也很难处理好。但有几个示例后，人能够<strong>根据知识和示例快速学习</strong>。GPT-3的提出就是这种动机，核心设计有两个：一是引入上下文学习(In-context learning, <strong>ICL</strong>)，大大提高了模型能力，尤其是处理复杂任务的能力；二是极致的暴力美学。</p><p>我们再回顾下这句话“人能够根据知识和示例快速学习”，这种根据知识和示例快速学习的想法并不是第一次提出。Meta-Learning有异曲同工之妙。假设有一个 task 的分布，我们从这个分布中采样了许多 task 作为训练集。我们希望 meta learning 模型在这个训练集上训练后，能在这个分布中所有的 task 上都有良好的表现，即使是从来没见过的 task。</p><div align=center><img src=/img/1701951328202-ff3bbb97-99b3-45e4-8825-1bde88ca6edf.png alt=img style=zoom:33%></div><p>MAML是比较经典的工作，目的是学习一个好的初始化，使得模型能快速适应新任务：</p><div align=center><img src=/img/1701951424268-e5e02dac-3981-4487-b3a7-e26693507fa7.png alt=img style=zoom:50%></div><p>其迭代分为内循环和外循环两个框架：</p><div align=center><img src=/img/1701951447840-70c3c79b-4f68-4b2b-a715-e65dcb77c6a0.png alt=img style=zoom:33%></div><p>外循环负责筛选一批任务进行<strong>一次梯度更新</strong>，内循环负责为每个任务计算更新梯度，但不会立马更新该任务梯度，而是在外循环中一次更新多个任务的梯度。这样的好处就是避免参数过于偏向最近训练的任务。</p><p>回到GPT-3上来，上下文分为三种：Zero-shot、One-shot和Few-shot：</p><div align=center><img src=/img/1701951924165-81ae47ce-3470-42c4-b866-694ecad11d65.png alt=img style=zoom:50%></div><p>对应到Meta-Learning框架上：</p><div align=center><img src=/img/1701952011305-283570d0-e91c-43ca-bb68-a92bf2b1b921.png alt=img style=zoom:50%></div><p>不过GPT-3相比Meta-Learning是个更优雅的范式。面对新任务时，无需训练参数，只需要在输入给任务描述和少量示例即可。</p><p>GPT-3另一个特点是极致的暴力美学：</p><div align=center><img src=/img/1701952272158-fb0a9ba8-3a3d-4f83-9675-fb9240cd1dcc.png alt=img style=zoom:50%></div><p>有两个发现：一是参数量对模型性能有极大影响，而是one-/few-shot对参数量大的模型性能提升更显著。</p><p>原文细节非常多，这里不再赘述。</p><h3 id=322-prefix-decoder>3.2.2. Prefix Decoder<a hidden class=anchor aria-hidden=true href=#322-prefix-decoder>#</a></h3><p>前缀解码器架构(也称非因果解码器架构)修正了因果解码器的掩码机制，以使其能够对前缀token执行双向注意力，并仅对生成的 token 执行单向注意力。代表工作是GLM(General Language Model)系列。</p><div align=center><img src=/img/1702012021618-023d7d24-96c9-455c-b816-8c9a80515756.png alt=img width=80%></div><p><strong>回归模型</strong>从左到右学习语言模型，适合于长文本生成和少样本学习，但不能捕捉上下文词之间的双向依赖关系。</p><p><strong>自编码模型</strong>通过去噪目标学习双向上下文编码器，适合于自然语言理解任务，但不能直接用于文本生成。</p><p><strong>编码器-解码器模型</strong>结合了双向注意力和单向注意力，适合于有条件的生成任务，如文本摘要和回复生成。</p><p>这三类语言模型各有优缺点，但没有一种框架能够在所有的自然语言处理任务中都表现出色。因此清华提出一个通用架构，想结合这三种框架的优点。</p><p>GLM有两个关键设计：一是随机挖掉若干个子序列，并打乱顺序做生成；二是二维位置编码。位置也是为了前者服务。</p><p>本质上GLM还是个自回归模式，只是预测的并不是句子的下个词，而是中间的序列。并且通过打乱机制让模型学到上下文能力。</p><div align=center><img src=/img/1702011917415-b4ef5e45-10a4-400c-a354-adada5fe0691.png alt=img style=zoom:50%></div><div align=center><img src=/img/1702012140210-777b1887-5038-4f2e-bbaf-4fc45b6befd2.png alt=img style=zoom:50%></div><h2 id=33-encoder-decoder>3.3. Encoder-Decoder<a hidden class=anchor aria-hidden=true href=#33-encoder-decoder>#</a></h2><p>Encoder-Decoder的代表工作BART，T5和Switch Transformer等，这里只介绍BART。</p><p>BART(<strong>B</strong>idirectional and <strong>A</strong>uto-<strong>R</strong>egressive <strong>T</strong>ransformers)设计如下：</p><div align=center><img src=/img/1702013372778-1ac334d0-3e3f-4c9d-bbf4-2ea8b09a28b8.png alt=img style=zoom:50%></div><p>除了上述架构，还有个比较重要的点是对输入加了噪声。</p><div align=center><img src=/img/1702013484235-f379d52f-ce63-43ce-bb57-8e25e2e78682.png alt=img style=zoom:50%></div><h2 id=34-moe>3.4. MOE<a hidden class=anchor aria-hidden=true href=#34-moe>#</a></h2><p>对于上述架构，我们可以通过专家混合（MoE）扩展来进一步扩展它们，在这种扩展中，每个输入的一部分神经网络权重被稀疏激活，例如Switch Transforme。MoE的主要优点是它是一种灵活的方式来扩大模型参数，同时保持恒定的计算成本。有研究表明，通过增加专家数量或总参数大小可以观察到显著的性能改进。尽管有这些优点，但由于路由操作的复杂、硬切换特性，训练大型MoE模型可能会遇到不稳定性问题。</p><div align=center><img src=/img/1706169833894-293713b0-88eb-41cd-8209-0fc0541be497.png alt=img style=zoom:50%></div><h2 id=35-架构选择总结>3.5. 架构选择总结<a hidden class=anchor aria-hidden=true href=#35-架构选择总结>#</a></h2><div align=center><img src=/img/1701758285706-4de29070-1690-49c4-a39a-913eac19dc31.png alt=img style=zoom:50%></div><p>目前主流的LLM架构是Causal Decoder，目前还无法从理论上证明这种架构的优越性。</p><div align=center><img src=/img/1701758258164-0b713b6b-3829-4cd0-8d3e-526df19f1b21.png alt=img style=zoom:50%></div><h1 id=4-参考文献>4. 参考文献<a hidden class=anchor aria-hidden=true href=#4-参考文献>#</a></h1><ol><li><a href=https://drive.google.com/file/d/1ie77V8Nbdj5bH_pn74mA8hhtzROV61r9/view>Apertium: a free/open-source platform for rule-based machine translation</a></li><li><a href=https://people.cs.umass.edu/~mccallum/papers/crf-tutorial.pdf>An Introduction to Conditional Random Fields for Relational Learning</a></li><li><a href=https://arxiv.org/pdf/1409.0473.pdf>Neural Machine Translation by Jointly Learning to Align and Translate.</a></li><li><a href=https://jalammar.github.io/illustrated-transformer/>The Illustrated Transformer</a></li><li><a href=https://jalammar.github.io/illustrated-bert/>The Illustrated BERT, ELMo, and co.</a></li><li><a href=https://jalammar.github.io/illustrated-gpt2/>The Illustrated GPT-2</a></li><li><a href=https://www.zhihu.com/question/347678607>如何理解Transformer论文中的positional encoding</a></li><li><a href=https://zxh.io/posts/zh/2020-08-05-meta-learning/>Meta Learning：一种套娃算法</a></li><li><a href=https://www.zhihu.com/question/339723385>Transformer的Score为什么Scale</a></li><li><a href=https://aimg8.dlssyht.cn/u/551001/ueditor/file/276/551001/1678415058523571.pdf>ChatGPT调研报告</a></li><li><a href=https://36kr.com/p/2169466994995713>AIGC：从不存在到存在</a></li><li><a href=https://zhuanlan.zhihu.com/p/637382548>清华大学通用预训练模型：GLM</a></li><li><a href=https://arxiv.org/pdf/2303.18223.pdf>A Survey of Large Language Models</a></li><li><a href=https://arxiv.org/pdf/1505.08075.pdf>Transition-Based Dependency Parsing with Stack Long Short-Term Memory</a></li><li><a href=https://arxiv.org/pdf/2304.13712.pdf>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</a></li><li><a href=https://arxiv.org/pdf/1301.3781.pdf>Efficient Estimation of Word Representations in Vector Space</a></li><li><a href=https://arxiv.org/pdf/1810.04805.pdf>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li><li><a href=https://arxiv.org/pdf/1802.05365.pdf>Deep contextualized word representations</a></li><li><a href=https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf>Improving Language Understanding by Generative Pre-Training</a></li><li><a href=https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>Language Models are Unsupervised Multitask Learners</a></li><li><a href=https://arxiv.org/pdf/2005.14165.pdf>Language Models are Few-Shot Learners</a></li><li><a href=https://arxiv.org/pdf/2302.13971.pdf>LLaMA: Open and Efficient Foundation Language Models</a></li><li><a href=https://arxiv.org/pdf/1703.03400.pdf>Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</a></li><li><a href=https://arxiv.org/pdf/2103.10360.pdf>GLM: General Language Model Pretraining with Autoregressive Blank Infilling</a></li><li><a href=https://keg.cs.tsinghua.edu.cn/jietang/publications/ACL22-Du-et-al-GLM.pdf>All NLP Tasks Are Generation Tasks: A General Pretraining Framework</a></li><li><a href=https://arxiv.org/pdf/1910.13461.pdf>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></li><li><a href=https://arxiv.org/pdf/1910.10683.pdf>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></li><li><a href=https://arxiv.org/pdf/2101.03961.pdf>Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</a></li><li><a href=https://arxiv.org/pdf/1701.06538.pdf>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a></li></ol></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://hpzhao.github.io/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8Bagent/><span class=title>« Prev</span><br><span>大模型Agent调研</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://hpzhao.github.io/>Huaipeng's Blog</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>