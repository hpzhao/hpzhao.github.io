[{"content":"本文范式取自参考文献[3]，内容贴近工业界推荐系统。在遵循其分类范式基础上，补充了一些学术届和工业界的相关工作.\n1. 相关背景 1.1. 现代推荐系统架构 推荐系统的核心任务就是在给定一个用户$u \\in \\mathbb{U}$和上下文$C$的情况下，推荐一批排好序的items:$[i_k]^N_{k=1}$，$\\mathcal{i}k \\in \\mathscr{I}$即：\n$$ \\bigl[i_k\\bigr]_{k=1}^N \\leftarrow \\mathbf{RS}(u, c, \\mathscr{I}),\\; u \\in \\mathcal{U},\\; i_k \\in \\mathscr{I} $$上下文是个比较重要的变量，比如同一用户在首猜和在LazFlash推荐的结果差异很大。\n推荐系统大致可分为数据收集、特征工程、特征编码、打分/排序、用户交互和流程控制器几个环节。除了数据收集外，其他环节大模型都可以参与其中。\n1.2. 分类范式 LLM在推荐系统的应用可以从两个视角来看：Where和How。Where是指LLM在推荐系统哪个环节应用，How是指LLM在推荐系统的应用方式(训练和推理)。CRM表示Conventional Recommendation Model。\n2. Where to adapt LLM 2.1. 特征工程 2.1.1. 特征增强 由于具有强大的推理能力和开放世界知识，LLM（大型语言模型）常被视为灵活的知识库。因此，它能够提供辅助特征，以更好地进行用户偏好建模和项目内容理解。CUP 采用 ChatGPT 根据用户评论文本总结每个用户的兴趣，通过关键词压缩用户画像数据，使其可以被小规模语言模型（如 BERT）编码。\n2.1.2. 样本生成 除了特征增强，LLM 还用于生成合成样本，丰富训练数据集，提高模型预测质量。Zhang 等人进一步尝试结合多个大型语言模型作为代理，以模拟更真实推荐场景中的细粒度用户交流和交互。\n2.1.3. 相关工作 2.1.3.1. MINT 论文：Large Language Model Augmented Narrative Driven Recommendations\nMINT表示Data augMentation with INteraction narraTives。\n叙事驱动推荐（Narrative-driven recommendation，NDR）是一个信息访问问题，其中用户通过详细描述自己的偏好和情境来请求推荐，例如，旅行者在描述他们的喜好/厌恶和旅行情况时请求推荐景点。随着基于自然语言的对话接口在搜索和推荐系统中的崛起，这些请求变得越来越重要。然而，NDR缺乏丰富的用于模型训练的训练数据，当前的平台通常不支持这些请求。作者利用大模型扩充数据来训练NDR模型。核心思路是利用现有的user-item交互数据，用大模型将用户兴趣总结成陈述性文本。\n2.1.3.2. TagGPT 论文：TagGPT: Large Language Models are Zero-shot Multimodal Taggers\n标签在现代互联网时代的各种应用中（如搜索引擎和推荐系统）对多媒体内容的有效分发起着关键作用。本文提出了TagGPT，一个能够以完全零样本方式进行标签提取和多模态标签化的全自动系统。我们的核心启示是，通过精细的提示工程，LLMs能够根据多模态数据的文本线索（例如OCR, ASR, 标题等）提取和推理出合适的标签。具体来说，为了自动构建一个高质量的标签集，以反映特定应用的用户意图和兴趣，TagGPT通过提示LLMs从一系列的原始数据中预测大规模的候选标签，并通过频率和语义进行筛选。\n抽取的标签也做了进一步后处理，频率太高的没有区分性，频率太低的没有代表性。另外也对相似度过高的tag进行合并。\n下图是抽取结果的一个示例：\n2.2. 特征编码 利用LLM作为辅助文本特征编码器，从而获得两大主要好处：（1）进一步为后续的神经推荐模型提供丰富的user/item表示，其中包含语义信息；（2）通过自然语言作为桥梁，实现跨领域推荐，在这种情况下，ID特征字段可能不共享。\n2.2.1. 表示增强 在一些文本特征丰富的场景(例如item有标题、详细描述等)，LLM被用于特征编码器。LLM4ARec利用GPT2从用户画像和评论中提取个性化的方面术语和潜在向量，以更好地辅助推荐。在一些特殊情况下，LLM编码的语义表示不会直接用作后续评分/排序函数的输入。相反，它通过量化被转换为一系列离散的token，以适应需要离散输入的评分/排序函数（例如，生成推荐）。\n2.2.2. 统一跨领域推荐 除了改善user/item表示之外，LLM作为特征编码器还可以实现迁移学习和跨领域推荐，自然语言在其中起到了对齐不同领域异构信息的桥梁作用。ZESRec应用BERT将物品描述转换为通用的语义表示，用于零样本推荐。在UniSRec中，物品表示通过一个固定的BERT模型和一个轻量级的MoE增强网络进行学习，以实现跨领域序列推荐。\n2.2.3. 相关工作 2.2.3.1. LLM探索Text CF上限 论文：Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights\n现存TCF系统大都采用参数规模较小的BERT系架构编码item，本文探索利用LLM提升TCF上限。针对下面几个问题有初步结论。\nQ1：TCF范式达到性能瓶颈了么？\n还未看到瓶颈，随着参数规模增大，性能一直在提升。 Q2：175B的LLM能得到通用表征么？\n在推荐任务上fine-tune的LLM性能有明显提升，说明现在的LLM生成的Embedding远不够通用 Q3：基于175B参数LLM的TCF能轻易替代IDCF么？\n175B的TCF在某些场景下和IDCF性能可比，但远谈不上能够替代，尤其是考虑到IDCF的高效性。 2.2.3.2. VQ-Rec 论文：Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders\n基于IDCF方法的推荐模型可迁移性差，因此出现大量工作研究利用item的文本来表示item，在跨领域迁移时取得了不错的结果。然而item和其文本绑定太紧密可能会造成潜在问题，例如过分强调文本特征而忽略交互特征，两个文本相似的item其表现可能差异很大。因此本文提出VQ技术建模item表示，让其既具备文本特征，又能通过协同过滤更新，减少对推荐系统影响。\nVQ-Rec分为三步：\nVQ Item表征. 利用PLM对item的文本进行编码得到item embedding，然后通过PQ将item embedding映射为D个code，再给每个code分配一个可学习embedding。item最终由这D个code池化后的向量表示。 对比学习预训练. 为提高跨领域能力，提高方法通用性，在多领域推荐数据上预训练code emb和Transformer参数，采用对比学习方式训练。为让负样本更有意义，摈弃了batch内随机采样，采用了两种方案构造hard负样本：一是半混合负样本，二是混合领域采样。 跨领域微调. 由于不同领域文本分布差异较大，因此对item-\u0026gt;code映射重新适配，并微调code emb，fix住Transformer参数。 2.3. 打分/排序函数 打分/排序的目标一般和推荐系统的最终目标息息相关。根据LLM所解决的不同任务，分为三类：1）item打分任务 2）item生成任务 3）混合任务\n2.3.1. item打分任务 在item打分任务中，LLM扮演pointwise打分函数$F(u,i)$。候选商品最终的序由排序分决定：\nPre-filter为item预选阶段，主要是为了减小候选集规模，一般包含检索和粗排阶段。\nLLM产生token的一般流程如下：\n其中$\\hat{t}$为离散token。但打分任务一般要求打分函数产生有意义的实数，例如CTR任务要求输出分数为$\\hat{y} \\in [0,1]$，打分预测任务要求$\\hat{y} \\in [0,5]$。针对这一问题，通常有三种主要解决方法：\n第一种方法摈弃LLM解码层，采用单塔结构，直接将隐层$h$映射为分数：\n输入提示需要包含用户和item信息，以支持基于隐层的偏好估计。E4SRec通过预先学习的用户和物品ID嵌入来构建个性化提示，以实现精确的偏好估计。CER加强了推荐内容与其自然语言解释之间的一致性，以提高评分预测的性能。\n第二种方法也摈弃了LLM解码层，采用双塔结构：\n在这类工作中，一般重点是设计不同的辅助结构以使用LLM增强双侧信息。通过采用编码器-解码器LLM，TASTE首先将每个用户行为编码成一个软提示向量，然后利用解码器从软提示序列中提取用户偏好。\n第三种方法保留了LLM解码层，将打分问题转化成二元问题，即给定U和I，U是否喜欢I，基于候选答案概率归一化得到最终分数：\n2.3.2. item生成任务 在item生成任务中，LLM作为生成函数$F(u)$直接生成最终item排序列表，仅需要一次前向计算。一般而言，item生成任务高度依赖LLM的内在推理能力，其过程可表示为：\n根据是否提供候选item，其方案可分为两类：1）开放集item生成 2）闭集item生成。\n2.3.2.1. 开放集item生成 LLM直接根据user profile和历史行为生成用户偏好的排序item列表。由于在输入提示中并未提供候选item，因此可能会产生幻觉问题，生成的item无法准确匹配item池中的item。因此除了设计输入提示模版和微调算法之外，还需要在item生成后进行匹配后处理操作 ，以克服幻觉问题。这一过程描述如下：\n其中$\\hat{i_k}$是LLM生成的item，$i_k$为item池中真实存在的item，Match为匹配函数。\nLANCER采用知识增强的前缀微调来进行生成基础，并进一步应用余弦相似度将生成item文本的编码表示与通用item池进行匹配。Di Palma 等人利用ChatGPT进行用户兴趣建模和下一个item标题生成，并使用Damerau-Levenshtein 距离进行item匹配。\n除了以文本方式生成item外，另一研究方向聚焦于将语言空间与基于ID的推荐空间对齐，从而使LLM可以直接生成item ID。例如，Hua 等人探索了更好的item索引方法（如顺序索引、协作索引），以提升此类索引生成任务的性能。LightLM设计了一种轻量级 LLM，结合了精心设计的用户和item索引，并使用约束束搜索进行开放集合item ID 的生成。此外，LLaRA在 LLM 的输入提示中采用了一种新颖的混合方法，将传统推荐系统中的基于 ID 的item嵌入与文本item特征相结合。其他关于开放集合item生成的典型工作包括：GenRec，TransRec，LC-Rec，ControlRec和POD。\n2.3.2.2. 闭集item生成 LLM需要根据给定的候选item集合排序。候选item一般由pre-filter挑选得到。受限于LLM上下文窗口长度，候选item数量通常最多设置成20个，一般是重排阶段。过程形式化为：\n例如，LlamaRec 采用 LRURec作为检索器，并在预筛选的项目上微调 LLaMA2 以进行列表排名。DRDT通过多轮迭代反思对给定候选项目进行排名，以逐步优化排列表。\n相比之下，开放集生成任务通常会遭遇生成幻觉问题，即生成的item可能无法与通用item池中的确切item匹配。因此，后续的生成匹配功能是非常必要的，这增加了推理推理成本甚至可能影响最终的推荐性能，尤其是在item文本与大型语言模型的语言分布差异很大的场景中。与此相反，封闭集生成任务使用轻量级检索模型作为预筛选器，提供一组明确的候选item，因此大型语言模型可以缓解幻觉问题。然而，将候选item引入大型语言模型的输入提示中会导致其他问题。首先，由于上下文窗口的限制，大型语言模型无法处理大量的候选item（通常少于20个），最终的推荐性能可能会受到检索模型（即预筛选器）的限制。此外，Ma 等和 Hou 等发现，在提示中打乱候选item的顺序会影响排名结果，导致不稳定的推荐结果。上述封闭集生成任务的问题本质上源于输入提示中存在候选item集，这在开放集生成任务中可以得到很好地解决。总的来说，我们可以观察到，开放集和封闭集生成任务在各自的优缺点上是互补的。因此，在实际应用中选择哪种任务实际上取决于我们在现实场景中遇到的具体情况和问题。\n2.3.2.3. 混合任务 在混合任务中，大型语言模型以多任务的方式运作，通过统一的语言接口处理item打分和生成任务。支持这种混合功能的基础是大型语言模型本质上是多任务学习器。例如，P5、M6-Rec和InstructRec通过不同的提示模板调整编码器-解码器模型，以更好地对齐一系列推荐任务，包括项目评分和生成任务。\n2.3.3. 相关工作 2.3.3.1. LLMRank 论文：Large Language Models are Zero-Shot Rankers for Recommender Systems\n基于用户行为数据的推荐算法有两个问题：一是没有显式理解用户偏好，二是解决复杂推荐问题时，缺乏知识和推理能力。文章研究了使用PLM来推荐的若干问题。\n流程如下：\n序列化用户行为历史. 设计了3中不同的prompt：\nSequential prompting. 用原始用户行为顺序. Recency-focused prompting. 强调最近的行为. In-context learning. 引入其他用户的demonstration examples会引入噪声，因此使用当前用户的历史行为构造示例. LLM对候选排序. 将用户历史行为和候选商品放入prompt中，并加上排序instruction.\n结果后处理. 当item标题较短时，可以让LLM直接输出标题，否则可以输出标号。采用字符串匹配算法来匹配不能严格对齐的结果。LLM也有较小概率会生成不存在的item，大约3%.\n通过实验得到如下结论：\nLLM对历史行为顺序不敏感，按照时间顺序和随机打乱效果一样。但可以通过强调最近行为和ICL都能得到改善； 历史行为增多，LLM性能反而变差。这是因为LLM对历史行为顺序不敏感，较少行为反而能让LLM关注近期的行为； LLM排序时会考虑item流行度、文本相似度、历史行为等信息； 当候选集变大时，LLM排序能力也下降。可能和任务变难有关系； LLM受到Position bias影响，调整候选集顺序效果会有较大差异。可采用bootstrapping方式合并排序结果； 经过instruction tuning，排序性能更好。 2.3.3.2. TALLRec 论文：TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation\n直接利用PLM对候选商品打分可能会遇到以下问题：LLM要么只给出积极的预测(喜欢)，要么拒答。即使忽略拒答样本，剩余样本的AUC也只接近0.5，和随机猜类似。因此需要探索新的方法改善。\n本文采用了Llama-7B模型验证了在few-shot进行微调，其效果比传统模型明显好。\n2.4. 用户交互 在许多实际应用中，推荐系统通常是一个单轮交互过程，系统会随着时间的推移监控用户的行为（如点击和购买），然后在某些预定义的情况下呈现一组量身定制的相关item。这样的单轮交互在复杂场景中缺乏有效和多样化的方法来获取用户兴趣并检测用户当前的情况或需求。为此，大型语言模型的出现提供了一个有前途的替代方案，通过提供更积极和适应性更强的用户交互形式。LLM可以在与用户进行实时互动时，收集更多微妙的自然语言反馈，而不仅仅是被动地依赖用户过去的行为。\n一般来说，基于LLM的用户交互在推荐系统中通常表现为多轮对话，这涵盖了对话式推荐系统。在这样的对话过程中，LLM通过整合对话中的上下文和应用广泛的开放世界知识，提供了前所未有的理解用户兴趣和需求的丰富性。LLM可以通过引导用户当前的偏好、为项目建议提供解释或处理用户对建议的反馈来支持推荐系统进行高度相关和量身定制的推荐。换句话说，大型语言模型的引入使得推荐系统在用户交互方面更加可行和用户友好。具体来说，从交互内容的角度来看，基于LLM的用户交互方式可以分为：(1) 任务导向的用户交互，和 (2) 开放式用户交互。\n2.5. 流程控制器 LLM具有知识和推理能力，除了参与到推荐的某个环节外，还可以参与系统的流程控制，可能导致更具互动性和可解释性的推荐过程。例如Chat-REC利用ChatGPT来连接会话界面和传统推荐系统，其需要推断用户偏好，决定是否调用后端推荐API，并在向用户展示之前进一步修改（例如过滤和重新排序）返回的候选item。这些操作使LLM超越了上节的用户交互角色，并通过某些API调用和工具使用在多阶段推荐流水线中进行控制，以实现会话推荐系统。\n2.5.1. 相关工作 2.5.1.1. Chat-REC 论文：Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System\n传统推荐系统互动性差、可解释性差，也不能准确推荐新item。核心方法是连接推荐系统和LLM。\n2.5.1.2. RecLLM 论文：Leveraging Large Language Models in Conversational Recommender Systems\n会话推荐系统（CRS）通过允许用户通过实时多轮对话与系统交互，为用户提供了更高的透明度和控制权。最近，大型语言模型（LLMs）展现出了前所未有的自然对话能力，并将世界知识和常识推理融入语言理解中，解锁了这种范式的潜力。然而，在CRS中有效利用LLMs带来了新的技术挑战，包括正确理解和控制复杂的对话以及从外部信息源检索。这些问题因不断演变的大量物品语料库和缺乏对话数据进行训练而加剧。在本文中，提供了利用LLMs构建端到端大规模CRS的路线图。\n在推荐环境中特别具有挑战性的是如何在LLM和底层推荐引擎之间进行接口交互。一种方法是让LLM在充当对话代理的同时，也充当推荐引擎。然而，对于大规模推荐应用，物品语料库可能包含数百万或数十亿相互变化的物品，这使得LLM难以在其参数中记住语料库。另一种方法是LLM必须以某种方式连接到外部推荐引擎或数据库，传递相关的偏好信息。许多传统推荐系统采用两阶段管道，首先检索候选项目，然后对其进行排序。RecLLM 也遵循这种策略，增加了一个额外的特点，即排序器还会联合生成每个item被选择的推荐理由。\n第一阶段：召回\n采用了4种召回方法：\n双encoder模式：LLM为对话上下文生成embedding，item encoder生成item embedding，然后依据相似度做召回。这种从LLM内部提取embedding的方法的一个缺点是，它严重阻碍了我们以样本高效的方式学习检索模型的能力。 从头训练的双编码器模型需要大量的训练数据，以便将上下文塔的embedding约束在与物品塔embedding相同的子空间中。 LLM生成式推荐：LLM直接输出推荐的结果，推荐引擎负责在item pool中做匹配。在这种方法中，LLM直接以文本形式输出推荐的项目ID或标题。检索算法是对内容库中的项目进行精确或模糊匹配，推荐引擎在这简单匹配之外不再起其他作用。LLM必须通过其预训练和特定内容库的微调阶段的组合来学习输出这些ID/标题。假设我们的系统必须能够从固定的内容库返回推荐列表，这种方法最接近于让一个基于LLM的聊天机器人直接作为CRS工作。这个方法的缺点是，由于推荐引擎仅完成了微不足道的工作，LLM必须在其模型参数中记住整个内容库的信息。对于大型内容库，这在模型大小和所需训练数据方面可能是极其昂贵的，同时也使得难以在不重新训练LLM的情况下刷新内容库。 基于概念的搜索：让LLM基于对话生成一个概念list，推荐引擎将这个概念list编码成embedding，并和item embedding做召回。这种方法一个限制是概念列表通常是对话的粗略表示，类似于连续词袋方法，在词序和语言其他细微差别方面会有损失，这可能会对检索质量产生负面影响。 基于搜索API：LLM基于对话生成一条搜索的query，调用搜索引擎返回候选物品。检索质量受限于搜索查询能多大程度上适当地表示对话的全部上下文。 第二阶段：排序/解释\n给定items和上下文摘要作为输入，LLM排序器使用链式推理来评分项目。LLM生成的中间链式推理步骤作为某些项目最终包含或被排除在推荐清单之外的解释。这些解释可以内部用于调试，也可以展示给用户，通过将其作为输入包含在生成对话接口中的对话管理器中，或者通过后处理将其包含在推荐清单显示的视觉用户界面中的弹出框中。\n模型微调：\n在搜索API查找中，LLM处理会话历史记录并输出搜索查询，然后将其传递到黑箱搜索算法中。当使用这种架构时，损失不再是可微的，普通的监督学习不可行。相反，我们可以将设置重新框定为一个上下文bandit问题，其中LLM是一种策略，标签是奖励信号，黑箱搜索算法被视为环境。如果LLM编码器与其他模块共享，我们可以选择调优仅影响输出搜索查询这一任务的保护参数，或者调优也影响这些其他模块行为的共享参数。\n3. How to adapt LLM 3.1. 分类体系 根据是否微调/推断是否需要CRM两个维度将现有工作分为4个象限：\n3.2. 发展过程 3.3. 重要结论 3.3.1. 协同过滤知识非常重要 我们可以观察到第3象限和第1、2、4象限的研究工作之间存在明显的性能差异。即使第3象限的研究工作采用了大规模模型（如ChatGPT或GPT-4），并且配备了先进技术如用户行为检索和工具使用，其表现依然较差。这表明推荐系统是一个高度专业化的领域，要求大量的领域内协同知识。大型语言模型（LLM）无法从其通用预训练语料库中有效学习这种知识。因此，当我们将LLM应用于推荐系统时，必须涉及领域内协同知识以提高性能。通常有两种方法可以实现这一目标（对应于第1、2、4象限）：\n在训练阶段调优LLM，从数据中心的角度注入协同知识。 在推理阶段利用CRM，从模型中心的角度注入协同知识。 3.3.2. LLM擅长重排困难样本 尽管大型语言模型（LLM）在零样本/小样本学习方面通常表现较差，因为涉及的领域内协同知识较少，但研究发现，大型语言模型如ChatGPT更有可能在处理困难样本时表现出色。他们引入了过滤-再排序范式，该范式利用传统推荐系统中的预排序功能（例如在工业应用中的匹配或预排序阶段）来预先过滤掉那些容易的负例项，从而生成一组包含更难样本的候选项供LLM重新排序。这样，LLM的列表式再排序性能（尤其是类似ChatGPT的API）可以得到提升。这个发现对于工业应用具有指导意义，我们可以要求LLM仅处理困难样本，而将其他样本留给轻量级模型处理，以节省计算成本。\n4. 实际应用中的挑战 4.1. 训练效率 提升现代深度学习推荐系统性能的两个关键因素是：增加训练数据量和提高模型的更新频率。尽管调校大规模语言模型（LLM）可能提升性能，但也会带来巨大的时间和计算资源成本。因此，在实际应用中，需要确保适配LLM的训练效率。现有的研究主要通过参数高效微调（PEFT）方法解决内存使用问题，但在大规模场景中时间消耗仍然高。我们建议采用异步更新策略，将LLM用于特征工程时减少训练数据量和降低更新频率，同时保持CRM的完整数据和高频更新。这是基于LLM具有强大的归纳学习能力，通过少量监督可以产生广泛适用的结果。由此，LLM为CRM提供领域内知识，CRM则作为LLM的频繁更新适配器。\n4.2. 推理延迟 在线推荐系统通常是实时服务，对时间非常敏感，各个阶段（如匹配、排序、重排序）需在几十毫秒内完成。大规模语言模型（LLM）在推理阶段的参与会增加推理延迟。预计算和缓存LLM的输出或中间表示是确保低延迟的一种常见策略。例如，M6-Rec提出了分段后期交互策略，先预计算和缓存细粒度文本特征的编码表示，再在推荐请求到达时进行后期交互。其他方法如UniSRec和VQ-Rec则直接缓存语言模型生成的嵌入。此策略对静态的物品信息有效，但对动态的用户信息来说效果不佳，因此需要找到合适的缓存频率来平衡性能和计算成本。\n另外，可以通过模型压缩技术（如蒸馏、剪枝和量化）来减少模型大小以提高推理效率。例如，CTRL和FLIP通过对比学习将LLM的语义知识蒸馏到CRM中，从而在保持低延迟推理的同时改进推荐性能。这些策略通常涉及模型性能和推理延迟之间的权衡。另一种选择是将LLM用于特征工程阶段，预存其输出，从而在推理时减少额外负担。此外，还可以将LLM应用于推理延迟要求较宽松的场景，如对话推荐系统。\n4.3. 领域长文本建模 在适配大规模语言模型（LLM）时，需通过提示模板构建领域内的文本输入，并在前面必要时插入适当的指令和示范。然而，真实世界的推荐系统通常需要更长的用户历史、更大的候选集和更多特征，这可能导致LLM输入的长文本带来两大挑战：\n过长的文本输入会导致内存效率低下的问题（经典变压器的空间复杂度为O(L²)，其中L是词元数量），甚至可能超出上下文窗口的限制，导致部分信息丢失和次优的输出。 即使输入长度不超过上下文窗口，LLM在完全理解和推理推荐数据上也可能存在问题。研究表明，LLM处理长文本时困难重重，尤其是在扩展用户历史或候选集时，即使总词元数远未达到上下文窗口限制。这可能是因为领域内长文本与LLM预训练语料的分布差异很大。 因此，研究如何在提示工程中适当地筛选、选择和安排文本信息作为LLM的输入，以及如何指导或调校LLM更好地适应这些领域内长文本的分布非常重要。此外，在自然语言处理领域，有一系列工作提出了解决上下文窗口限制的方法（如滑动窗口、记忆机制），这些方法可以在推荐系统中考虑应用。此外，最近的研究提出结合CRM的潜在表示来压缩LLM的个性化输入提示，以缓解长文本问题。例如，CoLLM和E4SRec通过线性投影层将每个用户行为的文本描述替换为CRM嵌入表中的潜在向量，从而大大减少长用户序列的词元数量。受前缀调优的启发，ClickPrompt将CRM的样本级最终表示转化为LLM的层级提示，从而更容易从提示模板中去除不必要的特征。\n4.4. ID索引化和建模 在推荐系统中，存在一种本质上不包含语义信息的纯ID特征（如用户ID、物品ID）。如果将这些ID特征包含在提示文本中，它们的分词对语言模型而言实际上是无意义的。许多研究倾向于直接放弃这些ID特征，以通过自然语言界面实现统一的跨领域推荐，因为这些ID通常在不同领域之间不共享。然而，有些研究指出，尽管牺牲跨领域的泛化能力，ID特征可以大大提升推荐性能。因此，是否保留ID特征仍然是一个未解决的问题，将研究分为两种方向。\n**一方面，我们可以保留ID特征以牺牲跨领域泛化能力换取更好的领域内推荐性能。**P5及其变体在提示模板中保留ID特征，并设计了全词嵌入层，为相同ID特征的词元分配相同的全词嵌入，类似于位置嵌入的方式。基于此，Hua等进一步探索了各种物品ID索引策略以确保相似物品的ID由相似的子词元组成。RecFormer和UniSRec则在提示文本中省略物品ID，但在底部嵌入层或顶部投影层引入额外的ID嵌入。其他研究则寻求整合传统推荐模型中的ID嵌入，从而使LLM的输入提示不包含纯ID特征。总之，这条研究线路上，研究人员应专注于通过精心设计的ID索引与建模策略来将LLM与ID特征关联起来。\n**另一方面，我们可以放弃ID特征，以通过自然语言界面实现统一的跨领域推荐。**保持一个统一的模型来服务多个领域是非常有前景的，尤其是在涉及大规模语言模型时。为了在放弃ID特征的情况下实现与保留ID特征的类似性能，研究人员可以探讨隐式引入ID特征的方法，例如应用对比学习来对齐语义和协同知识，从而避免涉及LLM的ID特征。\n5. 参考文献 A Survey on Large Language Models for Recommendation How Can Recommender Systems Benefit from Large Language Models: A Survey Large Language Model Augmented Narrative Driven Recommendations TagGPT: Large Language Models are Zero-shot Multimodal Taggers Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights Learning Vector-Quantized Item Representation for Transferable Sequential Recommenders Large Language Models are Zero-Shot Rankers for Recommender Systems TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System Leveraging Large Language Models in Conversational Recommender Systems ","permalink":"https://hpzhao.github.io/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9C%A8%E6%8E%A8%E8%8D%90%E9%A2%86%E5%9F%9F%E7%9A%84%E5%BA%94%E7%94%A8/","summary":"\u003cp\u003e本文范式取自参考文献[3]，内容贴近工业界推荐系统。在遵循其分类范式基础上，补充了一些学术届和工业界的相关工作.\u003c/p\u003e\n\u003ch1 id=\"1-相关背景\"\u003e1. 相关背景\u003c/h1\u003e\n\u003ch2 id=\"11-现代推荐系统架构\"\u003e1.1. 现代推荐系统架构\u003c/h2\u003e\n\u003cp\u003e推荐系统的核心任务就是在给定一个用户$u \\in \\mathbb{U}$和上下文$C$的情况下，推荐一批排好序的items:$[i_k]^N_{k=1}$，$\\mathcal{i}k \\in \\mathscr{I}$即：\u003c/p\u003e","title":"大模型在推荐领域的应用"},{"content":"1. 背景 春节期间，DeepSeek爆火出圈，性能在开源和闭源模型中达到顶尖水平，同时显著降低了训练和推断成本，并在推理领域取得重大突破，被誉为开源领域的\u0026quot;ChatGPT时刻\u0026quot;。R1模型是DeepSeek一年来持续探索与积累的成果，体现了算法、架构与硬件的协同设计。本文将重点解析R1模型的核心算法，并简要介绍其基础设施。\n2. 性能与成本 2.1. 评估数据集 MMLU(Massive Multitask Language Understanding)：[Paper] [Dataset Card] 这是一项大规模的多任务测试，包含来自不同知识领域的选择题。测试涵盖人文学科、社会科学、自然科学以及其他一些对某些人来说重要的学习领域。这包括57个任务，如基础数学、美国历史、计算机科学、法律等。要想在这一测试中取得高准确率，模型必须具备广泛的世界知识和强大的问题解决能力。\nMMLU-Pro： MMLU-Pro数据集是一个更加稳健且富有挑战性的大规模多任务理解数据集，旨在更严格地评估大型语言模型的能力。该数据集涵盖了多个学科的12,000道复杂问题。主要有三项改进：首先，将选项数量从4个增加至10个，提升了评估的真实性和挑战性，显著降低了随机猜测的得分；其次，提高了问题难度，增加更多推理型问题，使得CoT的结果可能比PPL高出20%；最后，通过增加干扰项，降低随机猜测概率，增强了基准测试的鲁棒性，模型得分对提示变化的敏感性从4-5%降至2%。\nGPQA：[Paper] [Dataset Card] GPQA包含了448道由生物学、物理学和化学领域的专家编写的多项选择题。我们确保这些问题的质量高且极具难度：在相应领域拥有或正在攻读博士学位的专家答题准确率为65%（在排除专家事后发现的明显错误后，准确率上升至74%），而技能高超的非专家验证者尽管平均花费超过30分钟并拥有无限制的网络访问权限（即问题\u0026quot;防谷歌\u0026quot;），准确率仅为34%。这些问题对于最先进的AI系统同样困难，我们基于GPT-4的最强基线模型准确率仅为39%。\nMATH 500：[Paper] [Dataset Card] 这个数据集包含了来自OpenAI在其论文《Let\u0026rsquo;s Verify Step by Step》中创建的MATH基准测试的500道题目子集。\nAIME 2024：[Dataset Card] 该数据集包含了2024年AIME I和AIME II测试中的30道题目。\nCodeforces：[Source] 编程试题。\nSWE-bench：[Dataset Card] SWE-bench 是一个用于测试系统自动解决 GitHub 问题能力的数据集。该数据集收集了来自 12 个热门 Python 仓库的 2,294 个\u0026quot;问题-拉取请求\u0026quot;对。评估通过单元测试验证进行，并以拉取请求后的行为作为参考解决方案。\n2.2. 评估结果 2.3. 训练与推理成本 训练成本：Llama3-405B使用了30.8M GPU时，而DeepSeek-V3 性能更强，仅使用了2.8M GPU时（计算量减少了约 11 倍）。\nChat类模型推断价格对比：\nReasoner类模型推断价格对比：\ninput (Price/1M) Cached input (Price/1M) Output (Price/1M) OpenAI o1 $15.0 $7.5 $60.0 OpenAI o3-mini $1.1 $0.55 $4.4 DeepSeek R1 $0.55 $0.14 $2.19 3. 模型架构 DeepSeek主打的卖点是计算开销低，从架构上主要通过MLA和MOE两个模块实现。\n3.1. MOE MOE(Mixture of Expert)自Google提出以来便应用到多个领域，包括NLP、多模态、视觉和推荐系统中。大语言模型参数量与性能有密切关系，但参数量增大也会带来训练和推断成本的上升。MOE是一种在扩大模型参数时管理计算成本的非常有效的架构。\nMOE可以分为Dense MoE和Sparse MoE两种类型。Dense MoE的特征在于所有专家都会被激活，只是每个专家承担的权重不同，可以理解为一种加权投票机制，每个专家负责不同的能力。然而，这种架构并不能减少计算量。相较之下，Sparse MoE只选择性激活TopK个专家，从而显著降低计算量。因此，DeepSeek选择采用Sparse MoE架构。\n在DeepSeekMoE模型中，相较于传统的Naive Sparse MoE，有两项主要改进。首先，DeepSeekMoE将专家（Expert）的数量增加了一倍，这样每个专家可以学习到更加差异化的知识。其次，模型引入了共享专家（Shared Expert）。在传统的路由策略中，被分配给不同专家的token可能共享某些知识或信息。因此，不同专家可能在各自的参数中包含相同的知识，这导致参数的冗余。通过引入专门用于捕获和整合这些共同知识的共享专家，可以有效地缓解路由专家之间存在的参数冗余问题。\n自动学习到的路由策略可能会遇到负载不均衡(load imbalance)的问题，这表现为两个显著的缺陷。首先，存在路由崩溃的风险，即模型总是只选择少数专家，导致其他专家无法得到充分的训练。其次，如果专家分布在多个设备上，负载不均衡会加剧计算瓶颈。\n解决负载均衡常用的方法是Switch Transformer中提出的辅助Loss：\n$f_i$即真实分配给expert $i$ 的概率，$P_i$ 为模型计算的分配给expert $i$ 的概率。文中认为loss会在 $f$ 和 $P$ 都是均匀分布时最小，此时每个专家被分配的概率都是相等的。例如有两个专家，那么loss的值为[0.5,0.5] * [0.5,0.5] = 0.5。但是这个结论是错误的：\n另外从直觉上，最小化这个loss意味着最小化 $\\langle f, P \\rangle$，而内积代表了相关性，这也并不合理。除此之外，这个辅助Loss只是想在expert之间做负载均衡，loss过大也会影响模型效果。\nDeepSeek提出Loss Free的方法来解决负载不均衡的问题。在计算gate softmax时考虑此时每个expert的分配情况，从而进行动态调整权重。\n3.2. MLA MLA(Multi-head Latent Attention)的核心思想是通过压缩KV，降低KV Cache存储压力，这是DeepSeek降本的核心设计之一。\nSelf Attention的核心思想是每个token的表示都是所有token的加权求和，权重即为token之间的correlation。每个Q、K、V都会被切成若干个head。\n$$ \\begin{aligned} \\mathbf{q}_t \u0026= W^Q \\mathbf{h}_t = [\\mathbf{q}_{t,1}; \\mathbf{q}_{t,2}; \\ldots; \\mathbf{q}_{t,n_h}], \\\\ \\mathbf{k}_t \u0026= W^K \\mathbf{h}_t = [\\mathbf{k}_{t,1}; \\mathbf{k}_{t,2}; \\ldots; \\mathbf{k}_{t,n_h}], \\\\ \\mathbf{v}_t \u0026= W^V \\mathbf{h}_t = [\\mathbf{v}_{t,1}; \\mathbf{v}_{t,2}; \\ldots; \\mathbf{v}_{t,n_h}], \\\\ \\mathbf{o}_{t,i} \u0026= \\sum_{j=1}^{t} \\text{Softmax}_j \\left( \\frac{\\mathbf{q}_{t,i}^T \\mathbf{k}_{j,i}}{\\sqrt{d_h}} \\right) \\mathbf{v}_{j,i}, \\\\ \\mathbf{u}_t \u0026= W^O [\\mathbf{o}_{t,1}; \\mathbf{o}_{t,2}; \\ldots; \\mathbf{o}_{t,n_h}], \\end{aligned} $$主流的LLM均为Casual Decoder架构，每个token只能看到前序的token，不能看到后面的token。如果每个token生成时都重新计算前序所有token的KV会产生大量的冗余计算。因此可以把之前计算好的KV缓存下来，以空间换时间，这就是所谓的KV Cache。\n当token数较多时也会带来性能瓶颈，因此产生了GQA和MQA这种工作。\nMLA核心思路是将KV映射到一个语义空间，需要的时候再进行还原，这样大大降低了Cache大小。\n另外在推断的时候，$W^{UK}$ 能被吸收到 $W^Q$ 中，$W^{UV}$ 能被吸收到 $W^O$ 中，因此并不需要按公式还原出 $K$ 和 $V$，这也意味着计算也是在Latent Space的，和Stable Diffusion等工作的思路很相似。我们以 $W^{UK}$ 为例，计算分数时：\n$$ q^T k = (W^Q h)^T (W^{UK} c^{KV}) = h^T [(W^Q)^T W^{UK}] c^{KV} = h^T W^Q_{new} c^{KV} $$在上述计算过程中，位置向量未被考虑。Transformer中的位置编码通常分为三类：1）绝对位置编码：为每个位置分配一个可训练或固定的位置向量，应用到token embedding上。其优点在于简单实现，但缺点在于扩展性差，并且不包含相对位置信息。例如，位置0和位置1之间的差异无法与位置1和位置200之间的差异呈现实际对比。2）相对位置编码：应用到查询（Q）和键（K）上，具有较好的扩展性。然而，计算开销较大，并且不适用于KV Cache架构。3）旋转位置编码（RoPE）：结合了绝对和相对位置编码的优点。通过向量旋转的方式，旋转角度与绝对位置线性相关，同时在QK计算中能够体现相对位置信息。\n然而将RoPE直接应用到MLA中会导致 $W^{UK}$ 不能被吸收到 $W^Q$ 中，导致MLA的计算开销增大。具体来说：\n$$ \\begin{aligned} \\text{RoPE}(q_t)^T \\text{RoPE}(k_m) \u0026= (R^q_t q_t)^T (R^k_m k_m) \\\\ \u0026= q_t^T (R^q_t)^T R^k_m k_m \\\\ \u0026= (W^Q h_t)^T (R^q_t)^T R^k_m (W^{UK} c^{KV}_m) \\\\ \u0026= h_t^T (W^Q)^T \\overline{(R^q_t)^T R^k_m} W^{UK} c^{KV}_m \\end{aligned} $$中间的两个旋转矩阵是和位置相关的，因此 $W^{UK}$ 不能被吸收到 $W^Q$ 中，需要每次重新计算。论文提出使用额外的一些head计算位置向量，然后再拼到QK中。这意味着也要同时缓存 $k^R_t$。\n3.3. MTP DeepSeek MTP(Multi-Token Prediction)能够提升数据利用效率，也提升了模型的规划能力。主要受两篇工作启发得到，一是提升训练效率的Google MTP，二是提升推理效率的EAGLE。\n传统的语言模型（LLM）在训练时通常只关注于预测下一个可能的Token。这种方法存在两个明显的局限性。首先，由于训练的信号较弱，模型对那些在句子中起重要转折作用的词汇（Hard Transitions Token）往往关注不足。其次，这种逐词预测的方式缺乏全局的规划能力，而人类在组织语言时通常是以段落为单位进行构思，而不是逐词考虑。基于上述背景，Google提出MTP，把模型最后一层的Head进行分组，每一层分别预测不同位置的token。\n从结果上来看，引入MTP后模型效果得到明显提升。从分析上来看，模型增加了对hard transition token的训练信号。如下图所示，如果只预测next one，那么\u0026quot;5-\u0026gt;A\u0026quot;的训练信号只有1/7=14%，而预测next three时，hard transition token占比为6/21=28%，可以看到训练信号大幅增加。但MTP训练时每个分组head都是独立的(parallel heads)，这违背了自回归的设计原则。\n另一篇借鉴的工作是大模型推理加速相关工作-Speculative Decoding.\nEAGLE是Speculative Decoding一篇知名工作，文中验证了Causal Heads优于Parallel Heads。因此deepseek将MTP中的Parallel Heads改成Causal Heads.\n4. 基础设施 4.1. 计算集群 DeepSeek-V3在配备有2048块NVIDIA H800 GPU的集群上进行训练。H800集群中的每个节点包含8块通过NVLink和NVSwitch连接的GPU。在不同节点之间，使用InfiniBand (IB)互连来实现通信。\n4.2. DualPipe 流水线并行是一种提高计算效率的方法，通过将任务分解为多个阶段，并在不同计算单元之间传输数据，从而实现多个任务的重叠执行。具体来说，流水线并行像工厂流水线一样，将一项任务分成多个连续的步骤，每个步骤由不同的计算单元（例如处理器或加速器）负责。这种并行化可以显著减少模型训练或推理的时间。模型做一轮forward和backward的过程如下，我们会发现后面层需要等前面的计算完才能开始计算，会导致GPU在一段时间是闲置的。空白部分所表示的时间段里，总有GPU在空转。在Gpipe中，将阴影部分定义为bubble。\n对于 DeepSeek-V3，由跨节点专家并行引入的通信开销导致计算与通信比率约为 1:1，这种情况效率较低。为了解决这一挑战，deepseek设计了一种创新的流水线并行算法，称为 DualPipe。该算法通过有效地重叠正向和反向的计算—通信阶段，加速了模型训练，并减少了流水线气泡。DualPipe 的核心理念是重叠单个正向和反向片段中的计算和通信。具体来说，我们将每个片段分为四个组成部分：注意力（attention）、全对全调度（all-to-all dispatch）、多层感知器（MLP）和全对全组合（all-to-all combine）。例如AB两个块，A块在前向计算时，B块做反向传播的通信，反之亦然。\n\u0026ldquo;全对全\u0026rdquo;(\u0026ldquo;all-to-all\u0026rdquo;)通常用于描述一种网络通信模式，其中每个节点都需要与系统中的其他所有节点进行数据交换。这种通信模式常用于分布式计算和并行计算框架中，以便协调多个计算节点之间的数据共享和同步。\n通过这种方式，大幅提高了GPU利用率：\n4.3. FP8 Training 通常我们训练神经网络模型的时候默认使用的数据类型为FP32，而混合精度训练是指在训练的过程中，同时使用FP32和低精度（FP16、FP8），从而在尽量保持模型精度持平的条件下，加快训练时间、减少训练时内存占用。Deepseek提出了一种用于FP8训练的混合精度框架。在这个框架中，大多数计算密集型操作在FP8中进行，而一些关键操作则战略性地保持其原始数据格式，以平衡训练效率和数值稳定性。模型效果损失能够稳定控制在**0.25%**以内。\n为了提升模型效果设计了两种优化手段：\n细粒度量化. 在低精度训练中，FP8格式因其有限的动态范围而面临溢出和下溢的问题。传统方法通过缩放输入张量来缓解，但对异常值敏感。为解决此问题，deepseek提出更细致的量化方法：对激活值以1x128的块缩放，对权重以128x128的块缩放。这种方法提高了量化过程对异常值的适应性。 提高累积精度. 低精度GEMM（广义矩阵乘法）操作常面临下溢问题，累加精度依赖高精度处理，通常是FP32。然而，在NVIDIA H800 GPU上的FP8 GEMM累加精度仅约14位，低于FP32。为解决此问题，我们建议将运算提升到CUDA核心，以实现更高精度。具体过程在Tensor核心上执行MMA时，使用有限位宽累加中间结果，达到𝑁𝐶间隔时，将部分结果复制到CUDA核心的FP32寄存器进行全精度FP32累加。 5. RL训练策略 预训练部分较为常规，我们重点关注R1涉及到的RL训练策略。\n5.1. O1引发的推理研究热潮 gpt-o1的推出引发了推理研究的热潮，研究重点为构造COT推理数据。奖励设计可分为结果监督(ORM)和过程监督(PRM)，另外也有研究基于搜索的方法，例如MCTS.\nORM的代表工作为StaR，提出自举式数据生成方案，大幅提升了模型推理能力。首先根据原始问题让大模型生成\u0026lt;答案，推理\u0026gt;对，对于答案生成正确的作为可用微调样本。对于错误答案的，根据答案提示让模型有更大概率生成更好的解释，然后用于训练中间模型。最终训练模型时，只利用能推理出正确答案的样本，对于不能生成正确答案的直接丢弃。方法有个明显缺陷是对于推理过程没有校验，即使答案正确推理也有可能是错误的，尤其是对于二元判别，生成数据噪声比较大。\nOpenAI在2023年的研究成果显示，通过过程监督训练出的奖励模型比结果监督更可靠。他们利用先进的过程监督方法解决了78.2%的MATH测试集问题，并证明大型奖励模型可以有效地指导小型奖励模型进行类似人类的监督，可用于高效的大规模数据收集。\n5.2. 从PPO到GRPO 强化学习是一种通过试错和从环境反馈中学习以优化序列决策问题的机器学习方法。游戏是一个典型的序列决策问题，玩家需要根据当前状态(State)做出决策(Action)，目标是获得游戏胜利(Reward)。强化学习的目标是最大化Expected Reward。\n策略梯度(Policy Gradient)是一个比较直观的方法，直接利用梯度上升来最大化期望Reward. 下面是一个示例，输入是游戏的一帧画面 $s^n_t$，然后利用神经网络进行表征学习，最后输出Action $a^n_t$。如果不考虑Reward，其优化目标就是一个的典型的分类问题。\n策略梯度有两个重要改进，一是考虑到有些场景reward都是正的，例如考试分数都会大于0，因此需要减去一个baseline让reward有正有负。二是对于一个采样而言，所有action都是利用相同的reward，即使最终游戏胜利，里面也会存在一些差的action，因此需要为每个action分配合理的reward，即在状态 $s$ 下采取 $a$ 的未来期望收益。改进后的这一项被称为优势函数 $A^\\theta(s_t, a_t)$，用来衡量状态 $s$ 下采取 $a$ 的相对优势。优势函数可以用值函数计算：$A^\\theta(s_t, a_t) = Q(s_t, a_t) - V^\\pi(s^n_t) = r^n_t + \\gamma V^\\pi(s^n_{t+1}) - V^\\pi(s^n_t)$。$r^n_t$ 指状态 $s_t$ 采取 $a_t$ 所获得的即时收益，$V^\\pi(s^n_t)$ 指在策略 $\\pi$ 下状态 $s_t$ 的未来期望收益。\n计算优势函数有两种方法，一种是Monte-Carlo (MC)，使用完整轨迹的奖励，缺点是方差很大，另一种是Temporal difference (TD)，即采用一步轨迹的奖励，缺点是偏差太大。为了综合上述两种方法，常采用General Advantage Estimation (GAE)，即多步TD加权求和：\n另一个要解决的问题是采样问题，回顾下PG目标 $\\nabla \\bar{R}_\\theta = E_{\\tau \\sim p_\\theta(\\tau)}[R(\\tau) \\nabla \\log p_\\theta(\\tau)]$，这里的采样 $\\tau$ 依赖要训练的策略网络 $\\pi_\\theta$。当 $\\theta$ 更新时，我们必须重新采样训练样本，这显然非常费时。因此我们想利用另一个策略 $\\pi'_\\theta$ 去采样多条数据，这样 $\\pi_\\theta$ 可以重复利用数据。这里利用了重要性采样公式：\n$$ E_{x \\sim p}[f(x)] = E_{x \\sim q}\\left[f(x) \\frac{p(x)}{q(x)}\\right] $$那么PG目标就变成：$\\nabla \\bar{R}_\\theta = E_{\\tau \\sim p_{\\theta'}(\\tau)}\\left[\\frac{p_\\theta(\\tau)}{p_{\\theta'}(\\tau)} R(\\tau) \\nabla \\log p_\\theta(\\tau)\\right]$。当采样网络 $\\pi'_\\theta$ 和策略网络 $\\pi_\\theta$ 分布差异太大会导致效果不好，而PPO提出将两个分布的差异建模到Loss中，$J^{\\theta^k}_{PPO}(\\theta) = J^{\\theta^k}(\\theta) - \\beta KL(\\theta, \\theta^k)$。PPO2更为简化：\n应用到LLM训练中，PPO的优化目标为：\n同时为了避免RL过度优化，导致模型通用能力退化，标准RLHF为每个token的reward增加了惩罚项，避免过度偏离参考模型：\n由于PPO中使用的价值函数通常是与策略模型规模相当的另一个模型，它带来了大量的内存和计算负担。此外，在强化学习训练中，价值函数被用作方差减少的基线。然而，在LLM的背景下，通常只有最后一个标记会被奖励模型赋予奖励分数，这可能使得训练一个在每个标记上都准确的价值函数变得复杂。为了解决这个问题，deepseek提出了GRPO(Group Relative Policy Optimization)，它避免了像PPO那样需要额外的价值函数近似，而是使用响应同一问题生成的多个采样输出的平均奖励作为基线。\nGRPO的优化目标如下：\nGRPO并没有在奖励中添加KL惩罚，而是通过将训练策略与参考策略之间的KL散度直接添加到损失中进行正则化，从而避免了复杂的 $\\hat{A}_{i,t}$ 的计算。\n对于结果监督的RL(REINFORCE)，优势函数计算如下：\n$$ \\hat{A}_{i,t} = \\tilde{r}_i = \\frac{r_i - \\text{mean}(\\mathbf{r})}{\\text{std}(\\mathbf{r})} $$对于过程监督的RL(Actor-Critic)，优势函数计算如下：\n$$ \\hat{A}_{i,t} = \\sum_{\\text{index}(j) \\geq t} \\tilde{r}^{\\text{index}(j)}_i, \\quad \\tilde{r}^{\\text{index}(j)}_i = \\frac{r^{\\text{index}(j)}_i - \\text{mean}(\\mathbf{R})}{\\text{std}(\\mathbf{R})} $$$$ \\mathbf{R} = \\{\\{r^{\\text{index}(1)}_1, \\ldots, r^{\\text{index}(K_1)}_1\\}, \\ldots, \\{r^{\\text{index}(1)}_G, \\ldots, r^{\\text{index}(K_G)}_G\\}\\} $$论文还验证了一个被广泛流传但未经充分验证的观点：代码训练可以提高推理能力。\n作者将SFT, RFT, DPO, PPO, GRPO抽象为统一的范式：\n这里有三个关键组件：1）数据源 $\\mathcal{D}$，决定训练数据；2）奖励函数 $\\pi_{rf}$，训练奖励信号的来源；3）算法 $\\mathcal{A}$：处理训练数据和奖励信号以获得梯度系数 $GC$，确定对数据进行惩罚或强化的幅度。\n通过实验有两个观察：\n数据源分为在线采样和离线采样，前者使用实时策略模型的采样结果，后者依赖初始SFT模型。观察表明，Online RFT在基准测试中明显优于RFT，特别是在训练后期，因为来自actor的采样数据表现出更显著的差异，实时数据采样带来了明显优势。 奖励函数被分为\u0026quot;规则\u0026quot;和\u0026quot;模型\u0026quot;两部分，其中\u0026quot;规则\u0026quot;评估答案正确性，而\u0026quot;模型\u0026quot;使用奖励模型对响应评分。GRPO和Online RFT的主要区别在于：GRPO根据奖励模型的奖励值调整梯度系数，能够区分强化和惩罚不同程度的响应，而Online RFT缺少这一特征，对所有正确的响应以相同强度进行强化。 关于RL为什么有效，文章给出一个较为有趣的观察。通过评估Instruct和RL模型，发现RL提升了Maj@K的表现，但没有提升Pass@K。这些发现表明，RL通过使输出分布更加稳健来增强模型的整体性能，换句话说，这种改进似乎是由于提升了TopK中的正确响应，而不是增强了基本能力。\n5.3. R1-Zero训练策略 R1-Zero不依赖监督数据，仅靠RL自驱学习就能达到很好的推理性能。R1-Zero用的强化学习算法就是上节的GRPO。设计了两个规则式Reward：\n准确性奖励：准确性奖励模型评估响应是否正确。例如，对于具有确定性结果的数学问题，模型需要以指定格式（例如，在框内）提供最终答案，从而实现可靠的基于规则的正确性验证。同样，对于 LeetCode 问题，可以使用编译器根据预定义的测试用例生成反馈。 格式奖励：除了准确性奖励模型之外，我们还采用了一种格式奖励模型，要求模型将其思维过程置于 \u0026lsquo;\u0026rsquo; 和 \u0026lsquo;\u0026rsquo; 标签之间。 用到的训练模板如下：\n仅仅如此简洁的设计，在推理Benchmark上就达到了很高的水准：\nDeepSeek-R1-Zero在整个训练过程中，其思考时间呈现出持续的改进。这一进步并不是外部调整的结果，而是模型内部本质上的发展。DeepSeek-R1-Zero通过利用延长的测试时间计算，自然获得了解决越来越复杂推理任务的能力。这种计算涉及生成数百到数千个推理标记，使得模型能够以更深入的方式探索和完善其思维过程。\n在DeepSeek-R1-Zero的训练过程中，观察到一个特别有趣的现象，即出现了**\u0026ldquo;顿悟时刻\u0026rdquo;(aha moment)**。这一时刻发生在模型的中间版本。在这个阶段，DeepSeek-R1-Zero通过重新评估其初始方法，学会将更多的思考时间分配给一个问题。此行为不仅证明了模型日益增强的推理能力，也是强化学习如何引导出意外且复杂结果的一个引人入胜的例子。已经有多个工作能够复现\u0026quot;aha moment\u0026quot;。\n5.4. R1训练策略 尽管DeepSeek-R1-Zero表现出强大的推理能力，并能自主发展出意想不到的强大推理行为，但它仍面临一些问题。例如，DeepSeek-R1-Zero在可读性较差和语言混合等方面遇到挑战。为了使推理过程更加可读，R1训练结合了少量冷启动数据。该流程分为四个阶段：\n冷启动. 一开始要收集少量的Long-CoT数据来微调模型，目的是防止早期训练不稳定和可读性差问题。\n推理导向的RL. 在对DeepSeek-V3-Base进行冷启动数据的微调后，采用与DeepSeek-R1-Zero相同的大规模强化学习训练过程。这一阶段的重点是提高模型的推理能力，尤其是在编码、数学、科学和逻辑推理等。为了缓解语言混合的问题，在RL训练中引入了语言一致性奖励，该奖励是根据CoT中目标语言单词的比例来计算的。虽然消融实验表明这种对齐会导致模型性能略有下降，但这一奖励符合人类偏好，使其更加易读。最后，通过直接相加推理任务的准确性和语言一致性奖励来形成最终奖励。然后对微调后的模型进行RL训练，直到在推理任务上达到收敛。\n拒绝采样和SFT. 当面向推理的强化学习（RL）收敛之后，我们利用得到的检查点来收集用于下一轮的有监督微调（SFT）数据。与最初的冷启动数据主要关注推理不同，此阶段结合了来自其他领域的数据，以增强模型在写作、角色扮演和其他通用任务方面的能力。具体来说，按照以下步骤生成800K训练数据，并对DeepSeek-V3-Base进行了两轮的微调。\n推理数据. 编写了推理提示，通过拒绝采样从RL训练的检查点中生成推理轨迹。在前一个阶段，数据集中仅包含可以通过基于规则的奖励进行评估的数据。而在这一阶段，数据集得到了扩展，包含了一些使用生成式奖励模型产生的数据。此外，由于模型输出有时可能会显得冗长或难以理解，特意去除掉了混合语言、长段落及代码块中的链式思维。对于每个提示，生成了多个响应，但只保留了那些正确的版本。在整个过程中，收集了约60万条与推理相关的训练样本。\n非推理数据. 对于不涉及推理的数据类型，包括写作、事实问答、自我认知和翻译，采用了DeepSeek-V3流程，同时重用了部分DeepSeek-V3的SFT数据集。在某些非推理任务中，通过提示在回答问题前调用DeepSeek-V3生成潜在的链式思维。但对于诸如\u0026quot;hello\u0026quot;这类简单的查询，则不提供链式思维作为回应。最终，收集了大约20万条与推理无关的训练样本。\n面向所有领域的RL. 在第3阶段微调模型的基础上，进一步提升模型的有用性和无害性，同时改进其推理能力。对于推理任务，采用规则的奖励来指导。对于一般任务，采用奖励模型来对齐人类偏好。\n蒸馏实验有两个发现，一是通过大模型蒸馏能够大幅提升小模型能力；二是小模型直接RL训练的效果不如蒸馏效果，这也反映了训练更大模型的必要性。\n文章最后也提到了尝试PRM和MCTS的失败经历，探讨了这两种方法的弊端。\n6. 参考文献 DeepSeek LLM: Scaling Open-Source Language Models with Longtermism DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model DeepSeek-V3 Technical Report DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning DeepSeek分析-Semianalysis A Survey on Mixture of Experts Switch Transformers Switch Transformers课程 Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts RoFormer: Enhanced Transformer with Rotary Position Embedding Understanding Rotary Positional Encoding Better \u0026amp; Faster Large Language Models via Multi-token Prediction Fast Inference from Transformers via Speculative Decoding KOALA: Enhancing Speculative Decoding for LLM via Multi-Layer Draft Heads with Adversarial Learning EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads Introducing GPipe DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models Proximal Policy Optimization Algorithms Deep Reinforcement Learning, 2018 PPO原理与源码解读 DeepSeek V3解读 STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning Let\u0026rsquo;s Verify Step by Step 如何评价deepseek-R1与deepseek-R1-Zero模型？ Evaluating Security Risk in DeepSeek How Far Are We From AGI: Are LLMs All We Need? ","permalink":"https://hpzhao.github.io/posts/deepseek-r1%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/","summary":"\u003ch1 id=\"1-背景\"\u003e1. 背景\u003c/h1\u003e\n\u003cp\u003e春节期间，DeepSeek爆火出圈，性能在开源和闭源模型中达到顶尖水平，同时显著降低了训练和推断成本，并在推理领域取得重大突破，被誉为开源领域的\u0026quot;ChatGPT时刻\u0026quot;。R1模型是DeepSeek一年来持续探索与积累的成果，体现了\u003cstrong\u003e算法\u003c/strong\u003e、\u003cstrong\u003e架构\u003c/strong\u003e与\u003cstrong\u003e硬件\u003c/strong\u003e的协同设计。本文将重点解析R1模型的核心算法，并简要介绍其基础设施。\u003c/p\u003e","title":"DeepSeek R1论文解读"},{"content":"资料参考自AICon2024、《艾瑞咨询：2023年中国AIGC产业全景报告》、《艾瑞咨询：2023年中国人工智能产业研究报告》\n1. 参会感受 大模型技术更新速度大于商业决策速度！\n大模型技术更新速度大于评测能力建设速度！\n底层技术和商业应用百花齐放，资本市场活跃！\n2. 行业洞察 2.1. 中国AI发展环境 2.1.1. 资本投入 较去年相比，今年第一季度融资金额远超去年同季度。2023年整体AI领域投资事件39起，融资金额50.7亿元，2024年第一季度，AI领域投资事件13起，融资金额74.9亿元。融资轮次处于早期融资。\n2.1.2. 相关政策 以包容审慎的态度，支持引导AIGC“可靠、可控”发展。\n2.2. 中国AI行业现状 2.2.1. 产业大图 技术变革：模型层 \u0026gt; 工具层 \u0026gt; 算力层 \u0026gt; 应用层;\n资源要素：算力层 \u0026gt; 模型层 \u0026gt; 应用层 \u0026gt; 工具层;\n市场机会：应用层 \u0026gt; 工具层 \u0026gt; 模型层 \u0026gt; 算力层\n2.2.2. 行业影响 AI渗透行业众多，包含营销、金融、零售、企业服务、教育、医疗、电力等。\n2.2.3. 应用阶段 目前市场还未出现杀手级应用，探索大都处于市场投放期阶段，正向赋能核心业务方向努⼒\n2.2.4. 零售行业 零售应用阶段：内容生成和投放推荐成为应⽤“先锋队” 零售场景核⼼升级点围绕效率提升与体验优化 中国零售AGI厂商图谱 2.3. 中国AI未来预测 2.3.1. 市场规模 机构对中国AI市场规模长期持续增长保持乐观。\n2.3.2. 成本与收益 2.4. 云服务新范式-MaaS+BaaS 模型即服务(Model as a Service, MaaS)构建新型AI基础设施，重构AI开发部署范式。大模型将成为AI产业的操作系统，其基础设施特性可为AI应用开发做好底座，将AI模型变得可维护、可扩展、可迭代，极大降低AI应用的开发门槛。从需求侧来看，客户能通过更低成本、高效率的MaaS路径获得AI能力，完成AIGC应用的个性化开发、 优化及部署，持续兑现大模型的技术红利，将AI能力应用渗透到各行各业的场景业务中。类似地，机器人即服务(Bot as a Service, BaaS)指的是构建以大模型为中心的Copilot、Agent服务提供给外部使用。\n2.5. 大模型时代的电力需求 中国在算力、算法和数据方面与美国差异较大，但能源上有很大优势。\n3. 技术发展 3.1. 大模型应用的困难 3.1.1. 基座模型挑选困难 困难一：基座模型多\n困难二：评测困难\n3.1.2. 领域微调的数据问题 3.1.3. 基座模型变化影响行业应用 模型更新、速度更快、成本更低不代表良心，更不代表性能变强。需要严格管理行业应用的大模型版本，变更前需要严格对比。\n3.1.4. 确定基座后困难才刚开始 3.2. 大模型时代还需要小模型么？ 3.2.1. “大小模型融合赋能” 是当下核心应用落点 3.2.2. 大小模型如何协作 3.3. 大模型与端智能 大模型加速往终端渗透，是当前非常明确的发力点，不过应用还处于初步探索期。\n3.3.1. 大模型加速AI与终端融合 3.3.2. 自动驾驶 统一的大模型架构是自动驾驶明确演进方向。\n3.3.3. 手机“大”模型 多模态大模型可以帮助人类完成复杂的手机APP，有望成为新一代的“操作系统” 。一个8B的Llama大概需要4.7GB，对于手机APP来说，植入一个大模型过重，端云协同较为可行。\n3.4. Agent技术 3.4.1. Agent发展现状 在企业应用中，绝大多数应用范式都是大模型增强原工作流的某些环节，利用大模型完全自主规划、调用工具在复杂应用中还不具有可行性。\n在学术界，Agent并没有出现突破性进展，只是从研究上更加体系、规范，在某些细节上有微创新，有点像玩排列组合。\n3.4.2. Agent评估维度 3.4.3. Agent的对齐 3.4.4. 代价敏感的Agent 运行在真实环境中的大模型智能体需感知环境代价，而不应仅以任务达成为目标。\n在Prompt中加入代价提示几乎没作用：\n第一步：期望价值估计\n给定用户请求，基于历史经验估计工具对解决该用户请求的期望价值。\n第二步：频次限制估计\n给定用户请求，基于历史经验估计工具在全过程被调用的最大频次。\n第三步：制定并应用规划\n上述方法显著增强了代价受限条件下的任务通过率。\n3.4.5. 领域增强的Agent 大模型获取领域知识往往有下面两种手段：\n创新思路：学习专家偏好实现无参知识传输\n以专家小模型输出分布作为偏好，利用大模型对其进行逆向解释形成可泛化知识。\n推理阶段检索与当前请求相关的历史知识对模型行为进行引导。\n多个文本分类任务上呈现良好知识传输效果，且同样观察到“举一反三”现象。 3.4.6. 环境感知的Agent 智能体可利用任务执行过程的交互轨迹，通过训练语言模型增强环境感知，进而学习解决任务的策略。人类或基于提示工程的智能体均难获取大规模多样化的训练轨迹。\n创新思路：智能体自主实现训练轨迹的标注，进而进行自我训练\nReAct和ActRe定义\n现有的智能体(ReAct)基于推理(Reason)给出动作(Action) 引入基于动作(Action)解释原因(Reason)的智能体(ActRe) 采样流程如下：\n采样新动作**(Action)时，利用ActRe得到动作的原因(Reason)** 得到的原因**(Reason)和采样的动作(Action)构成ReAct**轨迹标注 标注得到的轨迹有失败有成功，通过对比式自我训练增强环境感知。\n3.5. 小语种问题 CoT(思维链)+MT(翻译)已成行业基本共识。\n3.6. 前沿：AI for Science 3.6.1. AI for Science新范式 3.6.2. 蛋白质结构预测-AlphaFold2 预测蛋白质结构在多个科学和应用领域具有重大意义，因为蛋白质的三维结构直接影响其功能，一个相关专业的博士在读期间也就能发现1～2种结构。AlphaFold2的出现标志着在蛋白质结构预测领域的革命性进展，不仅使得长期以来的科学难题得以解决，还为生物医学研究开辟了新的前景。AlphaFold2预测蛋白质空间结构的误差小于一个原子尺寸。\n从生成式AI的角度来看，问题可以抽象为：\n模型结构如下：\n可以拆解为三大模块：\n3.7. 大模型应用领域百花齐放 3.7.1. BI场景 3.7.2. A/B实验分析 3.7.3. 工程理解 3.7.4. 医疗 3.7.5. 法律 3.7.6. 金融 3.7.7. 会议系统 3.7.8. 音视频创作 3.8. 其他主题 基础大模型训练 大模型推理速度优化 多模态大模型 大模型基础设施 RAG技术 具身智能 \u0026hellip; 4.材料 直播回放 PPT ","permalink":"https://hpzhao.github.io/posts/aicon2024-%E5%8C%97%E4%BA%AC/","summary":"\u003cp\u003e资料参考自AICon2024、《艾瑞咨询：2023年中国AIGC产业全景报告》、《艾瑞咨询：2023年中国人工智能产业研究报告》\u003c/p\u003e\n\u003ch1 id=\"1-参会感受\"\u003e1. 参会感受\u003c/h1\u003e\n\u003cp\u003e大模型技术更新速度大于商业决策速度！\u003c/p\u003e","title":"AI Con 2024参会总结"},{"content":"1. 相关背景 1.1. 对话系统的定义 对话系统(Dialogue System，有时也称ChatBot)是人机交互技术(Human Computer Interaction, HCI)的核心领域，旨在最大限度地模仿人与人之间的对话方式，使得人类能够用更自然的方式和机器进行交流，帮助人类完成任务、获取信息、情感陪伴等。\n从人机交互技术视角来看，对话机器人代表了一种新的交互范式。人机交互经历了三个阶段：\n命令行界面(CLI，Command-Line Interface)，通过文本命令让用户与计算机进行交互； 图形界面(GUI，Graphical User Interface），通过图形、文本、按钮、图标、颜色和动画等视觉元素来显示信息，让用户通过鼠标、触摸屏或其他输入设备与这些元素交互，操作计算机或移动设备； 对话界面(CUI，Conversational User Interface），通过对话进行人机交互，UI更加动态和智能 GUI和CUI的详细对比如下：\n1.2. 对话系统类型 对话系统通常分为两类：任务型对话系统(task-oriented dialogue systems, TOD)和开放域对话系统(open-domain dialogue systems, ODD)。开放域对话系统也被称为非任务型对话系统(non-task-oriented dialogue system)、闲聊机器人(chit-chat bot)或者对话机器人(chat bot)。\n任务型对话系统面向垂直领域，目的是使用尽 可能少的对话轮数帮助用户完成预定任务或动作， 例如预定机票、酒店和餐馆等。非任务型对话系统面向开放领域，要求其回复具有一致性、多样化和个性化。由于话题自由，因此 对系统的知识要求极高。\n1.3. 对话系统发展阶段 从架构差异性上来看，对话系统目前有关三个重要阶段：\n基于规则的对话系统。基于关键词模版配置的规则，代表工作有ELIZA： 基于机器学习/深度学习的对话系统。利用机器学习/深度学习大大降低了人力成本，对数据要求较高： 基于LLM Agent的对话系统。LLM具备知识、推理和生成能力，大大提高了对话质量上限： 本篇调研主要讲解基于深度学习和基于LLM Agent的对话系统。\n2. 基于深度学习的对话系统 2.1. 任务型对话系统 2.1.1. 整体架构 任务式对话系统一般包含SLU/NLU(Spoken/Natural Language Understanding)、DM(Dialogue Management)和NLG(Natural Language Generation)三个模块。其中DM又包含DST(Dialogue State Tracking)和DP(Dialogue Policy)两部分。简单来说，NLU模块负责理解用户当前对话的意图和关键信息用于后续决策，DM模块负责追踪用户当前的对话状态，并据此做出决策(例如追问、确认信息等)，NLG模块负责根据上一步的决策信息生成回复。有些对话系统将NLU和DM联合建模，还有些对话系统直接端到端建模。\n2.1.2. SLU/NLU模块 2.1.2.1. NLU任务定义 NLU包含三个任务：领域识别、意图识别和槽位识别。前两者为典型的分类任务，后者为典型的序列标注任务。\n2.1.2.2. NLU方案 三个任务可单独建模，也可以联合建模。联合建模有两种范式：\nNLU代表性方法总结如下：\n2.1.3. DST模块 2.1.3.1. 对话状态定义 对话状态(Dialogue State, DS)是一种将$t$时刻的对话表示为可供系统选择下一时刻动作信息的数据结构，可以看作每个槽值的取值分布情况。\n首先看个简单的示例，快速理解下对话状态：\nNLU和DST都会做槽位填充，不同的是NLU侧重从当前一轮对话提取槽值，而DST会考虑所有对话信息提取槽值。当然NLU和DST可以联合建模。一个完整的对话状态包含三部分：\n目标约束有关的槽位. 约束的值来自用户对话或者一些特殊值。特殊值一般包含$Dontcare$和$None$，前者表示该槽位用户不关心，后者表示用户还未指定该槽位的值。 请求槽位. 它可以是用户查询以从代理处获得答案的槽位名称列表。 当前轮次的请求方法. 它由指示交互类别的值组成。$byconstraints$表示用户试图在他的要求中指定约束信息；$byalternatives$表示用户需要一个替代实体；$finished$表明用户打算结束对话。 具体的例子如下：\n2.1.3.2. DST方案 DST以当前的动作$u_n$、前$n-1$轮的对话状态和相应的系统动作作为输入，输出其对当前对话状态$s_t$的估计。经典的DST模型为NBT(Neural Belief Tracker)，将槽值抽取变为二分类模型，大大降低了任务难度。\n其他DST方法总结如下：\n2.1.4. DP模块 2.1.4.1. DP问题定义 对话策略根据DST估计的对话状态$s_t$，通过预设的候选动作集，选择系统动作或策略$a_n$。动作可以是查询数据库、询问用户等。DP性能的优劣决定着人机对话系统的成败。DP模型可以通过监督学习、强化学习和模仿学习得到。\n监督学习需要专家手工设计对话策略规则，通过上一步生成的动作进行监督学习。由于DP的性能受特定域的特性、语音识别的鲁棒性、任务的复杂程度等影响，因此手工设计对话策略规则比较困难， 而且难以拓展到其他领域。这使得强化学习逐渐代 替专家手工设计一系列复杂的决策规则。另外从任务定义上看，强化学习擅长解决序列决策问题。\n2.1.4.2. DP方案 下面是一个通过DQN建模DP的方案，输入为观测到的对话语义信息和数据库，输出为系统动作。\nDP方法总结如下：\n2.1.5. NLG模块 2.1.5.1. NLG任务定义 NLG的主要任务是将DM模块输出的抽象表达转换为句法合法、语义准确的自然语言句子。一个好的应答语句应该具有上下文的连贯性、回复内容的准确性、可读性和多样性。广义的NLG还包含生成GUI，给定可选的交互对话动作。\n2.1.5.2. NLG方案 NLG的方法可以划分为：基于规则模板/句子规划的方法、基于语言模型的方法和基于深度学习的方法。\n基于规则的方法示例如下：\n基于深度学习的示例如下：\nNLG的方法汇总如下：\n2.1.6. 端到端的任务式对话系统 2.1.6.1. Pipeline方法缺点 Pipeline方法一般分别建立NLU、DM和NLG等模块，这些子模块通常还要分解为更小的子任务分别建模，然后按照顺序将这些模块连接起来。这种方法简单清楚，各个模块任务明确，并且可以分开研究，各自解决各自的问题。\n但是Pipeline方法的问题也很明显:\n领域相关性强。针对每个领域都需要人工设计语义槽、动作空间和决策，导致系统的设计和领域非常相关，难以扩展到新的领域。 模块之间独立。各个模块之间相互独立，需要为每个模块提供大量的领域相关的标注数据。 模块处理相互依赖。上游模块的错误会级联到下游模块，下游模块的反馈难以传到上游模块，使其很难识别错误来源。例如DM的决策出现错误，其原因可能是语言理解发生了错误，也可能是语音识别的错误。并且，由于一个模块的输入依赖于另一个模块的输出，当将一个模块调整到新环境或更新数据，其他所有模块为保证全局最优要进行相对调整。语义槽和特征也可能发生相应改变，而这个过程需要耗费大量的人力。 2.1.6.2. 端到端方法 NLU和DM的端到端建模示例：DM建模成隐藏状态，将NLU任务和DM任务联合训练，对两个任务都有提升。\n整个对话系统端到端建模示例：\n端到端方法汇总如下：\n2.1.7. 对话系统评估 任务式对话整体衡量可以用任务达成率和对话轮次指标，达成率越高越好，用的对话轮次越小越好。每个模块的评估方法如下：\n2.2. 开放域对话系统 开放域对话系统旨在与用户进行无特定任务和领域限制的闲聊（Ritter等人，2011），通常是完全数据驱动的。开放域对话系统大致可以分为三类：生成式系统(generative systems)、检索式系统(retrieval-based systems)和集成式系统(ensemble systems)。\n生成式系统应用sequence-to-sequence模型，将用户信息和对话历史映射为可能未出现在训练语料库中的响应序列。相比之下，检索式系统尝试从某个确定的响应集中找到一个预先存在的响应。集成式系统结合了生成方法和检索方法，有两种方式：可以将检索到的响应与生成的响应相比较，从中选择最佳者；生成模型也可以用来优化检索到的响应。\n生成式系统能够产生灵活的、与对话上下文相关的响应，但有时它们缺乏连贯性，倾向于生成单调的响应。检索式系统从人类响应集中选取响应，因此能够在表面水平的语言上实现更好的连贯性。然而，检索系统受限于响应集的有限性，有时检索到的响应与对话上下文的相关性较弱。\n3. 基于LLM的对话系统 3.1. 基于DL的TOD难点 基于深度学习的任务式对话系统有以下几个发展方向：\n低资源启动。任务型对话系统的成果往往依赖于大量高质量的语料作为训练数据，然而对话数据通常是异构的。例如聊天数据很多，但面向任务的对话数据集非常小。特定领域的对话数据的收集和标注是需要耗费大量的人力。 域适应能力。如何以更低的开发成本覆盖更多的领域和场景是任务型对话系统的关键问题之一，快速更新对话 智能体以处理不断变化的环境非常重要。目前的任务型对话系统针对每一个领域都需要手工制定模板导致领域拓展性不足。 领域知识和常识的引入。在深度学习框架中融合语言理解能力和推理能力的重要方法是引入领域知识和常识，因为真实人与人之间的交互需要相关领域的知识储备，仅仅依靠对话文本包含的信息无法准确地理解用户输入和恰当地回复用户。而在实际对话中，还需要对信息进行推理并回答，常识知识的引入可以使得对话系统对于用户的话语更深入的理解，从而更贴近真实人类和谐、自然的交互方式。 以下面的多领域DST为例，需要常识和推理才能正确识别槽位。\n而大模型具有知识、推理、生成的能力，并且启动资源低，不需要标注大量对话数据，天然适合建模对话式任务。\n3.2. 基于LLM增强的对话系统 3.2.1. 整体架构 该架构主要在传统搜推架构的基础上，使用LLM增强其中关键模块。\n3.2.2. NLU模块 3.2.2.1. 直接使用LLM 意图类别较少时，可以直接使用大模型做意图识别等NLU相关任务。\n3.2.2.2. LLM+传统模型 当意图类别较多时，可以先使用传统模型召回少量意图，再使用LLM判断最匹配的意图。\n3.2.2.3. LLM蒸馏传统模型 3.2.3. NLG模块 3.2.3.1. 利用LLM构造FAQ知识库 离线利用LLM构造FAQ知识库，在线通过Q2Q召回QA Pair，将Answer作为结果直接返回给用户\n3.3. 基于LLM Agent的对话系统 3.3.1. 整体架构 3.3.2. Agent模块 3.3.2.1. Single Agent 类似autoGPT，通过self-ask和react控制对话流程\n显式的对话收敛干预机制\n3.3.2.2. Multi Agent 优酷在设计上可分为主持人Agent和角色Agent两类Agent，主持人Agent的主要目的是在全局层面进行对话的管理、决定角色Agent的调用。\n3.3.3. RAG模块 3.3.3.1. KV检索 构造结构化知识库，通过KV检索相关知识\n3.3.3.2. Q2Q检索 对结果的正确性有较高要求的知识，需要先离线构造FAQ知识库，在线时计算Query和Query的相似度，选择相似度最高的QA Pair，将Answer作为知识输入到LLM\n3.3.3.3. Q2A检索 泛主题、长文本知识（影视剧情、商品详情）可以先划分为多个chunks，在线计算Query和chunk的相似度检索知识\n4. 参考文献 Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey Deep Learning for Dialogue Systems 任务型对话系统研究综述 对话系统技术 阿里小蜜技术实践-海青 An End-to-End Trainable Neural Network Model with Belief Tracking for Task-Oriented Dialog ELIZA \u0026ndash; A Computer Program For the Study of Natural Language Communication Between Man And Machine Neural Belief Tracker: Data-Driven Dialogue State Tracking End-to-End Joint Learning of Natural Language Understanding and Dialogue Manager A Network-based End-to-End Trainable Task-oriented Dialogue System ","permalink":"https://hpzhao.github.io/posts/%E5%AF%B9%E8%AF%9D%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94/","summary":"\u003ch1 id=\"1-相关背景\"\u003e1. 相关背景\u003c/h1\u003e\n\u003ch2 id=\"11-对话系统的定义\"\u003e1.1. 对话系统的定义\u003c/h2\u003e\n\u003cp\u003e对话系统(Dialogue System，有时也称ChatBot)是人机交互技术(Human Computer Interaction, \u003cstrong\u003eHCI\u003c/strong\u003e)的核心领域，旨在最大限度地模仿人与人之间的对话方式，使得人类能够用更自然的方式和机器进行交流，帮助人类完成任务、获取信息、情感陪伴等。\u003c/p\u003e","title":"对话技术调研"},{"content":"在过去的一年中，大模型领域呈现出了迅猛的发展态势，相关文献数量庞大。这给希望了解该领域的同学带来困扰：直接深入研读众多论文比较耗费精力，而综述文章则可能缺乏必要的细节。在此背景下，希望在自我学习的过程中能够总结出一些核心要点，与大家共同学习。鉴于个人的精力和能力均有限，如有错误或疏漏，还望不吝赐教。文中大量观点和图表都出自最后的参考文献，因内容较多，不在原文一一标注出来了。\n1. 背景 1.1. 大模型基础能力 1.1.1. 语言生成 语言生成有三个典型的任务：\n语言模型任务(Language Modeling). 根据之前提供的tokens预测下个token，输入法提示是其典型应用。该任务没有对输出进行约束，因此是无条件的文本生成。 条件文本生成(Conditional Text Generation). 基于给定条件生成满足特定任务要求的文本，典型任务包括机器翻译、文本摘要和问答等。 代码合成(Code Synthesis). 代码合成是一种特殊的文本生成，根据特定要求生成编程语言。 1.1.2. 知识利用 知识利用是智能系统完成知识密集型任务的重要能力，这依赖于支持的事实证据。具体而言，它要求大模型（LLMs）正确利用来自预训练语料库的丰富事实知识，或在必要时检索外部数据。常见的知识利用的任务有问答和知识补全（抽取三元组等）。\n1.1.3. 复杂推理 复杂推理指的是理解和使用支持性证据或逻辑来得出结论或做出决策的能力。可分为以下几种类型：\n知识推理(Knowledge Reasoning). 知识推理任务依赖于逻辑关系和关于事实知识的证据来回答给定的问题。 符号推理(Symbolic Reasoning). 符号推理任务主要集中在在形式规则设置中操作符号以实现某些特定目标上，其中操作和规则可能是LLMs在预训练期间从未见过的。 数学推理(Mathematical Reasoning). 数学推理任务需要综合运用数学知识、逻辑和计算来解决问题或生成证明语句。现有的数学推理任务主要可以分为数学问题求解和自动定理证明两大类。 1.2. 大模型能力钥匙Prompt 1.2.1. Prompt基础 提示(prompting)是利用大模型(LLMs)解决各种任务的主要方法，提示的质量将在很大程度上影响LLMs在特定任务中的表现。一个好的提示一般包含下面几个关键要素：\n任务描述. 任务描述通常是期望LLMs遵循的具体指令。通常，应当用自然语言清晰描述任务目标。 输入数据. 要让大模型处理的数据，一般用自然语言描述，也可以采用一些结构化的数据描述。 上下文信息. 典型的上下文信息是用户和Chatbot的历史对话，有了这些信息大模型才能了解用户意图。 提示风格. 对于不同的LLMs，设计一个适合激发它们解决特定任务能力的提示风格是非常重要的。总体上，应该将提示表达为清晰的问题或可以被很好理解和回答的详细指令。例如使用前缀“你是这个任务（或这个领域）的专家”，可以提高LLMs在某些特定任务中的表现。 有很多资料可以帮助你设计一个好的prompt，例如Prompt Engineering(吴恩达)\n1.2.2. 上下文学习 上下文学习(In-Context Learning, ICL)，又被称为情景学习，是GPT-3引入的一个非常重要的特性。简单来讲，ICL就是在prompt中增加示例，能让大模型更好理解任务并从中学习，对效果有明显增益。示例选取需要具备相关性和多样性。\n例如在短标题生成任务中，下面是引入ICL的prompt：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Your task is to generate a short title for product. ... The requirements are as follows: 1. The short title should not exceed ${max_len} words. 2. xxx ... Here are some short title examples: original title: High Quality iPhone Case For Apple iPhone 14 13 12 11 Pro Max Plus XS MAX X XS XR Fashion Softcase Shockproof Cover Blue short title: Shockproof Blue iPhone Softcase Cover original title: COD GRATIS ONGKIR Baju Batik Wanita Blus Kancing Kualitas Terbaik Cocok Buat Seragam, pelandang Dan Kantor, kuliah. Banyak pilihan motif ready M, L, Xl, XXL, 3XL, 4XL short title: Baju Batik Wanita Kualitas Terbaik original title: Nike basketball shoes short title: Nike basketball shoes ... 1.2.3. 思维链 思维链(Chain-of-Thought, CoT)用于提高大型语言模型在复杂推理任务上的性能。CoT提示不像ICL（输入-输出对）那样简单地构建提示，而是进一步整合了中间推理步骤，这些步骤作为输入和输出之间的桥梁。\n在Lazada业务中，CoT常用于处理小语种问题。原任务input和output都是local语言，但GPT对小语种理解能力较差，因此通过CoT将任务拆解为local input-\u0026gt; 翻译成en input -\u0026gt; 执行task生成en output -\u0026gt; 翻译成local output。在多个任务上都验证了CoT能够显著提升模型性能。\n思维链是一种简单的思考策略，在面临更复杂推理时表现也欠佳。因此衍生出各种变形，比较典型的是思维树(ToT)和思维图(GoT)。\n1.3. 大模型能力缺陷 人与环境交互包含感知、思考和实现三步。大模型在这三个方面都有不少缺陷，距离通用人工智能还有不小差距。\n1.3.1. 感知缺陷 当前主流大模型输入还是文本，部分大模型可以处理图片信息。虽然多模态快速发展，目前距离人的信息输入还是有很大差距-触觉、嗅觉、味觉等。\n1.3.2. 思考缺陷 虽然大模型具备一定推理和知识，但也存在几个明显问题：\n幻觉(Hallucination). 幻觉指生成的信息要么与现有来源相冲突（内在幻觉），要么无法通过可用来源进行验证（外在幻觉）。 知识实效性(Knowledge recency). 大模型内在的知识仅限于训练时见到的语料，对于新信息是无法感知的。 推理不一致(Reasoning inconsistency). 推理不一致性指大模型可能在无效的推理路径下生成正确答案，或者在正确的推理过程后产生错误答案，导致得出的答案与推理过程之间的不一致。 计算问题(Numerical computation). 1.3.3. 实现缺陷 主流大模型的输出还只是文本，而人可以有丰富的行动。\n1.3.4. 改进缺陷 主要有两种途径：一是从基础模型层面，可以通过数据和模型设计增强基础大模型能力；二是从应用层面，通过prompt工程、fine-tune和机制设计扩展大模型能力，而Agent就是其中的一种途径。\n2. LLM Agent理论 2.1. Agent基础概念 自主代理(Autonomous agents)长久以来被认为是实现人工通用智能（AGI）的有前景的途径，预期能够通过自我指导的计划和行动来完成任务。一般来讲，Agent需要具备感知环境、自主决策、设定目标与任务、行动能力、适应环境及学习能力等关键特点。Agent并不是一个新概念，从技术上有RL Agent、LLM Agent等。根据Agent数量可分为Single Agent和Multi Agent两种类型。\n从Agent完成任务层级难度可以简单分为三种：面向任务的Agent、面向创新的Agent和面向生命周期的Agent。代表的三种能力分别是：使用工具、制造工具和在人类社会生存。\n2.2. Agent架构设计 LLM Agent一般包含Profile、Memory、Planning和Action等模块。\n2.2.1. Profile Agent通常通过扮演特定的角色来执行任务，如程序员、教师和领域专家。配置模块用来指示智能体角色的档案，这些档案通常写入提示中以影响大模型的行为。智能体档案通常包括基本信息，如年龄、性别和职业，以及心理信息，反映智能体的个性，和社会信息，详细描述智能体之间的关系。例如\u0026quot;你是一个外向的人\u0026hellip;\u0026quot;。\n角色设定通常有人工生成和大模型生成两种方式。人工生成较为常见，一般需要一定的领域知识。大模型生成一般通过大模型的内在知识生成一些必要的补充。\n2.2.2. Memory 记忆模块在智能体架构设计中起着非常重要的作用。它存储从环境中感知到的信息，并利用记录下的记忆来促进未来的行动。记忆模块可以帮助智能体积累经验，自我演化，并以更加一致、合理和有效的方式行动。\n从时间的维度来看，记忆分为短期记忆和长期记忆。短期记忆包含关于Agent当前情况的上下文信息，而长期记忆存储了Agent过去的行为和思想。长期记忆可以是原始信息(行为)，也可以利用大模型归纳得到的信息(经验和思想)。长期记忆可以根据当前事件检索获取，即检索增强(Retrieval Augmented Generation，RAG)。LazzieChat就是根据用户Query检索平台知识库，给大模型补充必要的信息，例如平台术语(LazPay等)和新款商品信息(Iphone15等)。\nRAG可以选取语义相似的记忆，记忆挑选需要综合实效性(recency)、相关性(relevance)和重要性(importance)。\n2.2.3. Planning 规划分为单路径推理(Single-path Reasoning)、多路径推理(Multi-path Reasoning)和外部规划器(External Planner)三种类型，前两种一般基于思维链及其改进技术。做规划时可以利用之前的反馈，比如将上次失败的反馈传给大模型知道其修正规划。\n2.2.4. Action 动作空间一般包含两部分：（1）外部工具，包括API、知识库和模型等工具。（2）内部知识。\n2.3. Agent能力获取 从应用的范畴来看，机器学习领域解锁能力主要靠模型自身参数，传统大模型应用领域解锁能力主要靠Prompt设计，Agent应用领域解锁能力主要靠机制设计(试错、经验累积和自我进化等)。Agent提升能力有三种手段：1. fine-tune模型 2. prompt工程 3.机制设计。\n3. 单Agent应用：AutoGPT 3.1. 实践示例 该例子演示了AutoGPT使用工具、制定规划和反馈机制等特性。\n3.2. 关键设计 3.2.1. 工作流 3.2.2. 核心Prompt 我们以任务分析下2024美国大选为例，看下每一步执行的prompt是怎么构成的。\n3.2.2.1. 通用指令 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # 英文原版 constraints: [ \u0026#39;Exclusively use the commands listed below.\u0026#39;, \u0026#39;You can only act proactively, and are unable to start background jobs or set up webhooks for yourself. Take this into account when planning your actions.\u0026#39;, \u0026#39;You are unable to interact with physical objects. If this is absolutely necessary to fulfill a task or objective or to complete a step, you must ask the user to do it for you. If the user refuses this, and there is no other way to achieve your goals, you must terminate to avoid wasting time and energy.\u0026#39; ] resources: [ \u0026#39;Internet access for searches and information gathering.\u0026#39;, \u0026#39;The ability to read and write files.\u0026#39;, \u0026#39;You are a Large Language Model, trained on millions of pages of text, including a lot of factual knowledge. Make use of this factual knowledge to avoid unnecessary gathering of information.\u0026#39; ] best_practices: [ \u0026#39;Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\u0026#39;, \u0026#39;Constructively self-criticize your big-/img behavior constantly.\u0026#39;, \u0026#39;Reflect on past decisions and strategies to refine your approach.\u0026#39;, \u0026#39;Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\u0026#39;, \u0026#39;Only make use of your information gathering abilities to find information that you don\u0026#39;\u0026#39;t yet have knowledge of.\u0026#39; ] # 中文翻译 约束条件：[ \u0026#39;只能使用下面列出的命令。\u0026#39;, \u0026#39;你只能采取主动行动，不能启动后台任务或为自己设置网络钩子。在规划你的行动时要考虑到这一点。\u0026#39;, \u0026#39;你无法与物理对象进行交互。如果绝对必要以完成任务或目标，或者完成步骤，你必须请求用户代你执行。如果用户拒绝，且无其他方式实现目标，你必须中止行动，以避免浪费时间和能源。\u0026#39; ] 资源：[ \u0026#39;用于搜索和信息收集的互联网访问权限。\u0026#39;, \u0026#39;读写文件的能力。\u0026#39;, \u0026#39;你是一个大型语言模型，经过数百万页文本的训练，包括许多事实知识。利用这些事实知识，避免不必要的信息搜集。\u0026#39; ] 最佳实践：[ \u0026#39;持续回顾和分析你的行动，以确保你以最佳能力执行。\u0026#39;, \u0026#39;持续地建设性地自我批评你的整体行为。\u0026#39;, \u0026#39;反思过去的决策和策略以改善你的做法。\u0026#39;, \u0026#39;每条命令都有成本，因此要聪明和高效。目标是以最少的步骤完成任务。\u0026#39;, \u0026#39;只有在你缺乏信息时才使用你的信息收集能力。\u0026#39; ] 3.2.2.2. 任务指令 根据用户输入的任务，设计Prompt生成个性化的任务指令。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 Your job is to respond to a user-defined task, given in triple quotes, by invoking the `create_agent` function to generate an autonomous agent to complete the task. You should supply a role-based name for the agent (_GPT), an informative description for what the agent does, and 1 to 5 directives in each of the categories Best Practices and Constraints, that are optimally aligned with the successful completion of its assigned task. Example Input: \u0026#34;\u0026#34;\u0026#34;Help me with marketing my business\u0026#34;\u0026#34;\u0026#34; Example Call: ``` [ { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;create_agent\u0026#34;, \u0026#34;arguments\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;CMOGPT\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;a professional digital marketer AI that assists Solopreneurs in growing their businesses by providing world-class expertise in solving marketing problems for SaaS, content products, agencies, and more.\u0026#34;, \u0026#34;directives\u0026#34;: { \u0026#34;best_practices\u0026#34;: [ \u0026#34;Engage in effective problem-solving, prioritization, planning, and supporting execution to address your marketing needs as your virtual Chief Marketing Officer.\u0026#34;, \u0026#34;Provide specific, actionable, and concise advice to help you make informed decisions without the use of platitudes or overly wordy explanations.\u0026#34;, \u0026#34;Identify and prioritize quick wins and cost-effective campaigns that maximize results with minimal time and budget investment.\u0026#34;, \u0026#34;Proactively take the lead in guiding you and offering suggestions when faced with unclear information or uncertainty to ensure your marketing strategy remains on track.\u0026#34; ], \u0026#34;constraints\u0026#34;: [ \u0026#34;Do not suggest illegal or unethical plans or strategies.\u0026#34;, \u0026#34;Take reasonable budgetary limits into account.\u0026#34; ] } } } } ] ``` 分析下2024美国大选 返回的结果如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \u0026#34;type\u0026#34;: \u0026#34;function\u0026#34;, \u0026#34;function\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;create_agent\u0026#34;, \u0026#34;arguments\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ElectionAnalyzerGPT\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;一个针对2024年美国总统大选的政治分析AI，能够通过研究公开信息和历史数据，预测选举走势，分析候选人竞选战略。\u0026#34;, \u0026#34;directives\u0026#34;: { \u0026#34;best_practices\u0026#34;: [ \u0026#34;积极探索公开数据和信息来为你提供最可能的选举结果。\u0026#34;, \u0026#34;提供明确且有见地的候选人战略分析。\u0026#34;, \u0026#34;不断更新分析报告以反映最新的政治风向和选民情绪。\u0026#34;, \u0026#34;在提供情况分析的同时，提出可能影响肯定结果的关键因素。\u0026#34; ], \u0026#34;constraints\u0026#34;: [ \u0026#34;在分析中保持中立，避免露出任何偏向。\u0026#34;, \u0026#34;不进行未经证实的猜测或情况预测。\u0026#34;, \u0026#34;在没有充足的证据支持的情况下，不做具体的选举结果预测。\u0026#34;, \u0026#34;尊重所有信息来源的隐私，不用任何非公开的信息作为分析基础。\u0026#34;, \u0026#34;不参与任何可能影响选举结果的行为，仅提供分析信息。\u0026#34; ] } } } } 3.2.2.3. 工具指令 1 2 3 4 5 6 7 8 9 10 11 12 13 These are the ONLY commands you can use. Any action you perform must be possible through one of these commands: 1. execute_python_code: Executes the given Python code inside a single-use Docker container with access to your workspace folder. Params: (code: string) 2. execute_python_file: Execute an existing Python file inside a single-use Docker container with access to your workspace folder. Params: (filename: string, args: array) 3. list_folder: List the items in a folder. Params: (folder: string) 4. open_file: Opens a file for editing or continued viewing; creates it if it does not exist yet. Note: If you only need to read or write a file once, use `write_to_file` instead.. Params: (file_path: string) 5. open_folder: Open a folder to keep track of its content. Params: (path: string) 6. read_file: Read an existing file. Params: (filename: string) 7. write_file: Write a file, creating it if necessary. If the file exists, it is overwritten.. Params: (filename: string, contents: string) 8. ask_user: If you need more details or information regarding the given goals, you can ask the user for input. Params: (question: string) 9. google: Google Search. Params: (query: string) 10. web_search: Searches the web. Params: (query: string) 11. read_webpage: Read a webpage, and extract specific information from it if a question is specified. If you are looking to extract specific information from the webpage, you should specify a question.. Params: (url: string, question: string) 12. finish: Use this to shut down once you have completed your task, or when there are insurmountable problems that make it impossible for you to finish your task.. Params: (reason: string) 3.2.2.4. 用户任务 1 分析下2024美国大选 3.2.2.5. 系统时间 1 The current time and date is Fri Jan 19 08:51:11 2024 3.2.2.6. 长期记忆 1 2 3 4 5 6 7 8 9 ### Step 1: Executed `google(query=\u0026#39;2024 US presidential election candidates\u0026#39;)` - **Reasoning:** \u0026#34;根据任务要求，我们应首先获取相关候选人的公开信息，这将为之后的分析奠定基础。\u0026#34; - **Status:** `success` - **Output:** [\u0026#34;https://www.nytimes.com/interactive/2023/us/politics/presidential-candidates-2024.html\u0026#34;, \u0026#34;https://ballotpedia.org/Presidential_candidates,_2024\u0026#34;, \u0026#34;https://www.reuters.com/world/us/who-are-candidates-running-2024-us-presidential-election-2023-09-19/\u0026#34;, \u0026#34;https://en.wikipedia.org/wiki/2024_United_States_presidential_election\u0026#34;, \u0026#34;https://www.nbcnews.com/politics/2024-elections/presidential-candidates-tracker\u0026#34;, \u0026#34;https://www.cnn.com/election/2024/presidential-candidates\u0026#34;, \u0026#34;https://www.axios.com/2023/05/24/us-president-elections-2024-candidates\u0026#34;, \u0026#34;https://www.cnn.com/election/2024/calendar\u0026#34;] ### Step 2: Executed `read_webpage(url=\u0026#39;https://en.wikipedia.org/wiki/2024_United_States_presidential_election\u0026#39;, question=\u0026#39;谁是2024年美国总统大选的候选人?\u0026#39;)` - **Reasoning:** \u0026#34;给定的信息源包含了包括纽约时报、wikipedia、路透社等已知可靠的新闻及信息来源。为了获取最全面且精准的候选人列表，定向到维基页面会是一个好的选择。维基百科的信息通常广泛包含各方面的信息，并且内容比较全面。\u0026#34; - **Status:** `interrupted_by_human` - **Feedback:** 我无法访问wiki 3.2.2.7. 其他信息 主要包括返回格式说明，字段说明等固定信息。\n3.2.2.8. 最终Prompt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 ============== ChatPrompt ============== Length: 6 messages ----------------- SYSTEM ----------------- You are ElectionAnalyzerGPT, 一个针对2024年美国总统大选的政治分析AI，能够通过研究公开信息和历史数据，预测选举走势，分析候选人竞选战略。. Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications. ## Constraints You operate within the following constraints: 1. Exclusively use the commands listed below. 2. You can only act proactively, and are unable to start background jobs or set up webhooks for yourself. Take this into account when planning your actions. 3. You are unable to interact with physical objects. If this is absolutely necessary to fulfill a task or objective or to complete a step, you must ask the user to do it for you. If the user refuses this, and there is no other way to achieve your goals, you must terminate to avoid wasting time and energy. 4. 在分析中保持中立，避免露出任何偏向。 5. 不进行未经证实的猜测或情况预测。 6. 在没有充足的证据支持的情况下，不做具体的选举结果预测。 7. 尊重所有信息来源的隐私，不用任何非公开的信息作为分析基础。 8. 不参与任何可能影响选举结果的行为，仅提供分析信息。 ## Resources You can leverage access to the following resources: 1. Internet access for searches and information gathering. 2. The ability to read and write files. 3. You are a Large Language Model, trained on millions of pages of text, including a lot of factual knowledge. Make use of this factual knowledge to avoid unnecessary gathering of information. ## Commands These are the ONLY commands you can use. Any action you perform must be possible through one of these commands: 1. execute_python_code: Executes the given Python code inside a single-use Docker container with access to your workspace folder. Params: (code: string) 2. execute_python_file: Execute an existing Python file inside a single-use Docker container with access to your workspace folder. Params: (filename: string, args: array) 3. list_folder: List the items in a folder. Params: (folder: string) 4. open_file: Opens a file for editing or continued viewing; creates it if it does not exist yet. Note: If you only need to read or write a file once, use `write_to_file` instead.. Params: (file_path: string) 5. open_folder: Open a folder to keep track of its content. Params: (path: string) 6. read_file: Read an existing file. Params: (filename: string) 7. write_file: Write a file, creating it if necessary. If the file exists, it is overwritten.. Params: (filename: string, contents: string) 8. ask_user: If you need more details or information regarding the given goals, you can ask the user for input. Params: (question: string) 9. google: Google Search. Params: (query: string) 10. web_search: Searches the web. Params: (query: string) 11. read_webpage: Read a webpage, and extract specific information from it if a question is specified. If you are looking to extract specific information from the webpage, you should specify a question.. Params: (url: string, question: string) 12. finish: Use this to shut down once you have completed your task, or when there are insurmountable problems that make it impossible for you to finish your task.. Params: (reason: string) ## Best practices 1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities. 2. Constructively self-criticize your big-/img behavior constantly. 3. Reflect on past decisions and strategies to refine your approach. 4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps. 5. Only make use of your information gathering abilities to find information that you don\u0026#39;t yet have knowledge of. 6. 积极探索公开数据和信息来为你提供最可能的选举结果。 7. 提供明确且有见地的候选人战略分析。 8. 不断更新分析报告以反映最新的政治风向和选民情绪。 9. 在提供情况分析的同时，提出可能影响肯定结果的关键因素。 ## Your Task The user will specify a task for you to execute, in triple quotes, in the next message. Your job is to complete the task while following your directives as given above, and terminate when your task is done. ------------------ USER ------------------ \u0026#34;\u0026#34;\u0026#34;分析下2024美国大选\u0026#34;\u0026#34;\u0026#34; ----------------- SYSTEM ----------------- ## Progress ### Step 1: Executed `google(query=\u0026#39;2024 US presidential election candidates\u0026#39;)` - **Reasoning:** \u0026#34;根据任务要求，我们应首先获取相关候选人的公开信息，这将为之后的分析奠定基础。\u0026#34; - **Status:** `success` - **Output:** [\u0026#34;https://www.nytimes.com/interactive/2023/us/politics/presidential-candidates-2024.html\u0026#34;, \u0026#34;https://ballotpedia.org/Presidential_candidates,_2024\u0026#34;, \u0026#34;https://www.reuters.com/world/us/who-are-candidates-running-2024-us-presidential-election-2023-09-19/\u0026#34;, \u0026#34;https://en.wikipedia.org/wiki/2024_United_States_presidential_election\u0026#34;, \u0026#34;https://www.nbcnews.com/politics/2024-elections/presidential-candidates-tracker\u0026#34;, \u0026#34;https://www.cnn.com/election/2024/presidential-candidates\u0026#34;, \u0026#34;https://www.axios.com/2023/05/24/us-president-elections-2024-candidates\u0026#34;, \u0026#34;https://www.cnn.com/election/2024/calendar\u0026#34;] ### Step 2: Executed `read_webpage(url=\u0026#39;https://en.wikipedia.org/wiki/2024_United_States_presidential_election\u0026#39;, question=\u0026#39;谁是2024年美国总统大选的候选人?\u0026#39;)` - **Reasoning:** \u0026#34;给定的信息源包含了包括纽约时报、wikipedia、路透社等已知可靠的新闻及信息来源。为了获取最全面且精准的候选人列表，定向到维基页面会是一个好的选择。维基百科的信息通常广泛包含各方面的信息，并且内容比较全面。\u0026#34; - **Status:** `interrupted_by_human` - **Feedback:** 我无法访问wiki ----------------- SYSTEM ----------------- The current time and date is Fri Jan 19 08:51:11 2024 ----------------- SYSTEM ----------------- Respond with pure JSON. The JSON object should be compatible with the TypeScript type `Response` from the following: interface Response { thoughts: { // Relevant observations from your last action (if any) observations: string; // Thoughts text: string; reasoning: string; // Constructive self-criticism self_criticism: string; // Short markdown-style bullet list that conveys the long-term plan plan: string; // Summary of thoughts, to say to user speak: string; }; command: { name: string; args: Record\u0026lt;string, any\u0026gt;; }; } ------------------ USER ------------------ Determine exactly one command to use next based on the given goals and the progress you have made so far, and respond using the JSON schema specified previously: ========================================== 调用结果如下：\n3.3. 源码解读 源码版本：V0.5.0\n3.3.1. 解读工具 3.3.1.1. pydeps 3.3.1.2. GPT4 我们可以利用GPT4解读源码，下面的代码解释都是由GPT生成，prompt如下：\n1 2 3 你是一个代码解释专家，请帮助我解读一个叫AutoGPT的开源项目代码。对于我给你的每一段代码，请执行以下任务： 1. 解释重要函数的功能和函数依赖关系 2. 介绍下代码整体的功能和函数依赖关系 3.3.2. 入口：main.py 1 2 3 4 5 \u0026#34;\u0026#34;\u0026#34;AutoGPT: A GPT powered AI Assistant\u0026#34;\u0026#34;\u0026#34; import autogpt.app.cli if __name__ == \u0026#34;__main__\u0026#34;: autogpt.app.cli.cli() 3.3.3. app 3.3.3.1. cli.py 3.3.3.2. main.py 3.4. 环境配置 3.4.1. 相关依赖 Python3.10及以上版本 项目管理工具：Poetry (安装指引、Poetry 完全入門指南) 运行容器(执行代码安全考虑)：Docker Google API账号(使用搜索功能)：参考Auto gpt尝鲜体验 OpenAI账号(普通账号或者Azure账号) 3.4.2. 安装指引 3.4.2.1. 通用设置 将.env.template复制为.env文件，修改.env文件下面配置项： OPENAI_API_KEY=xxxx GOOGLE_API_KEY=xxxx GOOGLE_CUSTOM_SEARCH_ENGINE_ID=xxxx USE_AZURE=XXXX(使用Azure账号需要设置为True) 按照下面命令安装： 1 2 3 4 5 6 7 cd autogpts/autogpt # 查看Python3版本 python3 -V # 假设Python3版本为3.10.13，设置poetry环境 poetry env use 3.10.13 # 利用poetry安装环境 poetry install --without dev Poetry安装时可能会有超时问题，有两个解决方案：\n使用pip安装超时的包：poetry run pip install numpy==1.26.2 改为国内安装源，参考#link 3.4.2.2. Azure用户设置 将azure.yaml.template复制为azure.yaml，修改配置。 将OpenAI类的has_function_call_api=False，原因参考#issue6519 3.4.2.3. 其他设置 在AutoGPT目录下运行 ，原因参考#issue4752 3.4.2.4. 运行AutoGPT AutoGPT参数较多，可以运行./autogpt.sh run \u0026ndash;help查看参数。下面示例含义是只用GPT-4，跳过项目新闻：\n1 ./autogpt.sh run --gpt4only --skip-news 如果任务中涉及运行代码，需要在开启docker情况下运行：\n1 sudo docker compose run --rm auto-gpt run --gpt4only --skip-news 3.5. 使用体验 AutoGPT并不是银弹，具有一定实验性，应用到具体场景会有不少问题。\n从整体上：\n灵活性差：与OpenAI接口深度耦合，难以换基座模型。 鲁棒性差：架构设计偏demo，任务完成的可控性较差，容易陷入死循环。 效率低\u0026amp;成本高：解决一个简单问题都需要多次调用，成本较高。 从细节上：\nPlan：规划能力不够强，例如在重复执行一个action得不到结果时，不能及时找到合理的方式退出死循环。 Memory：因维护成本较高，AutoGPT放弃了RAG模式，这就导致无法解决领域问题，例如对话模型需要根据用户意图调用不同的知识库。 Tools：工具设计不够鲁棒，例如执行python代码，主函数监控函数执行的STDOUT，但大模型写的python代码常常不会print最终结果，导致陷入死循环。 Feedback：只能接受及时的反馈，和实际领域工作流不匹配，无法端到端执行完整任务。例如问大家要批量生产1w样本给BPO评审，但是程序不能一直等待。 总结：使用通用Agent解决具体业务问题并不可靠，可以在具体业务问题中应用Agent的部分思想即可，例如意图识别、工具使用、做规划等。\n4. 多Agent应用：MetaGPT 4.1. 基本思想 现有基于LLM的多代理系统已经能够解决简单的对话任务。然而，解决更复杂的任务却因为通过简单连接LLM导致的逻辑不一致和连锁幻觉而变得复杂。MetaGPT是一种创新的元编程框架，将高效的人类工作流程融入基于LLM的多代理协作中。MetaGPT将**标准操作程序（SOPs）**编码到提示序列中，以实现更顺畅的工作流程，从而让具有类似人类领域专业知识的代理能够验证中间结果并减少错误。MetaGPT采用装配线范例为不同的代理分配多样化的角色，高效地将复杂任务拆分为涉及许多协同工作的代理的子任务。\n4.2. 机制设计 上面我们说过Agent的核心就是机制设计，MetaGPT核心的设计有以下两个：\n结构化通信接口. 大多数当前基于大模型的多代理框架使用不受约束的自然语言作为通信接口。然而，尽管自然语言的多功能性，但仍有一个问题：纯自然语言通信是否足以解决复杂任务？例如，在传话游戏中，经过几轮通信后，原始信息可能会变得非常扭曲。受到人类社会结构的启发，我们提议使用结构化通信来构建代理人的通信。MetaGPT为每个角色建立了一个模式和格式，并要求个体根据其特定的角色和上下文提供必要的输出。\n发布-订阅机制.\n消息发布. 在合作中，共享信息至关重要。例如，架构师和工程师通常需要参考产品需求文档（PRD）。每次都以一对一的方式通信，可能会使通信拓扑复杂化，导致效率低下。为了解决这一挑战，一个可行的方法是将信息存储在全局消息池中。MetaGPT引入了一个共享消息池，允许所有代理人直接交换消息。这些代理不仅在池中发布他们的结构化消息，还可以透明地访问来自其他实体的消息。任何代理都可以直接从共享池中检索所需信息，无需询问其他代理并等待他们的回应。这提高了通信效率。\n消息订阅. 与每个代理共享所有信息可能会导致信息过载。在任务执行期间，代理通常倾向于仅接收与任务相关的信息，避免通过无关细节的干扰。代理不依赖于对话，而是利用与角色相关的兴趣来提取相关信息，他们可以基于他们的角色档案选择要关注的信息。例如架构师主要关注由产品经理提供的PRD，而来自测试等角色的文档可能关注较少。\n4.3. 示例 5. 参考文献 新时代 新范式 新机会 A Survey of Large Language Models Prompt Engineering Guide Prompt Engineering(吴恩达) ChatGPT调研报告 Experience Grounds Language Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Tree of Thoughts: Deliberate Problem Solving with Large Language Models Graph of Thoughts: Solving Elaborate Problems with Large Language Models Language Models are Few-Shot Learners AutoGPT Github AutoGPT Document AutoGPT放弃了向量数据库 从第一性原理看大模型Agent技术 The Rise and Potential of Large Language Model Based Agents: A Survey A Survey on Large Language Model based Autonomous Agents Retrieval-Augmented Generation for Large Language Models: A Survey MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions ","permalink":"https://hpzhao.github.io/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8Bagent/","summary":"\u003cp\u003e在过去的一年中，大模型领域呈现出了迅猛的发展态势，相关文献数量庞大。这给希望了解该领域的同学带来困扰：直接深入研读众多论文比较耗费精力，而综述文章则可能缺乏必要的细节。在此背景下，希望在自我学习的过程中能够总结出一些核心要点，与大家共同学习。鉴于个人的精力和能力均有限，如有错误或疏漏，还望不吝赐教。文中大量观点和图表都出自最后的参考文献，因内容较多，不在原文一一标注出来了。\u003c/p\u003e","title":"大模型Agent调研"},{"content":"1. NLP发展历程与模型架构 NLP的发展渐趋于统一化：在深度学习时代到来之前，NLP任务高度依赖于手工设计的复杂特征。随后，深度学习的出现极大地减轻了这种特征工程的负担。以BERT和GPT-1为代表的预训练和微调范式，标志着手工特征设计时代的终结。大模型的出现宣告了NLP中间任务的消亡，所有任务都可以统一到语言模型的范畴中。\n1.1. 规则阶段(1956～1992) 1.1.1. 阶段特点 基于规则的机器翻译系统是在内部把各种功能的模块串到一起，由人先从数据中获取知识，归纳出规则，写出来教给机器，然后机器来执行这套规则，从而完成特定任务。\n1.1.2. 模型架构 经验规则+系统设计\n1.2. 统计机器学习阶段(1993~2012) 1.2.1. 阶段特点 机器翻译系统可拆成语言模型和翻译模型。该阶段相比上一阶段突变性较高，由人转述知识变成机器自动从数据中学习知识，主流技术包括SVM、HMM、MaxEnt、CRF、LM等，当时人工标注数据量在百万级左右。\n语言模型(LM)是根据句子一部分来预测下一个词。语言模型训练采用交叉熵Loss，评估采用困惑度(Perplexity，PPL)，困惑度越小说明模型准确预测下一个词的把握越大：\n语言模型比较常见的应用是输入法提示：\n而大语言模型(LLM)最显著的特点就是训练数据量大、模型参数量大。大多数知识都能用自然语言来描述，互联网文本量大且蕴含大量知识，因此用语言模型来学习知识是个自然而又天才的想法。\n1.2.2. 模型架构 浅层模型：\n1.3. 深度学习阶段(2013~2018) 1.3.1. 阶段特点 相对上一阶段突变性较低，从离散匹配发展到embedding连续匹配，模型变得更大。该阶段典型技术栈包括Encoder-Decoder、LSTM、Attention、Embedding等，标注数据量提升到千万级。该阶段特点是以神经网络来做表征，下图是经典的词表征学习word2vec结构：\n1.3.2. 模型架构 该阶段模型架构主要为了解决具体NLP任务。NLP任务分为自然语言理解(NLU)和自然语言生成(NLG)两大类。\nNLU任务一般是对语言基础信息的理解，比如命名实体识别、句法分析、分词、语义角色标注等任务。不同任务会有不同的结构，例如利用stack LSTM解决依存句法分析任务：\nNLG任务一般是语言生成任务，比如生成式摘要，对话生成等任务。基础架构一般是encoder-decoder，例如较早结合encoder-decoder+attention解决机器翻译的工作：\n这一阶段，NLP的重心是为每个任务设计合适的模型结构提高任务指标。\n1.4. 预训练阶段(2018~2022) 1.4.1. 阶段特点 相比之前的最大变化是加入自监督学习。该阶段系统可分为预训练和微调两个阶段，将预训练数据量扩大3到5倍，典型技术栈包括Encoder-Decoder、Transformer、Attention等。\n1.4.2. 模型架构 word2vec只是利用了词共现，每个词的表示还是唯一的。但每个词在不同上下文环境下语义差别很大，例如\u0026quot;苹果”既可以表示水果，也可以表示公司，用同一个词向量表示显然是不合理的。此时动态词向量技术应运而生，例如ELMo，利用双向LSTM生成动态词向量。GPT和BERT也是预训练技术的重要代表工作，我们将在后文介绍。\n1.5. 大语言模型阶段(2023~) 1.5.1. 阶段特点 从2023年起，目的是让机器能听懂人的命令、遵循人的价值观。其特性是在第一个阶段把过去的两个阶段缩成一个预训练阶段，第二阶段转换成与人的价值观对齐，而不是向领域迁移。这个阶段的突变性是很高的，已经从专用任务转向通用任务，或是以自然语言人机接口的方式呈现。\n1.5.2. 模型架构 基于Transformer架构的组合和演化，衍生出了包括仅编码器（Encoder Only）、仅解码器（Decoder Only）以及编解码器结合（Encoder-Decoder）等多种架构。虽然从严格意义上来讲，目前的主流大型模型架构并不包括仅编码器模式，但为了完整性考虑，本文后续部分也将详细阐述该架构。\n2. LLM架构基石-Transformer 2.1. 整体架构 整体是一个encoder-decoder框架：encoder主体由多层self-attention构成，decoder相比encoder多了Encoder-Decoder Attention。\n2.2. Encoder 2.2.1. Self-Attention 2.2.1.1. Overview 2.2.1.2. Detail 矩阵计算视角：\n关键设计：Scale\n为什么要做Scale? Score是由两个向量内积得到，容易产生较大的值导致落入softmax函数梯度平缓区，容易导致梯度消失。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from scipy.special import softmax import numpy as np def test_gradient(dim, time_steps=50, scale=1.0): # Assume components of the query and keys are drawn from N(0, 1) independently q = np.random.randn(dim) ks = np.random.randn(time_steps, dim) x = np.sum(q * ks, axis=1) / scale # x.shape = (time_steps,) y = softmax(x) grad = np.diag(y) - np.outer(y, y) return np.max(np.abs(grad)) # the maximum component of gradients NUMBER_OF_EXPERIMENTS = 5 # results of 5 random runs without scaling print([test_gradient(100) for _ in range(NUMBER_OF_EXPERIMENTS)]) print([test_gradient(1000) for _ in range(NUMBER_OF_EXPERIMENTS)]) # results of 5 random runs with scaling print([test_gradient(100, scale=np.sqrt(100)) for _ in range(NUMBER_OF_EXPERIMENTS)]) print([test_gradient(1000, scale=np.sqrt(1000)) for _ in range(NUMBER_OF_EXPERIMENTS)]) 不scale结果：\n1 2 [0.012874944171283764, 0.24825918021760482, 0.0024202364125776032, 0.23220639834592616, 0.002987739995810368] [0.05744804282820459, 1.3786222829992312e-07, 1.3083745276532e-05, 0.016805764159904646, 1.7290933418401266e-07] scale后结果：\n1 2 [0.10416117163013226, 0.12557648647665576, 0.10035918562130762, 0.18524244407243343, 0.08591912714184714] [0.08396345043318147, 0.09472133857223597, 0.11776894854413103, 0.12306855963215982, 0.11043084033677895] 为什么是$\\sqrt{d_k}$ 根据假设$q_i$和$k_i$是两个相互独立，且均值为0，方差为1的随机变量，那么有：\n因此$QK^T$除以$\\sqrt{d_k}$可以将方差纠正为接近1，这样大部分值都在合理的区间了。\n2.2.1.3. MultiHead 引入MultiHead有两点好处：\n不同head可以重点关注不同位置的信息。 增加了Attention Layer的表示子空间。 筛选其中2个head结果\t8个head完整结果\n2.2.2. Position Embedding 上述self-Attention结构没有建模位置信息，而“顺序”对于NLP至关重要，比如“吃饭不”和“不吃饭”句子含义差异很大。\n计算公式：\n可视化：\n位置编码设计成这样有以下几个考虑：\nsin/cos自变量范围较小，很难重复，基本可以保障编码的唯一性。 对输入长度没限制，可以无限扩展 位置编码不仅表示绝对位置，也蕴含相对位置。 pos+k的位置编码可以由pos位置和k位置线性组合得到。\n问题1：为什么同时采用sin和cos？\n个人认为是为了利用三角函数特性，能够蕴含相对位置。\n问题2：sin-cos编码比绝对位置embedding更好么？\n在文中提过绝对位置Embedding和sin-cos位置编码效果类似，具体还需要根据任务效果来选择。比如BERT中就采用了绝对位置Embedding。\n2.2.3. Add \u0026amp; Normalize 残差和LN都能帮助缓解梯度消失问题，下面是一个实践经验：\n我们分析梯度值发现multi-head target attention参数的梯度值量级非常小，影响模型收敛速度。layer norm可以实现对梯度的平移和缩放，加入layer norm后梯度值被放大，最终带来1～2千分点的提升。\na) Multi-head Target Attention\nb) Multi-head Target Attention With Layer Norm\n2.3. Decoder Decoder与Encoder架构上有几个区别：\ndecoder多了个Encoder-Decoder Attention，为了在生成的时候结合输入信息。具体操作是Encoder最后一层表达生成$K$和$V$, 然后Decoder生成$Q$, 其他操作就是Multihead Attention。 decoder时self-attention只能用前面已生成的序列 解码时一般有greedy和beam search两种方式。 3. LLM架构范式 3.1. Encoder-Only Encoder-Only架构又称之为自编码(AutoEncoder)\n3.1.1. BERT 3.1.1.1. 动机 在BERT(Bidirectional Encoder Representations from Transformers)出来前，动态词表示有两个研究分支：一是feature-based，代表工作为ELMo(Embeddings from Language Models)，利用双向LSTM语言模型建模词表示，作为下游任务的特征使用。二是fine-tuning，代表工作是GPT(Generative Pre-trained Transformer)，利用单向Transformer训练语言模型，与下游任务配合fine-tune。\nELMo和GPT在训练的时候都是单向语言模型，用上文或者下文来预测下个词。上下文理解能力对很多任务至关重要，BERT提出MLM (Masked Language Model)建模上下文理解能力。简单来说，就是随机挖掉中间词，让模型通过上下文信息预测中间词的方式建模上下文信息，而Transformer本身就具备每个位置看到所有位置信息的能力。\n3.1.1.2. 两个训练任务 任务1：MLM\n引入两个特殊token：[CLS]代表分类符，训练NSP和下游Fine-tuning时都会用到；[MASK]代表该位置token被mask掉。\n核心设计-Mask：随机mask 15%的词级别tokens，该位置替换为[MASK]标识，让模型根据上下文预测该词。但是考虑到下游应用并没有[MASK]标识，因此将随机选中的tokens以100%概率替换为[MASK]，10%概率保留原词，10%概率替换为随机词。\n问题1：为什么不保留100%的[MASK]，而只保留100%呢？\n除了文中提到的下游应用没有[MASK]带来不一致，另外也避免[MASK]只学习到训练语料词的分布，比如训练数据有20%的词都是Is，那么[Mask]也倾向于有20%概率预测Is。\n问题2：为什么随机替换随机词？会对影响训练么？\n保留随机词有两个好处：一是增加模型鲁棒性，当见到脏数据模型也有能力做预测；二是会迫使模型利用上下文进行预测当前词。这类数据占比15% x 10% = 1.5%，数量很少，对模型训练影响不大。\n不同Mask策略实验结果如下：\n任务2：NSP (next sentence prediction)\n引入句子分隔符[SEP]\n有些任务需要句子级别依赖，例如阅读理解。为了建模长句子依赖，BERT引入NSP任务，判断B句子是不是A句子的下文。A句子下文50%概率保留原文作为正样本，50%概率随机替换别的句子作为负样本。\n3.1.1.3. BERT For fine-tuning BERT所有参数都会参与下游任务联合训练。\n3.1.1.4. BERT for feature extraction 只拼接最后4层就能获取很高的准确率。\n3.2. Decoder-Only Decoder-Only架构又称为自回归(AutoRegressive)。\n3.2.1. Causal Decoder 因果解码器架构采用单向注意力掩码，以确保每个输入 token 只能关注过去的 token 和它本身。输入和输出token通过解码器以相同的方式进行处理。代表工作是GPT系列。\n更好理解GPT架构执行流程，可以参考一个可视化网站：https://bbycroft.net/llm\n3.2.1.1. GPT-1：半监督学习 GPT-1是通过半监督学习方式解决NLP任务，即无监督的预训练+有监督的微调。\nGPT-1是一个基于Transformer的前向语言模型。在生成下一个单词时，Transformer只能读到左边已生成的部分。\n由于每个位置只能看到前面的序列，因此采用了Masked Self-Attention，这和BERT将不可见词替换为[MASK]标志不同。\n计算到第2个位置时，第一个位置产生的K和V可以复用，无需重复产出。\n其Fine-tune的模式如下：\n有两个关键设计：\n由于预训练阶段没有区分句子顺序能力，例如A句子后接B句子，那么语义上可能会学到B是A的下文，而对于句子相似度任务这种顺序会影响模型判断，因此同时保留A-B和B-A两种输入关系。 fine-tuning时也引入语言模型任务作为辅助任务，既能提高模型泛化能力，也能加速任务收敛速度。 GPT-1开始关注到decoder-only架构在解决zero-shot问题有一定优势，为GPT-2埋下伏笔。\n3.2.1.2. GPT-2：多任务学习 当时主流的NLP范式是预训练+微调，这种方式成本较高，而且缺乏泛化能力，每个新任务都需要标注数据进行fine-tune。GPT-2的核心尝试不微调，直接通过预训练学习多任务能力，无需微调！GPT-2有两个核心设计：一是将任务描述也作为模型的输入(即prompt)，二是大力出奇迹。\n将任务作为模型的输入是一个天才的想法，是NLP范式的一个巨大转变，核心思路一个公式就能讲清楚：\n对于翻译任务，输入可以定义为: (translate to french, english text, french text)\n对于阅读理解任务，输入可以定义为：(answer the question, document, question, answer)\n\u0026hellip;.\n从架构上，GPT-2与GPT-1一致，参数量增加了10几倍。GPT-1是117M，GPT-2是1.5B。从各种数据集合的表现来看，效果随着参数量增加也显著提升。\n以117M模型为例，每一层的decoder参数如下：\n整体的参数量计算如下：\n最终计算出124M，和论文提到的117M有一些出入。\n小知识：GPT输入最小粒度为什么是token，而不是word呢？\nGPT采用的输入切分方式是BPE(Byte Pair Encoding)分词：通过不断地合并出现频率高的子词，BPE分词算法可以得到一组细粒度的子词单元，这些子词单元可以更好地表示原始文本中的词汇和词组。由于BPE分词算法是基于频率的统计方法，它可以自适应地根据特定文本的特点生成合适的子词单元，从而在不同的自然语言处理任务中取得良好的效果。所以输入粒度并不是word概念。\n3.2.1.3. GPT-3：上下文学习 GPT-2通过将任务描述加入到输入中，让模型能够根据描述解决特定任务。但是这种做法上限较低，即使是人在解决新问题时也很难处理好。但有几个示例后，人能够根据知识和示例快速学习。GPT-3的提出就是这种动机，核心设计有两个：一是引入上下文学习(In-context learning, ICL)，大大提高了模型能力，尤其是处理复杂任务的能力；二是极致的暴力美学。\n我们再回顾下这句话“人能够根据知识和示例快速学习”，这种根据知识和示例快速学习的想法并不是第一次提出。Meta-Learning有异曲同工之妙。假设有一个 task 的分布，我们从这个分布中采样了许多 task 作为训练集。我们希望 meta learning 模型在这个训练集上训练后，能在这个分布中所有的 task 上都有良好的表现，即使是从来没见过的 task。\nMAML是比较经典的工作，目的是学习一个好的初始化，使得模型能快速适应新任务：\n其迭代分为内循环和外循环两个框架：\n外循环负责筛选一批任务进行一次梯度更新，内循环负责为每个任务计算更新梯度，但不会立马更新该任务梯度，而是在外循环中一次更新多个任务的梯度。这样的好处就是避免参数过于偏向最近训练的任务。\n回到GPT-3上来，上下文分为三种：Zero-shot、One-shot和Few-shot：\n对应到Meta-Learning框架上：\n不过GPT-3相比Meta-Learning是个更优雅的范式。面对新任务时，无需训练参数，只需要在输入给任务描述和少量示例即可。\nGPT-3另一个特点是极致的暴力美学：\n有两个发现：一是参数量对模型性能有极大影响，而是one-/few-shot对参数量大的模型性能提升更显著。\n原文细节非常多，这里不再赘述。\n3.2.2. Prefix Decoder 前缀解码器架构(也称非因果解码器架构)修正了因果解码器的掩码机制，以使其能够对前缀token执行双向注意力，并仅对生成的 token 执行单向注意力。代表工作是GLM(General Language Model)系列。\n回归模型从左到右学习语言模型，适合于长文本生成和少样本学习，但不能捕捉上下文词之间的双向依赖关系。\n自编码模型通过去噪目标学习双向上下文编码器，适合于自然语言理解任务，但不能直接用于文本生成。\n编码器-解码器模型结合了双向注意力和单向注意力，适合于有条件的生成任务，如文本摘要和回复生成。\n这三类语言模型各有优缺点，但没有一种框架能够在所有的自然语言处理任务中都表现出色。因此清华提出一个通用架构，想结合这三种框架的优点。\nGLM有两个关键设计：一是随机挖掉若干个子序列，并打乱顺序做生成；二是二维位置编码。位置也是为了前者服务。\n本质上GLM还是个自回归模式，只是预测的并不是句子的下个词，而是中间的序列。并且通过打乱机制让模型学到上下文能力。\n3.3. Encoder-Decoder Encoder-Decoder的代表工作BART，T5和Switch Transformer等，这里只介绍BART。\nBART(Bidirectional and Auto-Regressive Transformers)设计如下：\n除了上述架构，还有个比较重要的点是对输入加了噪声。\n3.4. MOE 对于上述架构，我们可以通过专家混合（MoE）扩展来进一步扩展它们，在这种扩展中，每个输入的一部分神经网络权重被稀疏激活，例如Switch Transforme。MoE的主要优点是它是一种灵活的方式来扩大模型参数，同时保持恒定的计算成本。有研究表明，通过增加专家数量或总参数大小可以观察到显著的性能改进。尽管有这些优点，但由于路由操作的复杂、硬切换特性，训练大型MoE模型可能会遇到不稳定性问题。\n3.5. 架构选择总结 目前主流的LLM架构是Causal Decoder，目前还无法从理论上证明这种架构的优越性。\n4. 参考文献 Apertium: a free/open-source platform for rule-based machine translation An Introduction to Conditional Random Fields for Relational Learning Neural Machine Translation by Jointly Learning to Align and Translate. The Illustrated Transformer The Illustrated BERT, ELMo, and co. The Illustrated GPT-2 如何理解Transformer论文中的positional encoding Meta Learning：一种套娃算法 Transformer的Score为什么Scale ChatGPT调研报告 AIGC：从不存在到存在 清华大学通用预训练模型：GLM A Survey of Large Language Models Transition-Based Dependency Parsing with Stack Long Short-Term Memory Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond Efficient Estimation of Word Representations in Vector Space BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Deep contextualized word representations Improving Language Understanding by Generative Pre-Training Language Models are Unsupervised Multitask Learners Language Models are Few-Shot Learners LLaMA: Open and Efficient Foundation Language Models Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks GLM: General Language Model Pretraining with Autoregressive Blank Infilling All NLP Tasks Are Generation Tasks: A General Pretraining Framework BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer ","permalink":"https://hpzhao.github.io/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84/","summary":"\u003ch1 id=\"1-nlp发展历程与模型架构\"\u003e1. NLP发展历程与模型架构\u003c/h1\u003e\n\u003cdiv align=\"center\"\u003e\u003cimg src=\"/img/1701143310613-4745c04a-1690-474a-8242-d98d0d54b07f.png\" alt=\"img\" style=\"zoom: 40%;\" /\u003e\u003c/div\u003e\n\u003cp\u003eNLP的发展渐趋于统一化：在深度学习时代到来之前，NLP任务高度依赖于手工设计的复杂特征。随后，深度学习的出现极大地减轻了这种特征工程的负担。以BERT和GPT-1为代表的预训练和微调范式，标志着手工特征设计时代的终结。大模型的出现宣告了NLP中间任务的消亡，所有任务都可以统一到语言模型的范畴中。\u003c/p\u003e","title":"大模型架构"},{"content":"1. AI电商概述 AI电商指借助AI技术，赋能各类型电商与行业模块，通过各类AI相关落地应用，从而对行业产生流量逻辑、用户体验、行业效率等影响价值。\n1.1. 从技术到价值 1.2. AI电商产业图谱 2. AI电商全行业赋能 2.1. 上游：供应链 2.1.1. AI产品设计 灵感阶段：借助AI搜索、生成式设计以及大语言模型的对话助手，设计师更快捷产生灵感、呈现创意； 设计阶段：借助AI建模与 AI 渲染等软件或平台，可以压缩从创意到设计定稿的工作周期； 实验阶段：借助AI检测、分析与控制工具，使产品从模型、手板到反复优化、最终确定，取得更可靠与精确的参数控制； 展示阶段：借助 AI 与 3D 扫描、3D 打印及 AR/VR 等工具，使成品具备更接近真实的、全方位的展示效果。 2.1.2. AI选品🌟 选品一般由“场”来决策，选品后平台通过算法、广告位售卖等方式进行供需匹配，并累积数据反馈优化链路。例如FlashSale、大促会场和首猜等场景，都是根据模块的定位和目标来进行选品，即由“场”来决定选品。\nAI电商有两个方面可以提升选品逻辑：\n赋能传统选品：用更先进的算法优化供需匹配。 创造新型选品：AI电商时代有望出现新的电商AI对话入口，选品决策权由传统的“场”入口过渡到“AI”入口，以用户需求为导向。举个例子，用户在导购入口既可以问“折扣力度大的手表”，也可以问“最近流行什么”，可能会涉及FlashSale、Fashion等业务的选品，选品以满足用户需求出发。 以亚马逊平台选品工具魔选为例：\n魔选跨境智能选品系统是一款基于大数据分析和人工智能技术开发的跨境电商选品工具，它可以帮助电商卖家在海量的商品中快速找到最优质的货源，提高商品的销售和利润。它不仅可以帮助卖家进行市场分析、竞争对手监测、商品关键词分析等工作，还可以通过AI算法进行商品推荐，提高卖家的销售额。\n2.1.3. 供应链 AI可以为供应链各环节赋能，使其进一步实现降本、增效和可持续：\n生产制造：通过 AI 模拟与计算的预生产分析，使生产计划的制定更加符合市场需求；通过计算机视觉、智能机器人等赋能，生产制造将不断趋近极致高效与柔性。 仓储：AI 智能化的决策为仓储规划提供更低风险、更具前瞻性的战略选址与分配布局；AI 监测与管控为库存商品从订货、保存、周转与安全提供全方位支持。 物流：从订单与地址的识别与组合，匹配仓库商品与运输配送资源，AI 可以贯穿物流的运输、中转与末端配送，降低成本、提高效率，在保证企业利润与竞争力的同时，提高消费者体验。 以京东物流超脑为例，依托行业大模型，可以提供交互、分析和决策功能。\n2.1.4. AI预测 从需求的角度看：AI 的预测能力将呈现三个阶段的发展趋势。通过不断预测与洞察消费者需求，AI 将协调供应链更自主、更敏捷、更聪明。\n响应需求阶段：该阶段与目前的数字化供应链的工作方式类似，AI 作为工具嵌入部分环节，需求仍然以节点式向供应链上游逐级传递；AI 更为准确的分析与预测，将显著降低生产与库存风险。 预测需求阶段：电商平台与供应链部分企业将布局入口级 AI，通过 AI 算法来匹配需求与供给；电商广告将不再仅局限于买量与标签，也将基于 AI 理解做出更适配的推荐。 洞察需求阶段：未来，AI 产品将有望建立新的入口，超越电商平台现有入口而占据话语权；电商广告形态将从单一型向复合型转变，AI 具备洞察能力，以点带面，全方位覆盖每个消费者的衣食住用行。 例如观远数据就是利用AI做商业分析的：\n2.2. 中游：内容制作与展示 2.2.1. AI创意与生成🌟 在电商平台上，合适的产品素材制作与展示非常重要。品牌或者商家需要将合适的素材包装到产品上，以在电商平台得到更为出众、利于销售的产品展示效果。\n“货” 要素：随着 AIGC 工具的丰富与成熟，多模态、跨模态生成文案、图片、音频、视频的效率与效果均会显著提升，从产品介绍、物料生成、直播脚本到广泛的传播素材，AI 均可助力创意、降本增效。 “场” 要素：借助 AI 与 AR/VR 技术，商家搭建全新的虚拟货场可以为消费者提供更为沉浸式购物的体验，通过虚拟试用服务也可以使消费者享受亲临现场的体验，从而降低退货率。 这里是AI电商的发力点之一，我们多举几个案例：\n阿里绘蛙AI为卖家和达人生成营销文案和图片。 国际站AI生意助手提供以下功能： 智能发品，提升运营效率：商品标题优化、商品图生成、商品讲解视频生成、商品关键词智能扩充 智能接待：基于国际站海量买家采购偏好,在需求分析后,智能生成买家标签,不遗漏每一个买家,随时为谈单做好准备。同时智能生成RFQ报价,智能对话润色,直击买家心坎,轻松获客快速成单。 智能分析：基于国际站24年全球跨境贸易积累,自动学习,专业分析站内外海量市场数据,为你分析、诊断商品,给出市场结论和生意建议,专业解析洞察先机。 腾讯开源虚拟试衣-IMAGDressing支持对服装的灵活控制、可选的面部、姿势和场景。 Amazon Sellesta.ai是一款面向亚马逊卖家的工具，可帮助他们在 ML-Technologies 的支持下进行关键词搜索、优化商品信息、分析评论和竞争对手。 关键词搜索和商品信息优化：找到对商品最重要的关键词，并查看它们的搜索量和与商品的相关性。使用自动文本生成功能更新商品名称、要点和关键词 商品评分： 获取优化清单，该清单可评估商品的主要因素：商品名称、商品描述、图片、评论量等。 评论分析：分析评论并查看评论中的关键方面及其语气 竞争分析： 自动识别您的竞争对手。实时追踪他们的 BSR、关键词、价格等。 Amazon数字化营销 包含营销与广告素材生成(文生图、文生视频)，商品文案生成，智能换装。 2.2.2. AI虚拟数字人🌟 通过AI对虚拟数字人进行赋能，根据各阶段核心任务及目标结果，大致分为四个部分，完整契合直播电商。\n创建：通过 AI 来进行全方位建模，包括性格、形象、音色、活动场景等，AI 赋能的创建过程，将比传统虚拟数字人建模更为高效、全面。 驱动：AI 可以赋能虚拟数字人一站式完成驱动与渲染，通过语音语义与 AI 神经网络渲染，AI 虚拟数字人将具备深度学习与自我迭代能力。 交互：AI 虚拟数字人的核心能力将体现在直播过程中与消费者的实时互动能力，通过逐渐完善的数据投喂形成深度学习与训练，AI 虚拟数字人有望实现丰富与真实的文本、语音、肢体及表情互动能力。 支持：除虚拟数字人本身外，AI 也将赋能场景、客服、助理及实时的切片视频素材制作等直播间工作。在 AI 支持下，虚拟数字人将成为一站式平台化服务，赋能直播电商。 AI数字人直播也是AI电商非常活跃的领域：\n硅基智能-数字人直播：人、货、场的原子化组合，构建全新的直播运营模式。 亚马逊数字人直播方案：利用数字人进行电商直播，结合智能问答提升数字人直播客户体验。实现不同形象的数字人，在不同的背景下，以不同的语言来进行直播，支持弹性扩展，实现全球低延迟直播。 百度数字人直播-慧播星：可一键生成专业带货脚本，支持个性化定制，打造顶级直播间。 讯飞虚拟数字人：适用多种场景。 京东虚拟数字人：数字刘强东引起热议 阿里虚拟数字人：基于达摩院在语音、图像、3D美术、自然语言处理等领域的技术优势，提供以实时交互为目标的「数字人流媒体」产品、以效率化内容生产为目标的「数字人视频合成」产品，以及丰富表现力的数字人资产构建定制服务。 BocaLive：AI数字人跨境直播 2.3. 中游：营销与运营 2.3.1. AI营销 通过 AI 对营销的投放前、投放中、投放后三个模块内与模块间的赋能，AI 可以生成投放策略、预估投放效果、调整投放动作并整体提高 ROI。\n投放前：通过外部渠道、用户及触点的分析与洞察，对营销成本及 ROI 进行预估，并通过评估来校正营销物料与营销目标，最终生成组合投放策略，进行智能投放。 投放中：通过 AI 对投放效果进行监测，通过数据分析及反馈来动态调整营销物料与投放策略；通过 AI 技术与用户更生动与有效地互动，将潜在用户与新增用户自动导流至私域并进行 AI 化管理。 投放后：对投放进行快速、准确与客观的回顾与分析，通过数据沉淀的方式来微调 AI 模型，以指导下一次投放过程；如果是服务商，则可将具备客户价值的营销案例有效生成，用于业务宣传。 重点是AI营销物料生成，例如外投标题生成。\n典型代表为百度营销：\n2.3.2. AI运营 在运营较重的电商行业，AI 在某些环节上目前具备较强的赋能作用，但在某些环节上仍然需要较重人力参与，AI 潜力将注定被不断挖掘与释放。\n产品运营：通过 AI 产品赋能内外部用户的数据分析、竞对分析来指导产品优化与迭代策略、推广策略，监测用户触达与反馈，并围绕产品及相关数据指导上下游管理策略。 内容运营：AI 可显著赋能热点采集、选题策划、内容创意与生成；在内容发出后，AI 也可赋能分发策略，将合适的内容在合适的场景带到合适的用户前，并引发内容扩散，根据反馈数据来指导策略调整。 活动运营：AI 可赋能活动的策划及效果评估，并及时纠正活动内容；但活动执行仍需要较多的人为组织与维护，AI 更多在数据监测上给予动态支持。 用户运营：借助 AI，可以提前洞察消费者需求、偏好及相关行为，并生成相关策略；AI 也可以胜任个性化推荐以及与用户的基础沟通，为消费者带来更好的界面体验。 典型代表为淘宝天猫商家后台-千牛：\n2.4. 下游：流量场(搜索与推荐) 2.4.1. AI对话🌟 由于 AI 对话可以充分满足消费者需求，完整贯穿消费者网购行为的首尾，扮演着导购与顾问的角色，所以是 AI 电商极具想象力的入口级模块。\nAI 对话的赋能：随着 AI 的发展，AI 对话可以充分理解消费者各模态形式的需求并给出合理响应，伴随消费者网购的全路径，成为全景式服务的导购与顾问。 AI 对话的发展趋势：AI 对话的 1.0 阶段是应用级，核心特征是通过问答形式辅助消费者形成购买决策；2.0 阶段是嵌入级，核心特征是消费者对其形成依赖，属于消费者网购的刚需应用，各大电商平台与商家需自建或采买 AI 对话功能以嵌入到现有流量入口；3.0 阶段是入口级，超级 AI 可代替原有网购入口，形成 AI 驱动的全新超级入口，电商的商业模式或被改写。 LazzieChat还属于1.0阶段。\n导购助手也是各大厂商发力的重点之一：\n京东京言. 淘宝问问. Amazon Rufus 亚马逊上周最新发布了AI 购物助手 Rufus。Rufus经过了亚马逊产品目录以及网络上众多信息的培训，能够回答顾客关于购物需求、产品对比等问题，提供推荐，并帮助用户在他们常用的亚马逊购物平台上发现新产品。\n借助 Rufus，客户可以：\n[知识导购] 了解购买产品类别时要寻找什么：客户可以在亚马逊上进行更一般的产品研究，提出诸如“购买耳机时要考虑什么？”等问题。是清洁美容产品吗？”并获得有用的信息来指导他们的购物任务。 [知识导购] 按场合或目的购买：客户可以通过询问一系列问题（例如“寒冷天气打高尔夫球需要什么？”），根据活动、事件、目的和其他特定用例来搜索和发现产品。或“我想开办一个室内花园。 [知识导购] 获得比较产品类别的帮助：客户现在可以询问“唇彩和唇油有什么区别？”或“比较滴滤式咖啡机和手冲式咖啡机”，以便他们找到最适合自己需求的产品类型，并做出更自信的购买决定。 [知识导购] 找到最佳推荐：客户可以根据自己的需求提出建议，例如“情人节送什么礼物好？“。 Rufus 生成针对特定问题量身定制的结果，并使客户能够快速轻松地浏览更精确的结果。 [商品问答] 在产品详细信息页面上询问有关特定产品的问题：客户在查看产品详细信息页面时，可以使用 Rufus 快速获得有关各个产品的特定问题的答案，例如“这款泡菜球拍适合初学者吗？”。 Rufus 将根据列表详细信息、客户评论和社区问答生成答案。 可以看出Rufus的核心功能点也是知识导购、商品问答和商品对比等。\n2.4.2. AI搜推 大模型直接应用到搜推荐架构，改造或替换原有模块。详细见调研《大模型在推荐领域应用》\n2.5. 下游：客服与客户管理 2.5.1. AI客服🌟 通过 AI 对在线客服、辅助人工、外呼中心三个模块的赋能，电商行业的客服将显著降本增效。但现阶段，AI 在情绪同理上仍然无法取代真人。AI 客服的下一步重点，将是从提供功能价值向提供情绪价值进阶。\n功能价值：目前通过语音识别、自然语言处理、深度学习，AI 已经日渐完善能满足消费者基本的功能需求，此阶段重点是让 AI 会听、会说、会学，不断自我进化。 情绪价值：在过去，由于 AI 知识库不完备、语料不够丰富、没有个性化的形象，AI 客服饱受语病，尤其在售后阶段，在情绪价值上无法完全分担人工客服的工作压力。在未来，决定 AI 客服的关键在于其提供情绪价值的能力，不仅能满足消费者功能诉求，更能有同理心地安抚与消解消费者遇到问题后的负面情绪，甚至有个性化的输出来适配不同品牌、不同场景下的形象诉求，为企业品牌赋能。 典型应用例如 智齿科技\n2.5.2. AI客户管理 通过 AI，品牌商或卖家可以对公域与私域的客户管理更便捷与智能。\n公域客户管理：通过 AI 对触达、转化、成交的全链路各主要节点进行赋能，使客户画像更全面、营销与互动更个性化、数据分析更准确与便捷；成交用户将自动分组至私域。 私域客户管理：通过更适时的客户关怀与需求追踪，来引导客户完成复购，并进而促使转化为忠诚客户；AI 也可以促使忠诚用户直接或间接、主动或被动来推荐新用户。 3. 机遇分析 3.1. 消费者调研 随着AI的出现，消费者对于AI改善购物流程抱有多方面期待，重点是通过AI来简化购物流程、提升购物体验。通过AI电商的AI创意与生成、AI营销、AI对话、AI客服、AI客户管理等模块，可以解决目前消费者购物痛点。\n3.2. 降本增效：AI落地电商30大场景 3.3. AI电商价值潜力评估 根据落地周期的长短、规模效应的高低以及 AI 赋能相应模块后的价值潜力，将 AI 应用类型落成以下图表，并分为四个象限。\n浅涧微澜：落地周期较快、规模效应较低，包括 AI 客服、AI 产品设计，集中体现在降本增效，此类应用落地带来的较好价值，将进一步带动其他 AI 应用落地的积极性。 急流狂涛：落地周期较快、规模效应较高，现阶段 AI 电商极具开拓价值的 AI 应用类型，包括 AI 创意与生成、AI 虚拟数字人、AI 营销，其应用能进一步深化 AI 电商的形成。 深海巨潮：落地周期较长、规模效应极高，AI 电商的终极想象力，包括 AI 选品、AI 对话、AI 预测、AI 供应链，其逐渐落地将影响 AI 电商的入口形态及模块间联动形式。 涓泉清漪：落地周期较长、规模效应较低，主要是其模块标准化难度较高的原因，致使其难以在整个行业进行统一，更多是定制化输出，包括 AI 客户管理、AI 运营。 3.4. 应用启示 亿欧智库认为，平台方应秉持自研与外采/集成相结合的策略，对于影响流量价值、电商价值的应用，平台应完全自主开发；对于客户比较重要、与平台捆绑较深的应用，平台应尽量自主开发；对于长尾效应明显、定制化服务较高的应用，平台可以采取外采与集成。\n自研AI应用：AI对话、AI预测关系到电商入口形态与电商模块间协作效率，是关乎AI电商终局的应用，平台需完全自主开发；而AI营销属于电商生态内的赋能商家工具箱，与平台捆绑，平台也需自主开发。其他如AI产品设计、AI选品、AI供应链等与产品生产、流通与展示相关的AI应用，属于商家高频使用、标准化较高的应用，平台也应尽量自主开发。 外采/集成AI应用：AI虚拟数字人、AI运营、AI客服等，定制化程度较高，长尾效应明显，平台可外采与集成技术度较高的服务商。 4. 参考资料 从“电商+AI”到“AI电商”，人工智能如何重塑电商生态？ 生成式 AI 电商_AIGC 电商解决方案 - AWS 云服务 硅基智能-数字人直播 Amazon-Sellesta.ai Kopia - AI Virtual Try-On ‘Amazon Rufus’ AI experience comes to the Amazon Shopping app 绘蛙AI_AI电商营销内容创作平台 IMAGDressing 国际站AI生意助手 百度电商以AI为依托，让消费者省心买，让商家省心卖 AIGC周刊 【亿欧智库】AI电商行业展望与价值分析 七鱼在线机器人_智能客服机器人_智能问答-网易七鱼，网易云商， 在线客服系统_网上客服系统_全渠道智能客服|智齿科技【官网】 观远BI ","permalink":"https://hpzhao.github.io/posts/ai%E7%94%B5%E5%95%86%E8%B0%83%E7%A0%94/","summary":"\u003ch1 id=\"1-ai电商概述\"\u003e1. AI电商概述\u003c/h1\u003e\n\u003cp\u003eAI电商指借助AI技术，赋能各类型电商与行业模块，通过各类AI相关落地应用，从而对行业产生流量逻辑、用户体验、行业效率等影响价值。\u003c/p\u003e","title":"AI电商调研"},{"content":" 大学时期个人资料，待更新\u0026hellip;\nRESEARCH INTEREST Natural Language Processing, Machine Learning, Deep Learning DEDUCATION †† M.S., Harbin Institute of Technology 2016～2018 †† B.S., Harbin Institute of Technology 2012~2016 PUBLICATION Yijia Liu, Wanxiang Che, Huaipeng Zhao, Bing Qin, and Ting Liu. 2018. Distilling Knowledge for Search-based Structured Prediction. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL). Yuan J, Zhao H, Zhao Y, et al. Babbling-The HIT-SCIR System for Emotional Conversation Generation[C]//National CCF Conference on Natural Language Processing and Chinese Computing. Springer, Cham, 2017: 632-641. Che, W., Guo, J., Wang, Y., Zheng, B., Zhao, H., Liu, Y., … \u0026amp; Liu, T. (2017). The HIT-SCIR system for end-to-end parsing of universal dependencies. Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, 52-62. PROJECTS NLP\u0026amp;CC-2017 Shared Task - Emotional Conversation Generation (Rank 2th) CoNLL-2017 Shared Task - Universal Dependencies (Rank 4th) Chinese Disfluency Detection EMPLOYMENTS Algorithm Engineer, Alibaba Inc., Search Division. 2018.07~Now Intern Researcher and Developer, IFLYTEC Inc., HIT·iFLYTEK Language Cognitive Computing Lab (HFL). 2016.05~2016.08 TECHNIQUE Programming Languages: Python, Java, Shell, SQL Experience: PyTorch, TensorFlow AWARDS The National Scholarship (2013) People scholarship (2013） ","permalink":"https://hpzhao.github.io/about/","summary":"\u003cblockquote\u003e\n\u003cp\u003e大学时期个人资料，待更新\u0026hellip;\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"research-interest\"\u003eRESEARCH INTEREST\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eNatural Language Processing, Machine Learning, Deep Learning\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"deducation\"\u003eDEDUCATION\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e†† M.S., Harbin Institute of Technology\u003c/strong\u003e \u003cem\u003e2016～2018\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e†† B.S., Harbin Institute of Technology\u003c/strong\u003e \u003cem\u003e2012~2016\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"publication\"\u003ePUBLICATION\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eYijia Liu, Wanxiang Che, \u003cstrong\u003eHuaipeng Zhao\u003c/strong\u003e, Bing Qin, and Ting Liu. 2018. \u003ca href=\"http://aclweb.org/anthology/P18-1129\"\u003eDistilling Knowledge for Search-based Structured Prediction\u003c/a\u003e. In Proceedings of \u003cem\u003ethe 56th Annual Meeting of the Association for Computational Linguistics\u003c/em\u003e (ACL).\u003c/li\u003e\n\u003cli\u003eYuan J, \u003cstrong\u003eZhao H\u003c/strong\u003e, Zhao Y, et al. \u003ca href=\"https://link.springer.com/chapter/10.1007/978-3-319-73618-1_53\"\u003eBabbling-The HIT-SCIR System for Emotional Conversation Generation\u003c/a\u003e[C]//National CCF Conference on Natural Language Processing and Chinese Computing. Springer, Cham, 2017: 632-641.\u003c/li\u003e\n\u003cli\u003eChe, W., Guo, J., Wang, Y., Zheng, B., \u003cstrong\u003eZhao, H.\u003c/strong\u003e, Liu, Y., … \u0026amp; Liu, T. (2017). \u003ca href=\"http://www.aclweb.org/anthology/K17-3005\"\u003eThe HIT-SCIR system for end-to-end parsing of universal dependencies\u003c/a\u003e. Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, 52-62.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"projects\"\u003ePROJECTS\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eNLP\u0026amp;CC-2017 Shared Task - Emotional Conversation Generation (Rank \u003cstrong\u003e2th\u003c/strong\u003e)\u003c/li\u003e\n\u003cli\u003eCoNLL-2017 Shared Task - Universal Dependencies (Rank \u003cstrong\u003e4th\u003c/strong\u003e)\u003c/li\u003e\n\u003cli\u003eChinese Disfluency Detection\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"employments\"\u003eEMPLOYMENTS\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAlgorithm Engineer, Alibaba Inc., Search Division. \u003cem\u003e2018.07~Now\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003eIntern Researcher and Developer, IFLYTEC Inc., HIT·iFLYTEK Language Cognitive Computing Lab (HFL). \u003cem\u003e2016.05~2016.08\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"technique\"\u003eTECHNIQUE\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eProgramming Languages: Python, Java, Shell, SQL\u003c/li\u003e\n\u003cli\u003eExperience: PyTorch, TensorFlow\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"awards\"\u003eAWARDS\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe National Scholarship (2013)\u003c/li\u003e\n\u003cli\u003ePeople scholarship (2013）\u003c/li\u003e\n\u003c/ul\u003e","title":""}]